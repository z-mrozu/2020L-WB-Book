[
["index.html", "ML Case Studies Preface", " ML Case Studies 2020-07-02 Preface This book is the result of a student projects for Case Studies course at the Warsaw University of Technology. Each team prepared an article on one of the topics selected from reproducibility, imputation, and interpretability. This project is inspired by a book Limitations of Interpretable Machine Learning Methods created at the Department of Statistics, LMU Munich XAI Stories. Case studies for eXplainable Artificial Intelligence done at the Warsaw University of Technology and at the University of Warsaw. We used the LIML project as the cornerstone for this repository. The cover created by Anna Kozak. Creative Commons License This book is licensed under the Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License. "],
["technical-setup.html", "Technical Setup", " Technical Setup The book chapters are written in the Markdown language. The simulations, data examples and visualizations were created with R (R Core Team 2018) and Python. The book was compiled with the bookdown package. We collaborated using git and github. For details, head over to the book’s repository. References "],
["reproducibility.html", "Chapter 1 Reproducibility of scientific papers", " Chapter 1 Reproducibility of scientific papers This chapter contains a wide range of studies on the reproducibility of scientific articles. Each subsection is a self-contained paper answering a different research problem. Please, note that each subsection contains a work of different authors and therefore subsections may differ at some points, for example, definitions of reproducibility used in particular studies. "],
["how-to-measure-reproducibility-classification-of-problems-with-reproducing-scientific-papers.html", "1.1 How to measure reproducibility? Classification of problems with reproducing scientific papers", " 1.1 How to measure reproducibility? Classification of problems with reproducing scientific papers Authors: Paweł Koźmiński, Anna Urbala, Wojciech Szczypek (Warsaw University of Technology) 1.1.1 Abstract After quite short time the computational aspects of scientific papers become obsolete or inexecutable (e.g. codes are not valid with current environment, resources are inaccessible, some classes require additional parameters). The extent of such a loss of timeliness of a paper may vary a lot. We develop a scale suitable for comparing reproducibility of various papers. This scale is based on enumerating and subjective evaluation of the impact of irreproducibilities on the reception of the paper. 1.1.2 Introduction The idea of reproducibility of scientific researches is crucial especially in the area of data science. It has become more important along with the development of methods and algorithms used in machine learning as they are more and more complex and complicated. This issue concerns users of all types: students, scientists, developers. Moreover, attaching code used in a paper, helps readers to focus on the real content rather than sophisticated explanations and descriptions included in the article. It is also valuable because the users can use the code as examples of using the package. However problem of the reproducibility is much more complex, because there is no explicit way of measuring it. It means that most of its definitions divide articles into 2 groups - reproducible and irreproducible. Thus, finding an appropriate reproducibility metrics, which would have wider set of values would result in changing the way reproducability is perceived. As a result such a metric would provide much more information for a person who would be interested in reproducing an article. 1.1.2.1 Definition Reproducibility as a problem has been addressed by scientists of various fields of studies. The exact definition also differs among areas of studies. For instance, Patrick Vandewall in 2009 suggested a definition of a reproducible research work: “A research work is called reproducible if all information relevant to the work, including, but not limited to, text, data and code, is made available, such that an independent researcher can reproduce the results” (Vandewalle, Kovacevic, and Vetterli 2009). On the other hand, Association for Computing Machinery (Computing Machinery 2018) divides the problem into three tasks as follows: Repeatability (Same team, same experimental setup): The measurement can be obtained with stated precision by the same team using the same measurement procedure, the same measuring system, under the same operating conditions, in the same location on multiple trials. For computational experiments, this means that a researcher can reliably repeat her own computation. Replicability (Different team, same experimental setup): The measurement can be obtained with stated precision by a different team using the same measurement procedure, the same measuring system, under the same operating conditions, in the same or a different location on multiple trials. For computational experiments, this means that an independent group can obtain the same result using the author’s own artifacts. Reproducibility (Different team, different experimental setup): The measurement can be obtained with stated precision by a different team, a different measuring system, in a different location on multiple trials. For computational experiments, this means that an independent group can obtain the same result using artifacts which they develop completely independently. For the needs of this chapter we will use the Vandewalle’s definition and treat papers as fully reproducible only when they meet the conditions listed there. 1.1.3 Related Work Reproducibility is a hot topic. “Open Science in Software Engineering” (Fernández et al. 2019) describes the essence of open source, open data, open access and other openness. The article mentions that ability to reproduce work is important for the value of research. Open Science has many positive effects: increases access and citation counts, supports cooperation through open repositories. “Reproducibility Guide” (“Reproducibility in Science: A Guide to enhancing reproducibility in scientific results and writing” 2014) contains a lot of informations and tips on how to make research easier to reproduce. The guide also contains the list of tools that can make our research more reproducible (for example version control and automation. And the most important for us: it includes the checklist of questions that help verify the ability to reproduce. Edward Raff emphasizes the word independent in his article (Raff 2020). Independent reproducibility means that we can obtain similar results independently of the author and his code. These articles highlight various aspects of reproducibility. We want to verify how the authors care about reproducibility, what are their biggest reproduction issues and what type of problems can we encounter reproducing articles. 1.1.4 Methodology We considered plenty of papers from many issues of The R Journal - one of the most popular magazines concerning scientific researches, including new R packages. The journal stands out from the magazines because the researches usually upload supplementary material with their articles so it is very easy to check if the code can be reproduced in right way. The articles we checked during our research were published in various years - the newest comes from December 2019 while the oldest is from 2009. We have to admit that the majority of articles could be reproduced without any problems. For the needs of this article we mention only papers where any problems occured. As we faced the problem of measuring reproducibility we discussed many ways of grading its level. One of the ideas was to create a unified measure of value that would calculate the ratio of functions that managed to execute. We quickly noticed that this approach is not appropriate as sometimes it is not fair to dock the mark by the same value in various examples. For instance we could meet a minor problem that interefered with executing an additional feature and, on the other hand, a vast problem that was a reason that we could not produce an important plot at all. Moreover, sometimes the success of executing the function, was not only defined by completing the work without errors but by the output’s quality. Sometimes the plots were produced without any number values what made them absolutely useless or without one minor annotation which still allowed to make similar conclusions as authors. These were the reasons why we did not decide to create a simple numeric measure of reproducibility, which probably would be very convenient for data scientists, especially statisticians. When we were checking the articles in terms of reproducibility we noticed that the problems we are facing can be group into a few categories of similar ones. It was on impulse to propose six major categories that can be faced during try of reproducing the results presented in a scientific paper: No access to external resources Some parts of code require access to external resources, for example third-party API or data downloaded from web. If the data was removed from the website, we may have a problem (or it can be impossible!) reproducing the results. No compatibility with current versions of used packages Some packages are deprecated or only available in the older version of R. It can cause problems and is unacceptable. Code vulnerable to user settings (especially graphic issues) The output often depends on the environment settings. For example the scale of the graphics can make it illegible and useless. There were cases that the code attached to article produced completely different figure than the presented one. Additional configuration required Some packages require a non-standard installation. To use some features it can be required to install system packages. Sometimes it is also required to take additional steps (configure access to API e.t.c.). Randomness problems Some functionalities are based on randomness. Sometimes changing the seed may change the results and make it difficult to draw correct conslusions. No access to source codes Some results shown in an article could not be reproduced because the codes had not been attached to or included in the article. We developed a 6-point scale (0 - completely irreproducible, 5 - fully reproducible) evaluating in what degree these problems belong to each category. Points are assigned subjectively depending on our feeling of the severity of the problem. When a category did not appear in the article, it was signed as N/A - not applicable. To minimilize the affect of the personal feels, every article was checked independently by at least two persons. 1.1.4.1 Mark examples Most of the articles were reproducible to some extent. None of them were fully irreproducible. However there were few examples, where inability of compiling the first chunks of code resulted in very low marks for the article and thus giving it up with no further research being carried through. Perfect example of such behaviour can be found in an article “MCMC for Generalized Linear Mixed Models with glmmBUGS”(Brown and Zhou 2010), where all of the following code chunks depended on first ones, which couldn’t be compiled. The reason was that the function, which was resposible for making crucial calculations couldn’t find a registry address. It ended up with displaying both the error and warning message. Second thing which led to lowering the mark was difficulty with code availibility. There were articles, for instance “RealVAMS: An R Package for Fitting a Multivariate Value-added Model (VAM)” (Broatch, Green, and Karl 2018), where there were no source codes for all of the figures, which were used in article. Moreover the figures were the main part of the article, thus we decided to lower the mark for access to the source code. Fortunately our team of scientists managed to reproduce the results, despite lack of source code. This article was also an example of having attached obsolete data. It resulted in poor similarity of graphs and plots between the figures we made ourselves and those, which were used in article. Majority of articles were given very satsfying marks, beacuse there were only a few things we could complain about. Fortunately they didin’t have such an impact on reproducibility itself, but rather were annoying for someone who wanted to achieve the same results. The perfect example of such an article is “tmvtnorm: A Package for the Truncated Multivariate Normal Distribution” (Wilhelm and Manjunath 2010). The code had to be manually copied from the article and then reformatted before pasting in into the R console. It’s not a major obstacle, but it may lead to some syntax mistakes and enlengthen the time needed to reproduce the results. 1.1.5 Results During the research, our team of scientists examined 16 scientific articles published in The R Journal in terms of reproducibility. As stated before, we decided to divide a mark into six categories and check the level of correctness with the results described in the paper. Every category was graded in six-point scale provided always that a category might not apply in a paper. However, we did not measure the effectivity or functionality of the code as it was not in the scope of this research. To avoid the effect of subjectivity, all articles were graded by at least two of us. Later we calculated the average of the marks. When any category was marked as “not applicable” by at least one marker, it was not taken into consideration in the final summary. The list of articles checked is here. The summary of our marks is presented in the boxplot. Articles’ marks distribution As we can see in the plot, packages dependent on external resources are in a minority. Despite that, when we examined one, there was often a problem with dependencies. It could be caused by many reasons, e.g. external data sources or other packages. On the other hand, availability of the source code and graphical aspects of articles turned out to be the most reproducible categories. Resistance to randomness was one of the most interesting categories in our opinion and it turned out to be a category with high variation of grades. Many authors coped with the problem by setting random seeds but sometimes the differences were unavoidable. 1.1.6 Summary and conclusions Having revised many different articles from different areas and years of publishing clearly we managed to take a wide snapshot of problems, which people can ecnounter while trying to reproduce the articles. There is no doubt that the categories we proposed are quite subjective and arbitrary, however we are convinced that most of the obstacles that could be faced during reproduction fall into one of these categories. Our scientific research team strongly beliefs that introducing these categories with an earlier mentioned 5-point describes ability to reproduce in much more sophisticated and understandable manner. Dividing articles into 2 groups - these, which are reproducible and these which are not is very misleading. Moreover there are articles, which are difficult to label and it may pose a threat of not labeling them correctly. Thanks to our new system it would be much simpler process for any person who would like to reproduce the article. To support our claim we carried the survey through among students, who also had an opportunity to deal with the reproduction itself. Accroding to them reproducibility topic was missing the decent scale, which could help in determining the level of reproducibility. Most of them made a point that our scale suits their needs perfectly and would rather have it instead of the 2-category one. By and large we are satsified with our small contribution into the world of reproducibility, because it seems it worked out well and made the classification of reproducibility more flexible and appealing to other scientists. References "],
["aging-articles-how-time-affects-reproducibility-of-scientific-papers.html", "1.2 Aging articles. How time affects reproducibility of scientific papers?", " 1.2 Aging articles. How time affects reproducibility of scientific papers? Authors: Paweł Morgen, Piotr Sieńko, Konrad Welkier (Warsaw University of Technology) 1.2.1 Abstract Reproduction of a code presented in scientific papers tend to be a laborious yet important process since it enables readers a better understanding of the tools proposed by the authors. While recreating an article various difficulties are faced what can result in calling the paper irreproducible. Some reasons why such situations occur stem from the year when the article was published (for example usage of no more supported packages). The purpose of the following paper is to prove whether this is a general trend which means answering the question: is the year when the article was published related to the reproducibility of the paper. To do so a package CodeExtractorR was created that enables extracting code from PDF files. By using this tool a significant number of articles could be analyzed and therefore results received enabled us to give an objective answer to the stated question. 1.2.2 Introduction Every article published in a scientific journal is aimed at improving our knowledge in a certain field. To prove their theories, authors should provide papers with detailed, working examples and extensive supplementary materials to reproduce results. Unfortunately, these conditions are not always fulfilled. In such a case, other researchers are not able to verify and accept the solutions presented by the author. Moreover, the article is not only useless for the scientific community but also for business recipients. Over the years, several different definitions of reproducibility have been proposed. According to Gentleman and Temple Lang (2007), reproducible research are papers with accompanying software tools that allow the reader to directly reproduce methods that are presented in the research paper. Other authors suggest that scientific paper is reproducible only if text, data and code are made available and allow an independent researcher to recreate the results (Vandewalle, Kovacevic, and Vetterli 2009). Second definition emphasizes the importance of accessibility to data used in researches, therefore it seems to be more suitable and complete interpretation of reproducibility. In addition, in this article, we used scale based on the spectrum of reproducibility, proposed by Peng (2011). In his work, he also mentioned reproducibility as a minimal requirement for assessing the scientific value of the paper. In the past few years, computing has become an essential part of scientific workflow. Some best practices for writing and publishing reproducible scientific article were presented by Stodden et al. (2013). Furthermore, she made a brief overview of existing tools and software that facilitate this task. Similar issue was closely described by Kitzes, Turek, and Deniz (2017). Tools created solely for reproducibility in R were proposed by Marwick (n.d.) in package rrtools. Although many articles focus on software or framework solutions for reproducibility problems, analysis of scientific papers reproducibility in the context of release date has, to the best of the authors’ knowledge, not been described before. The intention of such research is to find correlations between age of article and its reproducibility. Authors believe that finding these dependencies will allow to calculate the estimated life span of data science article. Furthermore, as replicability helps with applying proposed methods and tools, its approximated level might be helpful in estimating usefulness of every scientific article. 1.2.3 CodeExtractoR package To examine sufficent number of articles, package CodeExtractoR has been used. Its main functions - extract_code_from_pdf() and extract_code_from_html(), automatically extract code snippets from pdf and html files into R files. # install.packages(&quot;devtools&quot;) devtools::install_github(&quot;MrDomani/CodeExtractoR&quot;) library(CodeExtractoR) # Path to file path_file &lt;- &quot;filepath.html&quot; # Path to R file output output &lt;- &quot;output.R&quot; # Extracting code extract_code_from_html(input_file = path_file, output_file = output, console_char = &quot;&gt;&quot;) To extract code from pdf, API key to https://cloudconvert.com/ is required. 1.2.4 Methodology The first issue that should be touched upon, while considering the methodology behind preparing this article, is the scale used to assess the reproducibility of the papers. In the Introduction it was already mentioned how the scale was created but a more detailed description is required. The authors decided that the scale should consist of 4 levels (from 0 up to 3): The 0 grade was given in case when no chunk of code gave the anticipated results and no figure was reproduced successfully (in practice such situation occurred mainly when the package described in the article was no more available). The 1 grade means that at least one example gave the results that the authors waited for, while it also includes situations when about half of the code in the chunks behaved as expected. The 2 grade was awarded to the articles that were reproducible “in the majority” what also means that they were not reproducible in 100%. The 3 grade was received by the articles that were fully reproducible and no problems were encountered in the process. Such a result was highly anticipated by the authors but the criteria for this grade were rather strict. The second issue that also played a vital role in the authors’ work was the scope of the analysis. It was decided that in order to maintain “other thing equal” according to a well-known Latin phrase “ceteris paribus” only one online journal – The R Journal – was taken into account. Being equipped with a tool for faster reproduction of articles – the CodeExtractoR package – the authors agreed to examine about 20 articles that were published across a few years. Such a great number of papers meant that the approach taken could be described as holistic. It is also worth mentioning that usually 30 articles from each year were analyzed (at least whenever it was possible). Finally, it should be noted that in the case of the date when they were published the examined articles range from 2009 up to 2019. The third and final issue that should be considered in this part of the article focuses on the measures undertaken by the authors in order to tackle the problem of biased assessment. Although the scale that was proposed was not totally dependent on the person who was using it, it still left someplace for personal liking and disliking of the paper. As a way of marginalization of this trend the authors have taken part in many conversations when the facts that led to specific grading of the articles were shared. This enabled awarding the grades even more fairly. However, the final measure was much more simple and it was believed to be much more effective as well when compared to the previous one. The articles for each year were divided into 3 groups and assigned to one author each. Thus each author has examined the papers from the whole range of release dates that were taken into account. 1.2.5 Results Specific results are presented in Table 1.1, which shows the number of examined articles from 2009 up to 2019, grouped by received grade. The column “Grade” represents the 0 - III scale of reproducibility. The rest of the columns shows a number of papers that achieved a particular grade in each year. Grade 09’ 10’ 11’ 12’ 13’ 14’ 15’ 16’ 17’ 18’ 19’ 0 3 2 6 4 6 8 5 9 5 9 9 I 4 2 0 2 3 6 6 10 3 2 2 II 3 7 8 4 13 6 10 7 14 6 6 III 6 7 6 5 8 10 9 4 8 13 13 SUM 16 18 20 15 30 30 30 30 30 30 30 Figure 1.1 shows the results in the original 4-level scale. Although, the number of papers varies throughout the years in every reproducibility class, it is observable that intermediate ones - I and II, are less common in the oldest and newest papers. In addition, results in 2018 and 2019 are identical. It is important to remember that from 2009 to 2012, the overall number of papers oscillated around 18 per year. After 2013, the number of researched articles was constant. FIGURE 1.1: Number of papers by class and publication year After calculating the percentage of each class in a specific year ( Figure 1.2, Figure 1.3), it is observed, that in the two oldest examined years - 2009 and 2010 - a ratio of completely unreproducible papers (with 0 or I class ) is surprisingly low. Furthermore, papers with III class of reproducibility are nearly 40% of all articles in these years. FIGURE 1.2: Ratio of each class throughout years Except for 2019, 2018 and 2016, percentage of fully reproducible papers (III class) is stable. In the newest articles, this percentage is slightly higher. Year 2016 is the only one, where unreproducible papers were in the majority. Only in 3 cases, percentage of reproducible articles dropped below 60%. FIGURE 1.3: Summarized results throughout years 1.2.6 Summary and conclusions This article has presented an analysis of scientific papers reproducibility in the context of publication date. As a part of this work, 280 articles from RJournal have been verified. Collected results show that even though recently published articles are more likely to be fully reproducible than older ones, time does not affect reproducibility as we could have expected. A higher ratio of complete reproducibility occurs only for papers from the last 2 years. In articles from 2009 – 2017 there is no evident correlation between reproducibility and year of publication. However, several patterns are possible to recognize. Firstly, except for the year 2016, in each year percentage of reproducible articles (class II or III) was higher than unreproducible ones (class 0 or I). Secondly, the proportion of fully reproducible articles seems to oscillate between 30% - 40%. Moreover, the earliest years have a similar or even lower ratio of complete irreproducibility than later ones. These unexpected observations might be explained by the fact that older packages appear to be less dependent on other libraries. Additionally, modern packages contain more complex and dedicated functions, therefore they are more vulnerable to changes. Undoubtedly, this paper does not cover all aspects of scientific articles obsolescence, however, we believe that the data we have collected might be a valuable starting point for further research. References "],
["ways-to-reproduce-articles-in-terms-of-release-date-and-magazine.html", "1.3 Ways to reproduce articles in terms of release date and magazine", " 1.3 Ways to reproduce articles in terms of release date and magazine Authors: Mikołaj Malec, Maciej Paczóski, Bartosz Rożek 1.3.1 Abstract Reproducibility is a topic that is quite diminished in today’s science world. Scientific articles should be current as long as possible. Their results should be achievable by readers and be the same. Thanks to that science and business world can take advantage of them. The more article is difficult to reproduce, the chance of using knowledge coming from it is smaller. Many researchers tried to define or give principles for reproducibility. There is an article published in 2016: “What does research reproducibility mean?” (Goodman, Fanelli, and Ioannidis 2016b) which tried to warn about reproducibility crisis. Article in 2017: “Computational reproducibility in archaeological research: basic principles and a case study of their implementation” (Marwick 2016), compared computational reproducibility to archaeological research and give guidelines for researches to use reproducibility in computing research. But these are just two of many articles about reproducibility. Some articles are about tools and techniques for computational reproducibility (Piccolo and Frampton 2016). They encourage researchers to compute data using environments like Jupiter (Thomas et al. 2016) or R markdown (Marwick, Boettiger, and Mullen 2017). Thanks to that readers can reproduce finding on their own. What’s new about our approach to the subject of reproducibility is focusing on how can release date and magazine affect the amount of work needed to fully reproduce code or is it even possible. A comprehensive comparison of scientific magazines in terms of reproducibility is yet to be created and this article is our best effort to make it happen. 1.3.2 Methodology Our work was focused on analyzing journals as a whole, therefore we decided to choose three well-known magazines: R Journal, JMLR Machine Learning Open Source Software, and Journal of Statistical Software. Our aim was to define features that incense the reproducibility of a journal’s article as well as the ones that impede it. We would like to create a list of rules that improve and enable reproducibility. We divided the magazines, every one of us gets one to look up. Every one of us gets the different approaches to their magazine. Mostly it considered getting familiar with the magazine, like their style or rules for applying the article, but most importantly we looked at random articles to get a fill how one could replicate the results and would it worked. Because reproducibility can be achieved in many ways, we didn’t make specific rules when an article can be replicated. An article is more likely to be able to be reproducible if it has such features. We weren’t keen on replicating whole articles. We were more interested in looking for features, which would help in the reproducibility of an article. Features mostly took into consideration: Source code included in an article or added as a file ready to download Accessibility of data used in an article Availability of packages presented and used in an article Information about device and environment We used many methods to extract the quantity and quality of these features from magazines. In some cases, information was easy to get manually, so no more code was written. In another case, a scraper written in Python helped us collect the required information. Scraper for R and if the is on Cran respiratory. If not it checks when the package was (for how long was this article was reproducible). In the case of JMLR information about the link to author respiratory was by copying text on and counting words ‘abs’ and ‘code’. All articles have ‘[abs]’ and some have ‘[code]’ if they have a link to the author’s respiratory. We compare them by pointing their strengths and weakness, which we discovered. It will be automatically a set of guidelines for magazine publishers and researches which features should be included so their future articles would be reproducible easily. 1.3.3 Results R Journal ‘The R Journal is the open access, refereed journal of the R project for statistical computing. It features short to medium length articles covering topics that should be of interest to users or developers of R. The R Journal intends to reach a wide audience and have a thorough review process. Papers are expected to be reasonably short, clearly written, not too technical, and of course, focused on R’ ~ R Journal. It is a well-known magazine, which has a clear page and it is easy to find every article in the archive which contains all previous releases. Every article has its page with features such as short description, necessary CRAN packages, and supplementary materials. A list of packages contains links to CRAN. Here the first problem is spotted - not every package has the current version on CRAN or the page has error 404 - “Object not found”. When the package is not easy to download, install, and use, the reproducibility is significantly harder and when it is unobtainable reproducibility is simply impossible. The part of packages available on CRAN is shown in Figure ###. As it is said on the main page of R Journal - ‘Authors of refereed articles should take care to (…) provide code examples that are reproducible’, in 2016 ‘supplementary materials’ were introduced. It contains a link to download additional files such as R scripts, that allow the reader to reproduce things contained in the article e.g. plots and calculations. The link is located in every article’s page and part of articles having it is higher every consecutive year (shown in Figure 2). Figure 1 Figure 2 Journal of Statistical Software As stated on the journal website “The Journal of Statistical Software (JSS) publishes open-source software and corresponding reproducible articles…” articles published by JSS should be easy to reproduce. By going through all available articles we want to examine how much effort is put into keeping JSS articles reproducible. Journal of Statistical Software first started publishing in 1997 and gradually increased both number of articles per year and the percentage of articles discussing R. Starting from 13 articles per year they peaked in 2011 almost crossing the barrier of 100 articles published that year (Figure 3). Figure 3 The thankfully growing scale of the journal did not lead to less attention given to the topic of reproducibility. Each of 744 articles discussing R packages is published alongside with R file containing example code and described version of R package. However as stated at the JSS website, the journal does not serve as a software repository for software distribution, so it is not possible to update code or articles once they were published. That is why JSS suggests that software should also be available in the standard open-source repository. While trying to reproduce the article, it is discouraging to discover that the described package is no longer available on CRAN. Nevertheless, packages appearing in JSS articles are mostly available to download from CRAN (Figure 4). Figure 4 Another interesting aspect of the reproducibility of R related articles is the difference between the date of submission and publication. In other words how outdated was article when it appeared on JSS and if it is more difficult to reproduce such articles. Articles published and 2020 were on average submitted almost 3 years ago, which may result in some issues while trying to reproduce given code (Figure 5). Figure 5 In a perfect scenario, the version of the package on which the article is based should always work with example code, therefore any updates on CRAN would lower the reproducibility. However in reality many things can change - dependencies can get updated or the current R version may change. Based on that we will look at package updates like the ability to adapt and keep software compatible with past code and treat them as feature increasing reproducibility. As presented on figure average time since the last update of the package described in the JSS article is not unreasonable and should not pose a threat to reproducibility (Figure 6). Figure 6 JMLR The Journal of Machine Learning Research (JMLR) is an international forum for the electronic and paper publication of high-quality scientific articles in all areas of machine learning. All published works are available free of charge on the Internet. For the most part, the authors do not include code or references to their sites. Magazine administrators encourage you to post your code. Unfortunately, few authors apply to it. This does not mean that content of the articles is unreliable, but it makes articles unreproducible. The items included in the articles that should be reproduced are experiments on data sets and the code that was used to create the final tables/charts. However, some articles have a reference [code]. Most often it refers to the author’s repository (e.g. GitHub). The reproduction of such an article then depends on the author and how he created his code. Links began to appear from 2011 (the first article was in 2001). Over the years, there are more and more articles with links with code, but these are still a minority. Administrators have created a special tab with articles that have a link with a code, probably to popularise such links. Fortunately in the newest article (still in production) has record links with code. More than 20 and near half of the posted articles have this link. There could be a drastic change for this magazine. Figure 7 Summary and conclusions Careful analysis of mentioned journals revealed which features while writing and publishing article lead to boost to reproducibility and which make the task of code reproducing more difficult. Good practices such as publishing articles alongside R file containing example code and supplementary data. The unified way of naming and hosting those files is also welcomed. Solutions like distributing articles including links to authors Github profile or described package CRAN website are increasing journal reproducibility. The Journal board may also take actions like encouraging authors to ensure if the package is compatible with published code or simply supplying readers with easily accessible contact with the writer. One thing that journals lack is a clear way of informing readers whether the article is ready to be reproduced, for example, if all required files and data are available for download. Below is our vision of accomplishing that (Figure 8). Figure 8 References "],
["reproducibility-of-outdated-articles-about-up-to-date-r-packages.html", "1.4 Reproducibility of outdated articles about up-to-date R packages", " 1.4 Reproducibility of outdated articles about up-to-date R packages Authors: Zuzanna Mróz, Aleksander Podsiad, Michał Wdowski (Warsaw University of Technology) 1.4.1 Abstract The inability to reproduce scientific articles may be due to the passage of time. Factors such as changes in data or software updates - whether author-dependent or not - may make it difficult to reproduce the results after a long time. It would seem obvious that old scientific articles about R-packages are usually difficult to reproduce. But what if this package is still supported? In what way the continuous support changes the possibility of reproducing these articles? In this article we will look at examples of still updated R-packages from 2010 or older in order to analyse the type and degree of changes. 1.4.2 Introduction and Motivation The problem of the inability to reproduce the results of research presented in a scientific article may result from a number of reasons - at each stage of design, implementation, analysis and description of research results we must remember the problem of reproducibility - without sufficient attention paid to it, there is no chance to ensure the possibility of reproducing the results obtained by one team at a later time and by other people who often do not have full knowledge of the scope presented in the article. Reproducibility is a problem in both business and science. Science, because it allows credibility of research results (McNutt 2014). Business, because we care about the correct operation of technology in any environment (Anda, Sjøberg, and Mockus 2009). As cited from “What does research reproducibility mean?” (Goodman, Fanelli, and Ioannidis 2016b); “Although the importance of multiple studies corroborating a given result is acknowledged in virtually all of the sciences, the modern use of “reproducible research” was originally applied not to corroboration, but to transparency, with application in the computational sciences. Computer scientist Jon Claerbout coined the term and associated it with a software platform and set of procedures that permit the reader of a paper to see the entire processing trail from the raw data and code to figures and tables. This concept has been carried forward into many data-intensive domains, including epidemiology, computational biology, economics, and clinical trials. According to a U.S. National Science Foundation (NSF) subcommittee on replicability in science, “reproducibility refers to the ability of a researcher to duplicate the results of a prior study using the same materials as were used by the original investigator. That is, a second researcher might use the same raw data to build the same analysis files and implement the same statistical analysis in an attempt to yield the same results…. Reproducibility is a minimum necessary condition for a finding to be believable and informative.” 1.4.3 Related Work Other notable articles about reproducibility include “Variability and Reproducibility in Software Engineering: A Study of Four Companies that Developed the Same System” (Anda, Sjøberg, and Mockus 2009), “Reproducible Research in Computational Science” (Peng 2011) and “A statistical definition for reproducibility and replicability” (Patil, Peng, and Leek 2016). “Variability and Reproducibility in Software Engineering: A Study of Four Companies that Developed the Same System” focuses on the variability and reproducibility of the outcome of complete software development projects that were carried out by professional developers. “Reproducible Research in Computational Science” is about limitations in our ability to evaluate published findings and how reproducibility has the potential to serve as a minimum standard for judging scientific claims when full independent replication of a study is not possible. “A statistical definition for reproducibility and replicability” provides formal and informal definitions of scientific studies, reproducibility, and replicability that can be used to clarify discussions around these concepts in the scientific and popular press. In our article we focus on the reproduction of old scientific articles on R and packages, which are still being developed. We want to explore how the passage of time affects the ability to reproduce results using the currently available updated tools. We are therefore testing backward compatibility for different packages and checking what affects the reproducibility of the code. We were unable to find scientific articles on this exact issue. There are articles that give ways to measure reproducibility, as well as articles about packages that help with reproduction. But there are yet no articles that summarize the set of packages in terms of their reproducibility. 1.4.4 Methodology We have checked 13 articles with 16 R packages from at least 10 years ago to ensure that the code chunks match these categories: ade4: Implementing the Duality Diagram for Ecologists (Dray and Dufour 2007) AdMit: Adaptive Mixtures of Student-t Distributions (Ardia, Hoogerheide, and Dijk 2009) asympTest: A Simple R Package for Classical Parametric Statistical Tests and Confidence Intervals in Large Samples (Coeurjolly et al. 2009) bio.infer: Maximum Likelihood Method for Predicting Environmental Conditions from Assemblage Composition (Yuan 2007) deSolve, bvpSolve, ReacTran and RootSolve: R packages introducing methods of solving differential equations in R (Soetaert, Petzoldt, and Setzer 2010) EMD: Empirical Mode Decomposition and Hilbert Spectrum (Kim and Oh 2009) mvtnorm: New Numerical Algorithm for Multivariate Normal Probabilities (Mi, Miwa, and Hothorn 2009) neuralnet: Training of Neural Networks (Günther and Fritsch 2010) party: A New, Conditional Variable-Importance Measure for Random Forests Available in the party Package (Strobl, Hothorn, and Zeileis 2009) pls: principal Component and Partial Least Squares Regression in R (Mevik and Wehrens 2007) PMML: An Open Standard for Sharing Models (Guazzelli et al. 2009) tmvtnorm: A Package for the TruncatedMultivariate Normal Distribution (Wilhelm and Manjunath 2010) untb: an R Package For Simulating Ecological Drift Under the Unified Neutral Theory of Biodiversity (Hankin 2007) All articles come from the R Journal and Journal of Statistical Software. In our research on the subject we have decided to divide the code from the articles into chunks, according to the principle that each chunk has its own output, to which we give an evaluation according to the criteria we have set. In the process of testing and reproducing various articles, we have identified five categories, and marked them as follows: FULLY REP (no reproductive problems - the results were identical to original results shown in the article), MOSTLY REP (when the results were not ideally identical to the original, but in our opinion the chunks were working according to their purpose in the context of their article), HAD TO CHANGE STH (when we had to modify the code to produce correct results that will work in our current R version and can be neatly displayed in a document generated by markdown), HAD TO CHANGE &amp; STILL SOMEWHAT DIFFERENT (the code had to be changed and the results were not perfect, but they were correct in the terms of the aforementioned category MOSTLY REP, but we could consider them as satisfactory), NO REP (no reproducibility, either due to changes through time or problems with the article that had already been there, regardless of differences in R across years). These criteria can be considered not subjective, but setting such boundaries does not cause confusion in categorisation, thus we decided to use them in order to research and describe the introduced number of articles about R packages from at least 10 years ago. In some of the articles we found specific types of problems: There was no access to data or objects referred to in later calculations, The results were similar to the original, but the differences were most often due to the random generation of objects. This error was usually reduced later, when the package created some kind of data summary - then the result had a very small relative error with the original result. The names of individual variables or some of their attributes changed (e.g. column names in the data frame). When data or objects were unaccessible and there could not be found any alternative or history of the dataset, we classified this chunk as NO REP. What is more, in most cases chunks dependent on such objects automatically became NO REP too. However, if the code was only partially dependent on the lost dataset, it was classified as MOSTLY REP. When we stumbled upon problems with randomly generated objects, where the values were obviously different, but after aggregating the data summaries were close to original, such chunks were marked as MOSTLY REP. As we were reproducing various articles, this has become quite significant problem that a large number of publications were struggling with. If data could be somehow fixed - for example by changing column names, it was given HAD TO CHANGE STH mark, and the dependent chunks’ marks were not influenced by this change, which theoretically was allowing them to be marked even as FULLY REP. 1.4.5 Results Here can be seen the in-depth report. Below are the summarised results of our research. As it can be seen, the vast majority of chunks are fully reproducible. Even if the chunk is not executed identically to the original one, in most cases it differs only slightly, and the package itself serves its purpose. 88.3% (121/137) of the chunks are executed perfectly or correctly (within our subjective category of being acceptably incorrect), while 93.4% (128/137) of the chunks are working well enough not to throw errors. In practice, only 6.6% (9/137) of chunks were completely irreproducible, which would seem surprising for more-than-a-decade-old articles. However, given that we have focused particularly on packages that are still being developed, this is quite a feasible result. This can be seen quite clearly by the percentage of the chunks that required minor changes or slightly differed from the results shown in the article - there were 33.6% (46/137), which is clearly a result of the updates or changes that occured in the ever evolving R environment. Of course during our research we stumbled upon numerous packages that have not been updated since years or that have even been deleted from CRAN repository, so they were not within our field of interest. Nonetheless, we would like to emphasize that the results should not suggest that one-decade-old articles are reproducible. 1.4.6 Summary and conclusions To sum up, in most cases the packages we examined performed their tasks correctly. The packages themselves have of course changed, but its idea remained the same. Usually new features or improvements were added, but the idea behind the package was the same as it used to be. As a result, most of the packages still managed to cope with the problems of the old ones, in reproduction usually suffering from missing external data or unavoidable changes in the R language itself. All in all, almost in all cases the package does the job in spirit, differing from its old good ways only in vague confusion caused by neverending winds of change. It can therefore be concluded that most packages that we’ve checked are fully backward compatible, which is good programming practice. In order to increase the reproducibility of articles, this should definitely be taken care of. Additionally, authors should include supplements to their articles, that always help you understand and copy the code. References "],
["correlation-between-reproducibility-of-research-papers-and-their-objective.html", "1.5 Correlation between reproducibility of research papers and their objective", " 1.5 Correlation between reproducibility of research papers and their objective Authors: Przemysław Chojecki, Kacper Staroń, Jakub Szypuła (Warsaw University of Technology) 1.5.1 Abstract Reproducibility is a quality of research crucial for modern science. But is perfect reproducibility an ultimate requirement for every element of scientific process to be valid? Not necessarily, it is possible that important factor in this case will be purpose of given part of research. In this article we will attempt to provide a system of analyzing reproducibility of scientific papers involving mainly the purpose of theirs elements and search for dependencies between those variables. 1.5.2 Introduction and Motivation It is common knowledge that reproducibility is a way for science to evolve. It is the heart of the scientific method to revisit pre-existing measurements and to try to reproduce its results. However, the term „reproducibility” itself, as well it is crucial to the scientific methodology, it can be also universal at the expense of unambiguousness and usability. For the purpose of this paper we will have recourse to the definition introduced by ACM: “Reproducibility - The measurement can be obtained with stated precision by a different team, a different measuring system, in a different location on multiple trials. For computational experiments, this means that an independent group can obtain the same result using artefacts which they develop completely independently”. This particular definition illustrates perfectly how in the course of establishing the meaning of term „Reproducibility”, the level of importance of auxiliary measurements and settings of the experiment to the overall results is omitted. It is notably significant misconception, especially in the experiments from the field of computational science, when reproducing or even maintaining precise operating conditions is usually impossible. In the following chapters we will attempt to perform an analysis of reproducibility of the papers submitted to the RJournal, regarding especially presumed objectives of enclosed auxiliary computations and artifacts (i. e. code chunks) in overarching structure of a given paper. ### Related Work Although there are many research papers related to this article, the following three could be perceived as a “basis” for our study. 1. (Fern’andez et al. 2019) provide a definition of reproducibility this article uses, and distinguish it from replicability. 2. (Goodman, Fanelli, and Ioannidis 2016a) define multiple interpretations of reproducibility. It further divides and classifies reproducibility, and provides a basis on how one can do it. In their search the authors have not encountered other research papers that study the aspect of reproducibility this article focuses on. If said papers do not actually exist, then this article could provide insights on previously unexamined aspects of reproducibility. 1.5.3 Methodology 1.5.3.1 General approach The methodology presented in the following section is a direct consequence of how we approach scientific article as an experience devised by an author for the reader. Our focus is to determine, how author alter this experience by using code chunks instead of plain text. In other words, we ask a question “Why have the authors used the R code?” Assumption that execution of enclosed code is an integral part of said experience and by extension code chunks supposed to be reproducible for reader to percept the article as intended seems to be reasonable. However it may not be correct in every case. Let us consider a situation, where generated output is essential to the thesis stated in the article. If the code is irreproducible, the reader cannot believe the authors. It devastates their credibility. But what if a goal of the code was to illustrate general tendency in data and output is reproducible only to some degree? Then it may still fulfill its purpose in the article and the lack of full reproducibility does not intefere with experience for a reader. Following this thought process inevitably leads to new questions, f.e. is it possible for executable code to serve its purpose in article while being completely unreproducible? To explore this topic we decided to focus on objectives of code in scientific papers. We have decided that the most accurate and reliable way of finding the purposes of code chunks in scientific articles is by examples. That is why we have analyzed over \\(30\\) papers from The R Journal [https://journal.r-project.org]. We have gathered the code chunks into groups and considered a three degree of purpose: the whole article, the group of chunks, and the single chunk of code. We have prepared a list of possible purposes for every level and assign them to our examples. The whole list of purposes is explained in the next chapter. Then we have produced our measure of reproducibility, which is also detailed later. 1.5.3.2 Objectives Since this article is centred around objectives, our understanding of them is of utmost importance. That is why we divided them into three categories, further divided into classes. We described them in detail in the sections below. To limit our individual biases in assessing what the intended objective is, we referred to relevant paragraphs in the original research paper. It has to be noted, that an object (let it be an article, a group of chunks or a chunk) can have more than one objective. 1.5.3.2.1 Main Objectives Both code chunks by themselves and performed computations corresponding with them can provide wide variety of information. However we can identify and describe reasons why the programming language is present in general in a given paper. All code chunks serve together as a vital element supporting narration of the article and its objective usually can be identified with main objective of narration in article as a whole. We systematized main objectives and grouped them into the following general objectives: package overview - presenting general structure of specific package, providing example of aplications implemented functions and discussing its performance object construction - presenting process of constructing and developing virtual object introduction to subject - using performing code as a complement to educational resource concerning given topic method implementation - presenting in-depth process of developing solution and explaining it addressing an issue - presenting solution to specific scientific/computational problem error identification - recognising and presenting error in existing package, possibly submit alternative solution covering mentioned error 1.5.3.2.2 Intermediate Objectives Since code chunks in research papers seldom appear on their own, but rather are part of a larger group of chunks serving a certain purpose. For instance, let there be three chunks, named A, B, and C. Let A load data from an external source, B modifies the data and extract a subset of it, and let C generate a plot, based on the data obtained from the two previous chunks. While each chunk has its own distinct objective, together they have at least common one - in this example this is generating a plot. Plot generated by A, B and C can be used to compare between performance of various functions. These chunk group’s objectives we define as intermediate objectives. We systematized intermediate objectives and grouped them into the following general objectives: package usage - examples on how does an R package operate, how one can use functions provided by the package, in what manner output is generated etc. practical use - underscoring of the practical usage of code used in code chunks in that group. method comparison - comparison between functions and/or methods. For example, a microbenchmark between base R functions and functions from a presented package. generating output - generating an output, for example plots, .csv files, images etc. presenting specification - presentation on what package specification looks like. data preparation - preparation of data that may be used later in the paper. This includes both loading the data and modifying it. occurrence demonstration - demonstration of an occurrence described earlier in the article. introduction to analysis - introduction to analysing a certain topic and data related to it. possible error - description of a possible error one can encounter and how one can solve it. 1.5.3.2.3 Chunk Objectives Each chunk has a role - it serves one or more purposes, which we define as chunk objectives. We systematized chunk objectives and grouped them into the following general objectives: aesthethic example - an example showcasing how output generated by the code chunk looks like. functional example - an example of how functions showcased in the chunk work. instruction - an instruction on how one achieves desired effect using R code. instruction (package) - same as above, but using functions from the package introduced in the article containing the chunk. data preparation - preparation of data for the following chunks. data exploration - merging, subsetting, summarisation of data and other types of data manipulation used in order to explore data. foreign error - turning attention to an error in work done by other author(s). solving a problem - description of how one solves a given problem using R code. data modification - modifying data in order to achieve desired effect. presentation of results - presenting result of computation within the article. This can be done by specific summarising functions (e.g. summarise) or simply printing base R vectors. plot - plotting graphs in the article. generating files - generation of files, this includes graphical, text and other files. results processing - processing of results in order to improve their aesthethic value or to make them more readable. erroneous action - presenting code that does not run properly as an example of an action should be avoided. uncallable code - code that, in principle, is impossible to run. This includes pseudocode. comparison to foreign work - comparation of authors’ work (functions, methods etc.) with work of others, that achieves the same effect. This includes benchmark performance comparisons. empirical proof - validation of what is mathematically described in earlier sections. 1.5.3.3 Reproducibility The sole purpose of this paper is to explore interactions between purposes of code chunks usage and reproducibility aberrations. That requires a system of classification of reproducibility. We provide simple categorization of forms of reproducibility into the 6 types. This classification system shall serve as a tool for initial phase of our analysis, thus it is not directly involving purpose of discussed code at this stage. 1 - perfect reproducibility - code perform flawlessly and after initial configuration precise output is recreated 2 - margin of error - after initial configuration code provides output matching expectations within acceptable margin of error (f.e. difference in rounded decimals, default parameters of generated graphics) 3 - editorial correction - code requires minor corrections to be executable and viable due to editorial error or changes in naming conventions 4 - environment setup - code to execute properly requires major and time-consuming setup and environment changes or may be not able to provide expected results at all 5 - unreproducible - code undoubtedly cannot be reproduced (f.e. due to unavailable data, unavailable package, unsupported fatal error) -1 - missing point of reference - article does not provide (or vaguely provides) expected performence and determining reproducilibity is impossible 1.5.3.4 Tables description For analysis purposes we have gathered data from randomly selected scientific papers in The R Journal and arranged them into 3 distinct tables stored in 3 .csv files. 1.5.3.4.1 Table of articles Every row represents one article contains information about sum of lengths of the code chunks in given article and its general purpose, one-hot encoded (6 categories). length package overview object construction … error identification 15 TRUE FALSE … FALSE 2 TRUE TRUE … FALSE 9 TRUE FALSE … FALSE 1.5.3.4.2 Table of groups Every row represents one group of chunks and cointains information about sum of lengths of the code chunks in given group and its general purpose, one-hot encoded (9 categories). length package usage practical use … possible error 15 TRUE FALSE … FALSE 2 TRUE TRUE … FALSE 9 TRUE FALSE … FALSE 1.5.3.4.3 Table of chunks Every row represents one chunk and contains information its length, reproducibility and its general purpose, one-hot encoded (17 categories). length reproducibility scale aesthethic example … empirical proof 5 2 FALSE … FALSE 5 2 FALSE … FALSE 5 4 FALSE … FALSE 1.5.3.4.4 Length assessment To objectively determine a length of code we have decided to count it according to the following rules: * ignore all empty and commented lines * ignore assignments, unless it contains the execution of a function * ignore executions of functions library and data * ignore lines containing only parenthesis 1.5.4 Results In this section, we perform a basic analysis of the data we have gathered during our study. 1.5.4.1 Preliminary analysis The data can be loaded by: art &lt;- read.csv(&quot;./1-6-files/articles.csv&quot;) chnk &lt;- read.csv(&quot;./1-6-files/chunks.csv&quot;) grp &lt;- read.csv(&quot;./1-6-files/groups.csv&quot;) library(dplyr) library(ggplot2) library(reshape2) Frequency of every reproducibility score can be obtained by: # frequency of every reproducibility score chnk %&gt;% pull(reproducibility.scale) %&gt;% table() ## . ## -1 0 1 2 3 4 5 ## 33 1 194 45 11 13 41 As we can see, most chunks were fully reproducible, with total areproducibility being the second most common occurence. What further can be analysed is how reproducible is each objective, how often it occurs and the total length of each objective (i.e. sum of length in lines of all chunks containing specific objective). This analysis can be achieved by the following code: # mean reproduciblity scale score for reach objective mean_repr &lt;- chnk[-1,] %&gt;% sapply(function(x){x*chnk[-1,]$reproducibility.scale}) %&gt;% data.frame %&gt;% select(-reproducibility.scale, -length) mean_repr[mean_repr==0] &lt;- NA mean_repr[mean_repr==-1] &lt;- NA mean_repr &lt;- summarize_if(mean_repr, is.numeric, mean, na.rm = TRUE) %&gt;% melt colnames(mean_repr)[2] &lt;- &quot;mean_repr_score&quot; # length for every objective licznik &lt;- chnk %&gt;% select(-length, -reproducibility.scale) %&gt;% summarise_all(sum) %&gt;% melt tot_length &lt;- chnk %&gt;% select(-reproducibility.scale) %&gt;% melt(id.vars = &quot;length&quot;) %&gt;% filter(value) %&gt;% select(-value) %&gt;% group_by(variable) %&gt;% summarise(total_length = sum(length)) mean_repr &lt;- inner_join(inner_join(tot_length, licznik), mean_repr, by = &quot;variable&quot;) arrange(mean_repr, mean_repr_score) ## [90m# A tibble: 17 x 4[39m ## variable total_length value mean_repr_score ## [3m[90m&lt;fct&gt;[39m[23m [3m[90m&lt;int&gt;[39m[23m [3m[90m&lt;int&gt;[39m[23m [3m[90m&lt;dbl&gt;[39m[23m ## [90m 1[39m results.processi… 5 4 1 ## [90m 2[39m presentation.of.… 174 74 1.21 ## [90m 3[39m foreign.error 34 6 1.33 ## [90m 4[39m data.exploration 43 13 1.42 ## [90m 5[39m data.preparation 365 59 1.43 ## [90m 6[39m solving.a.problem 247 40 1.44 ## [90m 7[39m erroneous.action 7 3 1.5 ## [90m 8[39m empirical.proof 78 19 1.68 ## [90m 9[39m instruction..pac… 513 124 1.70 ## [90m10[39m instruction 264 35 1.76 ## [90m11[39m data.modification 98 23 1.83 ## [90m12[39m comparison.to.fo… 42 7 1.86 ## [90m13[39m functional.examp… 456 129 2.26 ## [90m14[39m plot 247 60 2.36 ## [90m15[39m aesthethic.examp… 182 41 2.46 ## [90m16[39m generating.files 64 19 2.83 ## [90m17[39m uncallable.code 74 21 [31mNaN[39m As it is shown, objectives vary in their mean reproducibility score with examples scoring worse than instructions and results presentation better than plots. On the following plot higher the value of variable - worst level of reproducibility. Exact values should not be interpreted as continuous variable, legitimite information can be obtained by analyzing order of categories sorted with regard to their avarage reproducibility score. On this plot we can observe difference in values of plot and presentation of results. Clearly plot is more likely to be irreproducible than a showcase of results in any other way Moreover, both aesthetic example and functional example values are almost the highest, what may indicate that in general authors put relatively low effort into them, since their perfect reproducibility may not be essential to the article as a whole. During our analysis result processing chunks were encountered only \\(4\\) times and all of them were reproducible, which in our opinion is not a basis for stating any meanigful conclusions. 1.5.4.2 Frequency of objectives Next, we will visualize the frequency of occurences of every objective, for every category. text text text After the objectives are sorted with regard to their frequency, one can see that they appear to be similar to 1/x function plot. It indicates that certain objectives are far more frequent than others, while the less popular ones remain at similar levels of frequency. 1.5.4.3 Average length Average length of a code chunk with a specific objective: erroneous action generally requiring a small amount of code suggests that standard aproach to this objective is simply execution of one line of code, just to trigger the error. Average length of a group of chunks with a specific objective: The code for method comparison has to show a solution offered by a described package as well as how to approach such a problem with other sources. What is more, the author has to show the differences between them, which as we can see usually requires additional lines of code. Average length of all of the code chunks in an article with a specific objective: Clearly error identifaction articles include the gratest amount of code, probably for the same reasons as porownanie.metod in the group objectives. It can be space and timeconsuming to showcase the solution to a problem with a package, without a package and point out precisely the difference between them. 1.5.4.4 Objectives correlations matrices Two important conclusions can be drawn from the correlation matrix. First - there is no strong correlation between individual objectives, which may imply that the objectives proposed by us do not overlap heavily. Second - correlation between the reproducibility score and individual objectives is also not that high. Because of that we cannot be certain whether there is a clear link between reproducibility score and objective of any given chunk. However, after closer inspection of the following matrices, we can formulate few additional general observations. On featured chunks objectives correalation matrix we can observe how: * There is a positive correlation between comparison to foreign work and empirical proof. However it is rather an effect of a small amount of data. There were only \\(7\\) instances of comparison to foreign work and \\(4\\) of them are also empirical proof. * We found no correlation between erroneous action and comparison to foreign work. When an author shows a problem in someone’s code or package, he does not compare it to his own. He would rather show it in the next chunk. On featured intermediate objectives correalation matrix we can observe how: * It is less likely to find a group that is both dzialanie.pakietu and przygotowanie.danych. It is because in the preparation of data most of the code is in plain R or other popular libraries and not in the one shown in the paper. * There is a big negative correlation between dzialanie.pakietu and prezentowanie.specyfikacji. When there is a code that is prezentowanie.specyfikacji, it is mostly a theoretical approach, while the dzialanie.pakietu underscore the practical approach. On featured main objectives correalation matrix we can observe how: * All of the researched article present a package, therefore the correlation is not defined for it. * Articles serving as an introduction to the topic are also implementing additional features. 1.5.5 Summary, conclusions and encouragement For purpose of research described in this article we developed an intuitive system of objectives classifaction of singular code chunks, groups of chunks and whole scientific papers concerning development of programming languageges (in this case - R). Initial analysis of the data acquired by using mentioned system may imply that lists of objectives proposed by us do not overlap heavily and therefore they indeed can be used as valid scientific method. To sum up the effects of applying this method for a selection of the scientific articles from The R Journal, correlation between the reproducibility score and individual objectives is not significantly high. It may indicate that dataset considered in this paper is simply not extensive enough. We sincerely encourage readers to perform their own analysis on wide variety of scientific articles not covered in this paper, preprocessed using developed system. References "],
["how-active-development-affects-reproducibility.html", "1.6 How active development affects reproducibility", " 1.6 How active development affects reproducibility Authors: Ngoc Anh Nguyen, Piotr Piątyszek, Marcin Łukaszyk (Warsaw University of Technology) 1.6.1 Abstract Researchers creating packages can keep them updated or abandon them. Our motivation was to examine how these two approaches impact the reproducibility of the scientific paper. Using ten different auxiliary variables like GitHub commits or stars count we tested which correlates with the most widespread classes of reproducibility issues we have found. As a result, we propose two guidelines for people designing packages. 1.6.2 Introduction and Motivation The key quality in measuring the outcome of researches and experiments is whether results in a paper can be attained by a different research team, using the same methods. Results presented in scientific articles may sometimes seem revolutionary, but there is very little use if it was just a single case impossible to reproduce. The closeness of agreement among repeated measurements of a variable made under the same operating conditions by different people, or over a period of time is what researches must bear in mind. Peng (2011) leading author of the commentary and an advocate for making research reproducible by others, insists reproducibility should be a minimal standard. There have been several reproducibility definitions proposed during the last decades. Gentleman and Temple Lang (2007) suggest that by reproducible research, we mean research papers with accompanying software tools that allow the reader to directly reproduce the results and employ the computational methods that are presented in the research paper. The second definition is according to Vandewalle, Kovacevic, and Vetterli (2009), research work is called reproducible if all information relevant to the work, including, but not limited to, text, data, and code, is made available, such that an independent researcher can reproduce the results. As said by LeVeque (2009) the idea of ‘reproducible research’ in scientific computing is to archive and make publicly available all the codes used to create a paper’s figures or tables, preferably in such a manner that readers can download the codes and run them to reproduce the results. All definitions converge into one consistent postulate - the data and code should be made available for others to view and use. The availability of all information related to research paper gives other investigators the opportunity to verify previously published findings, conduct alternative analyses of the same data, eliminate uninformed criticisms and most importantly - expedite the exchange of information among scientists. Reproducibility has great importance not only in the academic world but also it also plays a significant role in the business. The concept of technological dept is often used to describe the implied cost of additional rework caused by choosing an easy solution now instead of using a better approach that would take longer in software development. There are papers about using version control systems to provide reproducible results (Stanisic, Legrand, and Danjean 2015). The authors presented how we can manage to maintain our goal of reproducibility using Git and Org-Mode. Other researchers have created a software package that is designed to create reproducible data analysis (Fomel et al. 2013). They have created a package that contains computational modules, data processing scripts, and research papers. The package is build using the Unix principle to write programs that are simple and do well one thing. The program breaks big data analysis chains into small steps to ensure that everything is going in the right way. Some papers suggest using Docker to make sure our research can be reproduced (Hung et al. 2016). The main goal of our work is to measure the impact of the active development of packages on the reproducibility of scientific papers. Multiple authors (Rosenberg et al. 2020; Kitzes, Turek, and Deniz 2017) suggest using the version control system as a key feature in creating reproducible research. The second paper also provides evidence, that this is widely known. Git and GitHub were used in over 80% of cases. However, there are two kinds of using a version control system. An author can push software into the repository, to make it easily accessible and does not update it anymore. The second option is to keep the repository up-to-date and resolve users’ issues. We have not found any research on how these two approaches impact reproducibility. 1.6.3 Methodology Articles In our analysis, of reproducibility, we focused on articles introducing packages, that are actively developed on GitHub. Then we measure the reproducibility of an article using two versions of the package: current and the first after publication date to get the answer on the question, what if a package was never updated. In some cases, when it seems appropriate we used the last before publication. We selected 18 articles that were posted on R journal, that are on cran, are developed on GitHub, have code included to reproducibility, and doesn’t have too much impact on R environment. Measures of reproducibility We measured how many examples aren’t reproducible using these two versions. We categorized articles into 3 types of reproducibility: 1. The article is reproducible, minor differences can happen (e.g. different formating). 2. There are differences in function names, other packages that the article uses don’t work but at least half of it works. 3. Everything that doesn’t match 1 or 2. It means that the article is not reproducible. We have counted the three most common issues in each article: 1. Names - function or variable name has to be changed 2. API - way of using a function or their arguments has changed 3. Result - output differs Using these we can compare specific issues in the current and old versions of the package. Auxiliary variables To measure how a package is developed, we used several auxiliary variables from GitHub and CRAN collected on 2019-05-19: number of stars number of subscribers number of contributors number of issues number of open issues added and deleted lines since the publication date commits number since the publication date using Continuous Integration versions on CRAN since the publication date 1.6.4 Results Tested packages TABLE 1.1: Tested packages with measured reproducibility package old.version old.reproducibility new.reproducibility old.names.issues new.names.issues old.api.issues new.api.issues old.result.issues new.result.issues fanplot 3.4.1 1 1 0 0 0 0 0 0 cmvnorm 1.0-3 1 1 0 0 0 0 0 0 factorplot 1.1 1 1 0 0 0 0 3 3 FactoMineR 1.3 1 1 0 0 0 0 0 0 betategarch 3 1 1 0 0 0 0 6 6 tempdisagg 0.22 1 1 0 0 0 0 0 0 brainR 1.2 1 1 0 0 0 0 0 0 stringdist 0.7.2 1 1 0 0 1 1 1 1 phaseR 1.3 1 2 0 3 0 0 0 0 MVN 3.8 1 3 0 0 0 7 0 0 gridGraphics 0.1-5 1 3 0 0 0 0 1 3 VSURF 1.0.2 2 2 4 0 2 1 3 4 mldr 0.2.51 2 2 1 1 0 0 0 0 Peptides 1.0.4 2 2 0 5 0 0 0 0 mvtnorm 0.9-7 2 2 0 0 1 1 0 0 qmethod 1.3.0 2 2 0 0 0 0 1 1 rotations 1.3 3 3 0 0 0 0 3 3 ggmap 2.3 3 3 0 1 0 9 13 2 Reproducibility scale As shown in table below, most packages have same reproducibility scale in each version. Some are less reproducible in current version than in the old. Reproducibility issues count We compared if current versions of packages have more or less reproducibility issues of each type than the old ones. Only for few articles these counts differ, but this data suggests negative impact of active development on reproducibility. Correlations with auxiliary variables This heatmap shows the correlation between reproducibility scale and issue count increase (current-old) with auxiliary variables. Cross means insignificant correlation using Pearson method (p-value &gt; 0.05). The reproducibility scale does not seem to be correlated with any of them. But there is a strong correlation between name issues count and number of lines added and removed since the publication date. Variables associated with popularity could impact on API changes. There are correlations with results, but results should not be analyzed alone, because when API issue occurs, then we cannot check results. 1.6.5 Summary and conclusions Our analysis covered 18 packages in which we have found the most common reproducibility issues. Although the negative impact of active development is more common than positive most packages are not affected. We presented two interesting correlations that require further analysis due to the small number of articles. However, researchers involved in making reproducible research should keep in mind these guidelines: Doing a lot of changes it is easier to rename functions and variables. Most of them (changing letters case for instance) are not necessary but break the article. Some packages gain popularity and authors decide to change API. Rethinking it before publication could resolve this necessity. References "],
["reproducibility-differences-of-articles-published-in-various-journals-and-using-r-or-python-language.html", "1.7 Reproducibility differences of articles published in various journals and using R or Python language", " 1.7 Reproducibility differences of articles published in various journals and using R or Python language Authors: Bartłomiej Eljasiak, Konrad Komisarczyk, Mariusz Słapek (Warsaw University of Technology) 1.7.1 Abstract The aim of this article is to present differences in reproducing both code and results of papers from three popular journals: Journal of Machine Learning Research, Journal of Open Source Software, Journal of Statistical Software. Additionally, every journal has been split into two categories for Python and R articles respectively. Following we propose two general methods of scoring the reproducibility and based on them we mark several dozen articles. We have observed distinct traits of papers from each journal. R is shown to be the most reproducible language, moreover, the Journal of Open Source Software - a journal with least scientific credit received the highest marks in our comparison. 1.7.2 Introduction and Motivation Due to the growing number of research publications and open-source solutions, the importance of repeatability and reproducibility is increasing. Although reproducibility is a cornerstone of science, a large amount of published research results cannot be reproduced (Gundersen and Kjensmo 2018). Repeatability and reproducibility are closely related to science. “Reproducibility of a method/test can be defined as the closeness of the agreement between independent results obtained with the same method on the identical subject(s) (or object, or test material) but under different conditions (different observers, laboratories etc.). (…) On the other hand, repeatability denotes the closeness of the agreement between independent results obtained with the same method on the identical subject(s) (or object or test material), under the same conditions.”(Slezak and Waczulikova 2011) Reproducibility is crucial since it is what an researcher can guarantee about a research. This not only ensures that the results are correct, but rather ensures transparency and gives scientists confidence in understanding exactly what was done (Eisner 2018). It allows science to progress by building on previous work. What is more, it is necessary to prevent scientific misconduct. The increasing number of cases is causing a crisis of confidence in science (Drummond 2012). In psychology the problem has already been addressed. From 2011 to 2015 over two hundred scientists cooperated to reproduce results of one hundred psychological studies (Anderson et al. 2019). In computer science (and data science) scientists notice the need for creating tools and guidelines, which help to guarantee reproducibility of solutions (Biecek and Kosinski 2017, @Stodden1240). There exist already developed solutions which are tested to be applied (Elmenreich et al. 2018). Reproducibility can focus on different aspects of the publication, including code, results of analysis and data collection methods. This work will focus mainly on the code - results produced by evaluation of different functions and chunks of code from analysed publications. In this paper we want to compare journals on the reproducibility of their articles. Moreover, we will present the reproducibility differences between R and Python - two of the most popular programming languages in data science publications.There is discussion between proponents of these two languages, which one is more convenient to use in data science. Different journals also compete between each other. There are already many metrics devised to assess which journal is better regarding this metric (“Journal Metrics - Impact, Speed and Reach,” n.d.). There are no publications related to the reproducibility topic which compare different journals and languages. Although there are some exploring reproducibility within one specific journal (Stodden, Seiler, and Ma 2018). What is more, journals notice the importance of this subject (McNutt 2014). Also according to scientists journals should take some responsibility for this subject (Eisner 2018). 1.7.3 Methodology We decided to focus on three journals: The Journal of Statistical Software (JSTAT) The Journal of Machine Learning Research (JMLR) The Journal of Open Source Software (JOSS) The Journal of Statistical Software and The Journal of Machine Learning Research are well known among scientists in the field of data science. The Journal of Open Source Software is relatively new, was established in 2016. We choose articles randomly from the time frame 2010-present. From every journal we choose around 10 articles, of which around 5 are articles introducing an R package and around 5 are introducing a Python library. We choose only articles having tests on their github repositories. For our metrics we test the following: Tests on github provided by authors - for R packages test_that tests, for Python libraries pytest and unittest tests. Examples from the article - we test whether chunks of code included in the article produce the same results. Number of examples in an article varies a lot, in particular all the articles from Journal of Open Source Software do not have any examples. Examples provided by the authors on github repository in the catalog examples. 1.7.3.1 How to compare articles? 1.7.3.1.1 Insight to the problem Before anything can be said about the differences in journals and languages, first there has to be a measure in which they can be compared. Journals in general prefer articles of the same structure. What it means is that articles from different journals can vary substantially. This includes not only topics but number of pages, style of writing and most importantly for the topic of this article the way they present code. Thus it comes as no surprise that there are many means how the code can be reproduced. Every so often when an article is presenting a package there can be no examples and only unit tests. Naturally, the opposite can occur. Obvious conclusion is that the proposed measure must not be in favor of any way of presenting code in the given article. The problem of defining the right measure of article reproducibility deserves a separate article itself and It should be stated that metrics used by us are for sure not without a flaw. We do not assume that they are unbiased but that they are true enough that we can draw conclusions from them. 1.7.3.1.2 Proposed metrics First of all, we did all tests provided by the author in the article or located on an online repository. But if there was an example but there was no direct connection to if from the article it was not included in our reproduction process, because in our opinion it’s not a part of the article and therefore journal. Because of what has been said in the previous paragraph we decided to look at articles from two perspectives. One is more bias, second is true to the sheer number of reproducible examples and positive tests. 1.7.3.1.3 Student’s Mark Analysis of the problem has led to the decision that numerical assessment of article reproducibility has too many flaws and does not represent well the problems that occur while recreating the results of an article. What we propose is a 4-degree mark ranging from 2 to 5. The highest mark, 5 is given if the author provided all results to the code shown in the article and repository and if the results can be fully reproduced. The article is scored 4 when there are some minor problems in the reproducibility of the code. For example, an article may lack the outputs to some part of the code shown or there are some errors in tests. The major rule is that code should still do what it was meant to do. If some errors happen, but they are not affecting the results, they are negligible. The article is scored 3 in a few cases. If an article lacks all or the vast majority of code outputs, but when reproduced it still produces reasonable results. When in some tests or examples we can observe non-negligible differences, but this cannot happen to a key element of the article. For example, the method proposed in the article describing training machine learning model works and the model is trained well, but there are errors in the part of the code where the model is used by some different library. If we would have to score the article based only on this example we would give it a 3. The article is scored 2 if there are visible differences in reproducing results of key elements of the article. Or If the code from the article didn’t work even though we had all dependencies. 1.7.3.1.4 Reproducibility value Second metric we used to analyse articles is simple and puts the same weight to the reproducibility problems of the tests and examples. \\[ R_{val} = 1 - \\frac{negative \\ tests + negative \\ examples}{all \\ tests + all \\ examples} \\] So a score of 0 represents an article that failed in all tests and had only not working examples. 1.7.4 Results Results of reproducing all chosen articles are presented in the following table: Journal Language Title StudentsMark ReproducibilityValue JOSS Python Autorank: A Python package for automated ranking of classifiers (Herbold 2020) 4 0.95 JSTAT R Beyond Tandem Analysis: Joint Dimension Reduction and Clustering in R (Markos, D’Enza, and Velden 2019) 5 1 JSTAT Python CoClust: A Python Package for Co-Clustering (Role, Morbieu, and Nadif 2019) 2 0.3 JSTAT R corr2D: Implementation of Two-Dimensional Correlation Analysis in R (Geitner et al. 2019) 4 1 JSTAT R frailtyEM: An R Package for Estimating Semiparametric Shared Frailty Models (Balan and Putter 2019) 4 0.79 JOSS Python Graph Transliterator: A graph-based transliteration tool (Pue 2019) 4 0.93 JMLR Python HyperTools: a Python Toolbox for Gaining Geometric Insights into High-Dimensional Data (Heusser et al. 2018) 5 0.96 JOSS R iml: An R package for Interpretable Machine Learning (Molnar 2018) 5 1 JOSS R learningCurve: An implementation of Crawford’s and Wright’s learning curve production functions (Boehmke and Freels 2017) 5 1 JOSS R mimosa: A Modern Graphical User Interface for 2-level Mixed Models (Titz 2020) 3 0.67 JMLR R mlr: Machine Learning in R (Bischl, Lang, et al. 2016a) 4 0.98 JMLR R Model-based Boosting 2.0 (Hothorn et al. 2010) 4 0.95 JSTAT Python Natter: A Python Natural Image Statistics Toolbox (Sinz et al. 2014) 2 0 JSTAT R Network Coincidence Analysis: The netCoin R Package (Escobar and Martinez-Uribe 2020) 4 0.91 JMLR Python OpenEnsembles: A Python Resource for Ensemble Clustering (Ronan et al. 2018) 2 0.78 JOSS R origami: A Generalized Framework for Cross-Validation in R (Coyle and Hejazi 2018) 5 1 JOSS Python py-pde: A Python package for solving partial differential equations (Zwicker 2020) 2 0.86 JOSS Python PyEscape: A narrow escape problem simulator packagefor Python (Hughes, Morris, and Tomkins 2020) 2 0.8 JOSS Python Pyglmnet: Python implementation of elastic-net regularized generalized linear models (Jas et al. 2020) 5 0.98 JSTAT Python pyParticleEst: A Python Framework for Particle-Based Estimation Methods (Nordh 2017) 3 0.67 JSTAT Python “PypeR, A Python Package for Using R in Python” (Xia, McClelland, and Wang 2010) 3 1 JOSS R “Rclean: A Tool for Writing Cleaner, More Transparent Code” (Lau, Pasquier, and Seltzer 2020) 5 1 JMLR Python RLPy: A Value-Function-Based Reinforcement Learning Framework for Education and Research (Geramifard et al. 2015) 2 0.59 JSTAT R rmcfs: An R Package for Monte Carlo Feature Selection and Interdependency Discovery (Dramiński and Koronacki 2018) 2 0.57 JMLR Python Seglearn: A Python Package for Learning Sequences and Time Series (Burns and Whyne 2018) 4 0.88 JSTAT Python Simulated Data for Linear Regression with Structured and Sparse Penalties: Introducing pylearn-simulate (Löfstedt et al. 2018) 2 0.3 JOSS R tacmagic: Positron emission tomography analysis in R (Brown 2019) 5 1 JMLR Python TensorLy: Tensor Learning in Python (Kossaifi et al. 2019) 3 1 JMLR R The flare Package for High Dimensional Linear Regression and Precision Matrix Estimation in R (Li et al. 2015) 3 1 JMLR R The huge Package for High-dimensional Undirected Graph Estimation in R (Zhao et al. 2012) 3 1 To better present obtained results plots below show distribution of marks within each journal and language: FIGURE 1.4: Distribution of ‘Student’s Mark’ score of reproduced articles within each journal. FIGURE 1.5: Distribution of ‘Student’s Mark’ score of reproduced articles within each language. Following plots show means of ‘Student’s Mark’ scores of articles within each journal and each language: FIGURE 1.6: Comparison of mean ‘Student’s Mark’ score of reproduced articles between journals. FIGURE 1.7: Comparison of mean ‘Student’s Mark’ score of reproduced articles between languages. Based on the plots we can see that R articles had a better mean score and Journal of Open Source Software had also the best mean score among the journals. Similar plots below show means of ‘Reprodcibility Value’ scores: FIGURE 1.8: Comparison of mean ‘Reproducibility Value’ score of reproduced articles between journals. FIGURE 1.9: Comparison of mean ‘Reproducibility Value’ score of reproduced articles between languages. Same as with ‘Student’s Mark’ we can see that R articles had a better mean score and Journal of Open Source Software had the best mean score among the journals. We also examined similar statistics for 6 groups - every journal-language pair. FIGURE 1.10: Distribution of ‘Student’s Mark’ score of reproduced articles within each journal-language pair. FIGURE 1.11: Comparison of mean ‘Student’s Mark’ score of reproduced articles between journal-language pairs. FIGURE 1.12: Comparison of mean ‘Reproducibility Value’ score of reproduced articles between journal-language pairs. On these plots we can see that within each journal R always has a higher mean score than Python in both metrics. On the other hand, the difference between languages within JMLR and JOSS is not as significant as in JSTAT. 1.7.5 Summary and conclusions In this article, we presented the differences between three scientific journals and two most popular data science programming languages in the context of the reproducibility of articles. Based on our research, we have to come to the following conclusions: The Journal of Open Source Software has the highest score in both metrics. Articles from this journal are often published by professional developers in contrary to other journals dominated by theoretical scientists. That is why, in our opinion, the quality of code on JOSS could be higher. Similarly, the Journal of Machine Learning Research is occupied by people being more professional developers than the Journal of Statistical Software. This is because, as the names suggest, JMLR is connected more to machine learning and JSTAT to statistics. And this may be, from our point of view, why JSTAT scored the lowest in both metrics. According to our research R is more reproducible than Python. The difference between these two languages in JMLR and JOSS is not as significant as in JSTAT. It is due to the fact that R is a language “made by statisticians for statisticians” and JSTAT articles are focused on statistics more than articles from other journals. Some of the Python packages were created in Python2, which is no longer supported. This created many problems with reproducing them. Most of the articles from JOSS had specified requirements in contrast to the other two journals. This contributed to higher JOSS results. To sum up, our research suggests that articles published in JOSS and using R language are the most reproducible, but research conducted on a bigger sample is needed to confirm our results. What is more, a similar comparison of other journals and languages can be made. References "],
["imputation.html", "Chapter 2 Imputation", " Chapter 2 Imputation Imputation "],
["default-imputation-efficiency-comparison.html", "2.1 Default imputation efficiency comparison", " 2.1 Default imputation efficiency comparison Authors: Jakub Pingielski, Paulina Przybyłek, Renata Rólkiewicz, Jakub Wiśniewski (Warsaw University of Technology) 2.1.1 Abstract Imputation of missing values is one of the most common preprocessing steps when working with many real-life datasets. There are many ways to approach this task. In this article, we will be using 3 different simple imputation methods and 6 more sophisticated methods from popular R packages but without changing or tuning their parameters. We will test how their imputation affects the score of models trained on classification datasets. Effectiveness of the imputations will be checked by measuring models’ performance on different measures but focusing on one which is sensitive to class imbalance - Weighted TPR-TNR. We will try to answer the question of whether simple imputation methods are still insignificant for more complex methods with default parameters, or maybe in this case simple imputation methods are not only faster but more efficient? 2.1.2 Introduction and Motivation Data is becoming more and more useful and valuable in various industries. Companies started to gather data on a scale that has not been imagined before. But with more data comes more problems. One of them is missing values. Omitting observations affected by them is one way, but why doing it while the information that they possess might be still valuable? This is why the imputation of missing values is essential when dealing with real-life data. But there are various methods of imputation and for an inexperienced user, it might be confusing and hard to decide which one to use. It may be even harder to design metrics that allow comparing methods between them. We decided that we should judge the effectiveness on the performance score of the model which has been trained on dataset imputed using some method. So when model trained on data imputed with \\(method_1\\) has a higher performance score than the same model trained on the same data but imputed with \\(method_2\\), that means that \\(method_1\\) is more effective. Sometimes omitting rows or even removing columns and NA can give satisfying results but the majority of the time, it is worse than simple imputation methods. For simple tasks mean/median and mode are fast and computationally efficient and can give good results. But they have no variance and because of that can give worse results on complex datasets than more robust methods. On the other hand, more complex models like for example random forests are in theory more advanced and despite adding algorithmic complexity and computational costs to training procedure, are thought to be better in quality of imputation. So models that had been trained on data that has been previously imputed with more complex methods are more likely to achieve higher performance scores. Furthermore, parameter changing and tuning can be difficult and time-consuming. Deciding which method is the best for an inexperienced user who will not be tuning hyperparameters is the purpose of this article. We are benchmarking methods that have different complexity from various packages along with simple imputation methods but without parameter tuning, so that casual users can benefit from it. The default methods from packages are easy to use and despite not living up to their potential they might acceptable results. 2.1.3 Related Work Missing data imputation is a challenging issue in machine learning (ML) and data mining. Little and Rubin (2002) defined three main categories of missing values, but most more or less generic methods of imputation, which have been proposed in the last few decades, are suited for data from one of this category. This category is called missing at random, in short MAR. This assumes that the fact that a value is missing is not dependent on other features of the data. So we want to avoid a situation when for example occurrence of missing values in feature \\(X_i\\) is highly correlated with some values in feature \\(X_j\\). Many programmers are actively creating and looking for new strategies of data imputation that enable creating the best ML model possible. They use various packages to visualize and impute missing data with different methods like knn (k-Nearest Neighbor) or even different model for each feature with missing values (Kowarik and Templ 2016a). Some methods from popular packages follow a fully conditional specification approach where one variable with missing values becomes the target variable (van Buuren and Groothuis-Oudshoorn 2011a). To our knowledge, there are not many available papers comparing different methods of imputations from different packages and measuring their efficiency on multiple models and datasets. The use of an appropriate metric is crucial for this task because not all metrics are suitable for all datasets. Jadhav (2020) presented a new metric Weighted TPR-TNR for measuring the performance of ML models on imbalanced datasets, which according to the authors is the best overall metric available. 2.1.4 Methodology In our article, we compare different methods of imputations - simple and advanced methods (from several R packages) - and measuring their efficiency on multiple models and classification datasets. This section of the article contains information about selected datasets, methods of imputations, ML algorithms, and metrics used to assess the quality of the classification after applying imputation to given datasets. Also, at the end of the section, the workflow of our experiment is briefly presented. 2.1.4.1 Datasets In the experiment, we used various OpenML datasets, some of them belong to OpenML100 repository (Bischl, Casalicchio, et al. 2017). Datasets have been selected so that they have missing values and datasets differ in the number of observations, features, and percentage of data missing. All these datasets were designed for the classification task. The information about these datasets is given in Table 2.1. TABLE 2.1: Datasets Information Dataset Number of observations Number of features Imbalance ratio Percent of missing values cylinder-bands 540 31 1.368 5.299 credit-approval 690 16 1.248 0.607 adult 5000 13 3.146 0.958 eucalyptus 736 16 1.307 3.864 dresses-sales 500 13 1.381 14.692 colic 368 20 1.706 16.291 sick 3772 28 15.329 2.171 labor 57 17 1.850 33.643 hepatitis 155 20 3.844 5.387 vote 435 17 1.589 5.301 echoMonths 130 10 1.031 7.462 Imbalance ratio is computed as follows \\(imbalance\\_{ratio} = n/p\\) where \\(n\\) - negative class and \\(p\\) - positive class. 2.1.4.2 Imputing strategies We selected and analyzed nine different imputation strategies that are known and willingly used by programmers. Some of them belong to simple methods of imputation and others are more complex. A description of these methods is below. remove columns - removing columns with any missing values from the dataset. random fill - imputing with random values from the given feature. median and mode - imputing with median (for numerical features) and mode (for categorical features) from the given feature. missForest - imputing with Random Forests (using missForest package (Stekhoven and Buehlmann 2012a)). Function from this package builds a random forest model for each feature with missing values. Then it uses the model to predict the value of missing instances in the feature based on other features. vim knn - imputing with k-Nearest Neighbor Imputation based on a variation of the Gower Distance (using VIM package (Kowarik and Templ 2016a)). An aggregation of the k values of the nearest neighbors is used as an imputed value. vim hotdeck - imputing with sequential, random (within a domain) hot-deck algorithm (using VIM package). In this method, the dataset is sorted and missing values are imputed sequentially running through the dataset line (observation) by line (observation). vim irmi - imputing with Iterative Robust Model-Based Imputation (IRMI) (using VIM package). In each step of the iteration (inner loop), one variable is used as a response variable and the remaining variables serve as the regressors. The procedure is repeated until the algorithm converges (outer loop). pmm (mice) - imputing with Fully Conditional Specification and predictive mean matching (using mice package (van Buuren and Groothuis-Oudshoorn 2011a)). For each observation in a feature with missing values, this method finds observation (from available values) with the closest predictive mean to that feature. The observed value from this “match” is then used as an imputed value. cart (mice) - imputing with Fully Conditional Specification and classification and regression trees (using mice package). CART models select a feature and find a split that achieves the most information gain. Splitting process is repeated until the maximum height is reached or branches have only samples from one class. The first three methods will be called simple imputation methods. They are self-explanatory and do not have any hyperparameters to tune. In contrast, algorithms provided in missForest, VIM, and mice packages are complex and may require costly hyperparameters tuning. We used default implementations in our experiments, based on the assumption that additional complexity and more hyperparameters might create a too steep learning curve for an average data scientist. Since neither mlr nor caret package enables using custom imputing methods in modeling pipeline, the only way to avoid data leakage was to impute values separately for train and test datasets. This imposes a severe limitation on our imputation effectiveness, especially for small datasets, since information learned during imputing values in training data set cannot be used during test set imputation. 2.1.4.3 Classification algorithms All models were based on classifiers from the caret package (Kuhn 2008) with AUC being the summary metric that was used to select the optimal model. Our goal was to use classification models that are as different from each other as possible, to check if model complexity and flexibility have an impact on the effectiveness of imputation strategy. The most effective imputation strategy should enable models to achieve the highest performance scores. Selected classification algorithms are listed below. Bagged CART - simple classification trees with bootstrap aggregation. treebag builds multiple models from different subsets of data. At the end constructs a final aggregated and more accurate (according to the chosen metric) model. Random Forest - very flexible, robust, and popular algorithm, with no assumptions about data. Multiple classification trees trained on bagged data. For this article implementation called ranger will be used because of its lower computing time leverage. Multivariate Adaptive Regression Spline - assumes that relation between features and response can be expressed by combining two linear models at a certain cutoff point - can be regarded as a model with flexibility between linear and nonlinear models. Implementation called earth will be used. k-Nearest Neighbors - assumes that similar observations are close to each other in feature space, assumes a very local impact of features. knn is a simple algorithm and will be a good benchmark for others. Naive Bayes - nonlinear model with the assumption that all features are independent of each other. Pros of nb are: fast in training, easily interpretable, has little hyperparameters to tune. While some of these algorithms have multiple hyperparameters to tune it will not be covered in this article. Tuning for different data can be time-consuming especially when datasets are large and tuning them could mitigate the effect of data imputation. Based on this factor default implementations and parameters of those algorithms will be used. It might not get the best score in metrics but it will be fair for all the imputation methods. It can be argued that the impact of imputation will be even more visible without parameter tuning. All algorithms will be trained and tested on the same data but some will be encoded. 2.1.4.4 Metrics We performed the aforementioned imputing methods for 11 datasets from the OpenML repository. Since all datasets were designed for binary classification problems and many of them were heavily imbalanced, we used 4 metrics to evaluate our models’ performance (see Table below). Classifier Evaluation Measures Measure Formula AUC Measures area under plot of Sensitivity against Specificity Balanced Accuracy \\(\\frac{1}{2}(Sensitivity+Specificity)\\) Geometric Mean \\(\\sqrt{Sensitivity*Specificity}\\) Weighted TPR-TNR \\((Sensitivity*\\frac{N}{P+N})+(Specificity*\\frac{P}{P+N})\\) Area under ROC curve (AUC) is used as a general performance metrics, while the rest is used to specifically evaluate model performance on imbalanced data sets. Accuracy measures how a good model is incorrectly predicting both positive and negative cases. If your dataset is imbalanced, then “regular” accuracy may not be enough (cost of misclassification of minority class instance is higher than for majority class instance). That is why we use Balanced Accuracy, which is the arithmetic mean of the TPR (Sensitivity) and FPR (Specificity). Geometric Mean (of Sensitivity and Specificity) is a metric that measures the balance between classification performances on both the majority and minority classes. Weighted TPR-TNR is a very promising new single-valued measure for classification. It takes into account the imbalance ratio of the dataset and assigns different weights to the TPR and TNR (where P is the total number of positive cases and N is the total number of negative cases). 2.1.4.5 Experiment To sum up, in the experiment we used 5 different classification algorithms and 11 different datasets to assess the performance of the imputation of missing values using the previously described measures. The experiment workflow was presented in Figure 2.1. FIGURE 2.1: Experiment workflow The caret package in R programming was used to conduct the experiment. Classification algorithms used in this experiment were: ranger, earth, treebag, knn, nb existing in function train() from this package. The procedure followed to assess the performance of the imputation of missing values was as follows: The dataset was divided into train and test subsets with a split ratio of 80/20. We imputed values separately in train and test datasets. This process was repeated for all nine different imputing strategies. The time of imputation was measured independently outside this workflow. All imputed train and test datasets were used in five classification algorithms. Numerical variables were centered and scaled to mean 0 and standard deviation 1. For knn datasets were also encoded with one-hot encoding. The classifiers were built using transformed train datasets. The classification results were obtained using a transformed test datasets. Before dividing into training and test subsets and building classifiers, seed equal to one was set. 2.1.5 Results Before analyzing the results, three caveats: some of our datasets were quite small, with 100 or so observations some of our datasets were heavily imbalanced some imputations methods did not cover or generally impute data due to the specificity of the dataset. These methods are: remove columns 3/11 failed (labor, vote, and colic datasets) pmm (mice) and cart (mice) 3/11 failed (cylinder-bands, credit-approval, and labor datasets) vim irmi 5/11 failed (all of the above) For removing columns explanation is simple. In some datasets, missing values are in every column so after removing all columns model has no data to be trained on. For other methods, it is more complicated to explain. Apart from removing columns methods they did not throw an error and tried to imput the data but without success. While analyzing the results we are looking at two things: for which imputation method the metrics have the best results time of imputation Especially for big datasets, sophisticated imputation methods required many hours or even days of computing. For that reason datasets with tens of thousands of observations were truncated to 5000 (refers to the adult dataset). 2.1.5.1 Imputation results 2.1.5.1.1 Mean metrics With this first look at the results of imputations performance metric scores will be taken into consideration. In Table 2.2 was presented this performance metrics ordered by Weighted TPR-TNR. Note that in this experiment methods that failed to impute data will be omitted so the whole picture of efficiency among imputation metrics might be distorted. This issue will be addressed in the next section. TABLE 2.2: Mean metrics for each model Imputation name AUC BACC GM Weighted TPR-TNR random fill 0.8717 0.8037 0.7900 0.7687 median and mode 0.8670 0.7931 0.7783 0.7516 vim hotdeck 0.8534 0.7839 0.7701 0.7486 missForest 0.8515 0.7879 0.7611 0.7453 vim knn 0.8667 0.7771 0.7563 0.7445 cart (mice) 0.8581 0.7884 0.7702 0.7379 pmm (mice) 0.8568 0.7847 0.7678 0.7360 vim irmi 0.8220 0.7474 0.7215 0.6812 remove columns 0.7648 0.6623 0.5541 0.5523 As seen above random fill is the best imputation method in all metrics. Second in place in all metrics was median and mode. cart (mice) was third 2 times (GM and BACC) and vim knn with vim hotdeck were third each one time. Simple imputation methods were superior for these datasets. The worst idea seems to be removing columns and using irmi. 2.1.5.1.2 Ranking of results Each model has its specific features and might be thriving after applying different imputation methods. Seeing which method was the best (and the worst) for each model should give a more wide perspective. For creating ranking only Weighted TPR-TNR will be taken into consideration. This is because this metric takes into account the imbalance ratio of the datasets and thanks to its equal evaluation for all datasets will be ensured. Rank feature here is average of ranks from all datasets so if some imputation was second in the metric it would be given rank 2. If two imputations had the same values in the metric they would both have the same rank. If the imputation method failed to impute it was assigned the last rank. Top 3 best imputations was presented in Table 2.3. TABLE 2.3: Top 3 best imputations for each model Model name 1th method imputation 2th method imputation 3th method imputation ranger vim knn missForest pmm (mice) earth random fill median and mode cart (mice) treebag random fill cart (mice) median and mode knn missForest random fill vim knn nb random fill median and mode missForest Random fill is the best in 3 out of 5 models. For ranger and knn the best are VIM knn and missForest respectively. In our top 3 ranking also appears median and mode and two mice methods - pmm and cart. Now let’s see the top 3 worst imputations in Table 2.4. TABLE 2.4: Top 3 worst imputations for each model Model name 7th method imputation 8th method imputation 9th method imputation ranger vim hotdeck vim irmi remove columns earth pmm (mice) vim irmi remove columns treebag vim knn vim irmi remove columns knn vim irmi pmm (mice) remove columns nb cart (mice) vim irmi remove columns Removing columns is the worst choice when imputing data, which is not surprising. It removes vital information from data resulting in significantly worse performance. Second, from the end is almost for all models vim irmi. Visualization in Figure 2.2 shows the mean rank of imputation methods for each model. Ranks are a great way to visualize how effective the imputation really is. They are not sensitive to the dataset, the best rank will always be one, unlike some performance measure that might be significantly different between datasets. The average was measured on 11 datasets. The higher the rank, the better. Some trends and stability of methods are visible. FIGURE 2.2: Average rank plot Random fill is the best method. It was the best for three models and the second-best for one model. Median and mode and missForest were also really good for all models. The worst without a doubt is removing columns. Some methods are really good for certain models but very bad for others (for example pmm (mice)) in contrary to random fill and median and mode which were equally good for all models but not the best for knn and ranger. 2.1.5.1.3 Similarity of imputations To see how similar are imputations to each other we used biplot. Biplot is using PCA to reduce the dimensionality of the data in our case of results. If imputation names are close to each other, it means that they produce similar results. The closer the arrows are to each other the more metrics values are correlated. Data came from our imputation results for all models without NA values. Then for each imputation data was averaged and processed by PCA (see Figure 2.3). FIGURE 2.3: Rank biplot Some imputation techniques seem to be distant from others. Mainly VIM’s irmi, remove columns, pmm, and knn. Judging by the red arrows (loadings plot), there are similarities in ranks among treebag and earth models. Knn and ranger are not very correlated with ranks achieved by them. 2.1.5.2 Imputation time In imputation not only results matter. While having small dataset imputation time is not something worth considering. But when having tens or hundreds of thousands of observations some more complex imputations might take a really long time. In this case, maximum samples in data are 5000 but differences are already visible. This experiment was repeated 10 times to ensure higher reliability of data. 2.1.5.2.1 Mean time TABLE 2.5: Average results - time Imputation name Time remove columns 0.0140 random fill 0.0154 median and mode 0.0272 vim hotdeck 0.2484 vim knn 1.6833 pmm (mice) 8.1867 missForest 8.9694 vim irmi 10.1300 cart (mice) 20.0550 As one might predict, imputation with mean and mode removing columns or filling NA’s with random value are the fastest methods (see Table 2.5). Imputations from the VIM package also have small computation time. Imputing with missForest package or using predictive mean matching in mice package is more than 1000 times slower, whilst using cart instead of pmm in mice package additionally increases computation time more than threefold. 2.1.5.2.2 Distribution of time FIGURE 2.4: Time distribution Imputation time is shown in the shape of boxplots (see Figure 2.4). A clear distinction between mice, missForest, and VIM’ irmi packages functions which are significantly slower than other methods. On contrary here imputation time was taken into consideration even when NA’s were produced. 2.1.5.2.3 Influence of amount of NA on time FIGURE 2.5: Number of NA and imputation time The time needed to process NA’s is very volatile (see Figure 2.5). For simple imputation methods and VIM’s hotdeck time needed compared to the rest of the imputations seems to be constant. This plot was achieved by getting information from datasets and imputing ten times. Then the median was taken from times of dataset imputation. Fluctuations might be the effect of different data types, a number of factors, continuous variables distribution. 2.1.6 Summary and conclusions Our experiments have unexpected results - it turns out that using a naive strategy of imputing with random values resulted in models with the best performance. The reason for the poor performance of more complex imputation methods might be the fact that we used default values of hyperparameters and some tuning may be required to show the full potential of these methods. Some datasets were simple and small therefore potential imputation leverage could be insignificant. What we did prove was that simple imputation methods might still be valuable despite their apparent flaws. What was significant was the big overhead that simple imputations had in terms of results achieved in time. While having a big dataset with a limited number of missing values they might be the best solution because some methods use whole data to train it’s missing value imputation. We might not ignore huge computational overhead that is the result of using complex computing methods. The point of this paper was to test those methods for inexperienced users and find the most valuable one for them. As for now considering both performance and time the best for most models are random fill, median and mode. They achieve more than satisfying results, are reliable and work for every dataset. Judging by the results it is better to use simple imputation methods than more complex methods with default parameters from various packages. References "],
["the-hajada-imputation-test.html", "2.2 The Hajada Imputation Test", " 2.2 The Hajada Imputation Test Authors: Jakub Kosterna, Dawid Przybyliński, Hanna Zdulska (Warsaw University of Technology) 2.2.1 Abstract There are many different ways of dealing with missing values. Depending on their quantity and properties, various methods turn out to be the best. Using one and the same solution for each dataset is certainly not the best option, but sometimes it is convenient or even necessary to focus on one particular method without much insight into its effect. In this chapter we will take a look at several popular imputation methods and try to compare them with each other, including ranking algorithms and finally choosing the best one. For this purpose, we will analyse imputation times and predictions scores for several machine learning classification algorithms on various datasets. 2.2.2 Introduction and Motivation In statistics, imputation is the process of replacing missing data with substituted values. Over the years, humanity has created many different methods accomplishing that, including the simple and instinctive ones, but also more advanced and hard to be easily explained. Choosing the best-suited imputation method for the dataset with missing values is still the daily dilemma of every data scientist and there isn’t one universally recognized best technique. Some scientists believe the crux lies in the most advanced and sophisticated algorithms, others trust the simplest of all possible. In a way, both sides are right - the first naturally win in speed of execution, but the second ones do not have advanced operations implemented for nothing and this usually results in data that more accurately reflects reality or what might be expected from it. How do the individual popular ways fall on the famous collections from OpenML100? In this test, which we decided to name The Hajada Imputation Test (taken from the first two letters of creators’ names), we will find the answer to this question, by delving into how well classification on imputed datasets can perform and when predictions are the best, rank algorithms taken into account and finally choosing the best one. Hajada Test depends on prediction scores and imputation times. Obviously its result will not be the final verdict declaring which imputation algorithm is clearly better or worse, but considering the comparison of their quality on different datasets, we will be able to assess their collective performance and effectiveness against the backdrop of the whole. 2.2.3 Methodology To rank imputation methods fourteen datasets were taken into account, imputed by six different methods and each used to train five classification algorithms. Based on the predictions scores and imputation times methods were rated 1-6 grouped by datasets and algorithms, where 1 is the best method and 6 is the worst. 2.2.3.1 Imputation functions Six different methods have been taken into consideration. Their selection is well thought out - they are all widely known in the world of Data Science, and at the same time they differ significantly in approach, implementation, concept and results. Comparing them not only results in a fair view of the strictly indicated implementations, but also what result subsequent approaches result in. Three simple imputation methods and three more advanced algorithms offered by popular packages were taken to consideration and their comparison. The clash of implementations with such a degree of diversity of complexity gives a clear message whether it pays off to reach for an advanced algorithm from the package prepared by professionals for many hours, conducted on many tests, or rather stay with minimalism and write easy code solving the problem in a trivial way. Methods compared in subsequent stages are: mode / median replace - basic process which puts in a place of missing data median cells from their columns for numerical values and dominants for categorical ones. remove rows - trivial solution of removing rows containing any missing data. ‘bogo’ random replace - simple algorithm replacing each NA value with a random entry from its feature. (chosen independently for every missing value) mice (S. van Buuren and Groothuis-Oudshoorn 2020) - advanced method creating multiple imputations for multivariate missing data, based on Fully Conditional Specification, where each incomplete variable is imputed by a separate model. Standard imputation with maxit parameter of value 0 and default 5 number of imputed datasets. VIM (Templ, Kowarik, and Alfons 2020) - standard k-nearest neighbors algorithm taken from the library. missForest (Stekhoven 2013) - imputation based on random forest offered by the package. 2.2.3.2 Datasets The fourteen data frames were taken from OpenML100 collection and were corrected specifically for this research. Both small and simple frames were chosen as well as more complex and containing a lot of information, difficult to explain or present. They can be found under the following identifiers with the following names and statistics: Id Name No. rows No. columns No. missings 1590 adult 48842 15 6465 188 eucalyptus 736 20 448 1590 dressess-sales 500 13 835 29 credit-approval 690 16 67 38 sick 3772 30 6064 40536 SpeedDating 8378 123 18372 41278 okcupid-stem 50789 20 154107 56 vote 435 17 392 6332 cylinder-bands 540 40 999 1018 ipums_la_99-small 8844 57 34843 27 colic 368 23 1927 4 labor 57 17 326 55 hepatitis 155 20 167 944 echoMonths 130 10 97 These above have been placed in individual directories identified by id in the prepared directory. The six imputations mentioned earlier were conducted on all fourteen of them, but only for six, all of the methods were successful. 2.2.3.3 Binary classification algorithms In order to compare the quality of the imputed data and see how this supplementation of deficiencies works in practice, five binary classification algorithms have been selected. Their choice was made after careful analysis and extensive discussion, in order to find models that are both widely known and used, but also apply to different approaches and give reasonably distinguishable results using different techniques. The final choice fell on: classification tree from rpart (Therneau and Atkinson 2019a) - classic algorithm which uses a decision tree to go from observations about an item to conclusions about the item’s target value, k-Nearest Neighbors from class package (Ripley 2019) - standard knn model attributing a given observation to a target corresponding to its closest observations in space, naive Bayes from e1071 (Meyer et al. 2019) - well-known classification computing the conditional a-posterior probabilities of a categorical class variable given independent predictor variables using the Bayes rule. random forest from ranger (Wright, Wager, and Probst 2020) - algorithm consisting of many decisions trees uses bagging and feature randomness when building each individual tree trying to create an uncorrelated forest of trees whose prediction by committee is more accurate than that of any individual tree. Support Vector Machine from e1071 (Meyer et al. 2019) - a discriminative classifier formally defined by a separating hyperplane. In order to automise our code we developed it in such a way so that it is possible to easily conduct further experiments with entirely new datasets, imputation methods and classification models. Plenty of time neccesary for imputation and modelling was saved using parallel processing. 2.2.3.4 Metrics functions In order to test the same test-train splits we used random seed 1357 for all datasets. For each machine learning model after every imputation was created confusion matrix and the values of four basic metrics: accuracy - \\(\\frac{TP+TN}{TP+FP+FN+TN}\\) precision - \\(\\frac{TP}{TP+FP}\\) recall - \\(\\frac{TP}{TP+FN}\\) f1 - \\(2*\\frac{Recall * Precision}{Recall + Precision}\\) For the final conclusion, ranking concerned only the last one. Additionally, Matthews correlation coefficient measures were also computed. \\({\\displaystyle {\\text{MCC}}={\\frac {{\\mathit {TP}}\\times {\\mathit {TN}}-{\\mathit {FP}}\\times {\\mathit {FN}}}{\\sqrt {({\\mathit {TP}}+{\\mathit {FP}})({\\mathit {TP}}+{\\mathit {FN}})({\\mathit {TN}}+{\\mathit {FP}})({\\mathit {TN}}+{\\mathit {FN}})}}}}\\) These two give great information about the quality of imputation and a good comparison. It resulted in simple and clear information about which observations are well classified, which ones are wrong and how they should be. The main advantage of them is accounting for target variable’s inbalance. Considering sets’ different properties, even when it comes to balancing, comparing those measures, the fact that they show the pros and cons of given result will additionally be worthwhile to compare them. 2.2.4 Results In order to analyse and create results, except for comparing imputations’ time and effectiveness for ML models, we need to take into account that not all of imputations were successful for all datasets. In further rankings they will have to be considered as the worst for a given dataset. 2.2.4.1 Comparing imputation times A dataframe containing information about imputation methods was built. For a better view a boxplot was also created: Judging by the logarithmic scale, there is no surprise that removing rows and mode/median replenishment are definitely the fastest methods, with removing rows being several times faster. What would you expect, also “random values” imputation was quite fast, but slower than the two mentioned. Looking at more advanced ones, definitely VIM turned up to be the fastest - times usually being approximately 5-10 times shorter than missForest and mice. These last two are quite slow, with the former appearing to be slightly faster. Taking into account all the imputations which have been implemented, it is clearly seen that removing rows as well as replacing with mode or median really stand out. However, we cannot fully compare the other four - due to the fact that missForest failed on three datasets, and mice - up to seven, mostly for datasets greater in size. Considering such a large spread of data size, there is a very interesting difference between median and mean for VIM’s time - the first is almost seven minutes, the second - barely a second and a half. In general, however, it is certainly much longer than methods for removing incomplete rows and filling with mode/median. Also time was compared for those datasets for which all imputations were successful - these are ids 27, 38, 55, 56, 944, 188 (dataset with id 4 failed on removing rows, because each of rows had at least one missing entry). Taking into account the mean time of imputation, definitely missForest was the slowest, but also its standard deviation seemed to be incomparably large - this is probably due to the fact that for smaller sets it is doing well, but due to its complexity, its slowdown can be seen for very sizeable datasets. VIM turned out to be better for quick calculations than missForest and mice; considering the median imputation time, mice is comparable to missForest - so for small datasets there is not much difference between them, and a lot of time was definitely needed to devote to these larger data frames. 2.2.4.2 Best measures In order to compare F1 and MCC measures, mean results on all sets for all imputations were calculated. Then, for each machine learning algorithm a ranking was created - so that the imputation with best results obtained the first rank, second best - 2nd, etc. The rank’s distributions for every imputation came out as shown by the following boxplot: (rankings omit failed imputations) It might seem that the results of the methods are very close to each other, but the above visualization takes into account rankings only for successful imputations - to get the correct comparison there is a need to make a ranking with failing methods receiving the worst ranks instead of being omitted. The outcome was presented below: Considering the imputation times and results, it can be said that the k-nearest neighbors algorithm from the VIM package proved to be “universally” the best. Out of the advanced methods it falls out the fastest, and its results are also the best. However, it is more difficult to choose second and subsequent places - looking at the results, the simple methods are very quick, but they differ from the more advanced ones. Looking at the advanced ones, with the exception of VIM, MissForest stands out. However, his problem is time. So after VIM, it seems second best to use mean / median replenishment for a small amount of time and missForest for a large one. However, although mode / medain usually obtained worse ranks, there was no extensive difference in results between the methods - which is not visible in the charts. “removeRows” is characterized by large fluctuations, and therefore we advise against using it. 2.2.5 Summary The experiment can be considered successful when it comes to the datasets and tools made available to us. After examining fourteen datasets with NA values, imputated with six different methods (including three simple and three more advanced), as well as tested for output quality on five binary classification algorithms, we obtained valuable results. Our work resulted in calculated measures and visualizations, which led us, among others, to the following intuition: The use of VIM in all conditions results in a fairly satisfactory result in terms of classification’s quality and time, which makes it the best tested imputation. Supplementing the missing values with mode/medians from columns is not such an unwise idea as it may seem - not to mention the rapid action time, the results of this strategy also give good results. missForest is also noteworthy - if one has time for running it and the dataset isn’t too sizeable. 2.2.6 Conclusions Unfortunately, it is impossible to draw an objective conclusion based on such a small number of tested data - with as few as 14 sets, problems with imputations on some of them or too little data in frames not giving a satisfactory answer. Nevertheless, the code is certainly very valuable and for more datasets it could confirm one or another belief. Ready solutions and valuable code can be used to study the effects of imputation on a larger number of datasets, which would give even greater scientific value. Examining even all data sets from OpenML100 could more confidently answer the question of which imputations are doing better and which are worse. Looking at our small input, it would seem that VIM is a better method than missForest and definitely more effective than mice, but we can not be sure about such a conclusion due to the lack of a sufficiently large sample from a statistical point of view. References "],
["comparison-of-performance-of-data-imputation-methods-in-the-context-of-their-impact-on-the-prediction-efficiency-of-classification-algorithms.html", "2.3 Comparison of performance of data imputation methods in the context of their impact on the prediction efficiency of classification algorithms", " 2.3 Comparison of performance of data imputation methods in the context of their impact on the prediction efficiency of classification algorithms Authors: Ada Gąssowska, Mateusz Grzyb, Elżbieta Jowik (Warsaw University of Technology) 2.3.1 Abstract Dealing with missing data is a substantial part of feature engineering, as most of the machine learning algorithms do not accept incomplete datasets. Data imputation, especially replacing missing values with a value based on other available cases, is the cure for that problem. However, despite the acknowledged importance of it, in many practical cases, it is not handled with proper caution. Basic median/mode imputation as well as deleting rows with missing data (if the number of missing values is low) are often used as final solutions. As there are many different imputation techniques, choosing which one to use is complicated, but it may be beneficial. There are no objective rules to follow, the only way to choose the best one is to perform numerous comparisons. This is exactly what has been done during this study. The analysis shows, that selecting the best imputation method/classification algorithm combination, that will work best globally, is extremely difficult, because a lot depends on the structure of the dataset, the prediction correctness assessment technique, and the amount of computational power available. In the last section of this article, we suggest a sensible approach to this problem. 2.3.2 Introduction and motivation It is very common for datasets to contain incomplete observations. The analysis conducted by ignoring cases with missing data and considering only complete instances is perceived as naive, inefficient, and exposed to bias, as incomplete ones may also convey valuable information. To incorporate these sketchy observations, various methods of handling missing data may be considered, including both less and more sophisticated approaches. The purpose of our experiment is to gather knowledge about the best ways to impute data while using specific classification algorithms, to obtain the best possible classification performance. In other words, we measure the quality of various imputation methods by assessing their impact on classification accuracy. It turns out, that the performance of machine learning classification models moderately depends on the approach that is applied to the imputation problem, while the time of the imputation when using different techniques, is highly diverse. During this study, we considered ten data sets, six imputation methods, and four classifiers. Mentioned imputation methods contain one basic technique (median/mode imputation) and five more sophisticated ones, which origin respectively from mice, VIM, missRanger, and softImpute R packages. For testing purposes, as the classification algorithms, we used Ranger Random Forests, XGBoost, K Nearest Neighbors, and Naive Bayes classifiers. To evaluate the performance of each imputation method/classification algorithm combination on a given dataset, we used raw Area Under the Curve (AUC), Balanced Accuracy Score (BACC), and Matthew’s Correlation Coefficient (MCC) measures, as well as rankings based on them. Times of each imputation were also verified and compared. 2.3.3 Methodology Most of the algorithms that we used for tests do not accept missing values. To find, which one from all tested imputation methods is best for each tested classification algorithm, we decided to perform the following experiment. Firstly, we created wrapper functions for six different imputation methods: basic method using impute function from imputeMissings package. It imputes missing numeric values with a median and categorical variables with a mode of other records. mice function from mice package (Buuren and Groothuis-Oudshoorn 2011). The function uses Predictive Mean Matching to impute missing values. missRanger function from missRanger package (Mayer 2019). The technique uses the Ranger Random Forests to do fast missing value imputation by chained random forests. hotdeck function from VIM package (Kowarik and Templ 2016b). Each missing value is replaced with an observed response from a “similar” unit. kNN function from VIM package (Kowarik and Templ 2016b). It finds the k closest neighbors to the observation with missing data and then imputes them based on the non-missing values from its neighbors. softImpute function combined with mode imputation (Hastie and Mazumder 2015a). The first method originates from the softImpute package and is applied to impute numeric variables. Mode imputation is used for categorical features imputation. To automatize our work, we also created two specialized functions - split_and_impute and train_and_test. The first function divides given dataset into train and test sets (with configurable proportions and with stratification on the target variable) and imputes it with specified imputation function (one of the above). It returns imputed train and test sets and the time of the imputation (all of which are can be saved to files automatically). The second function performs cross-validation on train set (the default number of folds is 5) and makes the predictions for the test set (that is, after training the model on the whole train set), with a specified classification algorithm. The target variable and label of the positive class must also be passed before the function call. Based on the mentioned predictions, it calculates AUC (Flach Peter 2011), BACC (Velez and Moore 2007), and MCC (Boughorbel S. 2017) measures for both cross-validation and test set testing stages. It also returns plots of ROC curve, AUC, BACC, and MCC measures achieved during the cross-validation stage. All ten benchmarking datasets were obtained from https://www.openml.org/ and can be found there with detailed descriptions. Then, we proceeded in the following way: Firstly, all benchmarking datasets were split and imputed with all imputation methods using split_and_impute function. All the resulting subsets were then saved to corresponding files. This way, all results differences observed later did not depend on any splitting or imputation randomness, as these steps were taken only once for each benchmarking dataset. The time of each imputation was also measured and taken into consideration. Secondly, for all imputation method/dataset combination train_and_test function was called four times - once for each classification algorithm. All returned values were then used for two purposes: to create a specialized report (available here - https://github.com/PlentyCoups/WB-article-extras/blob/master/report.html) used for checking process correctness (mostly by looking at cross-validation scores variances) and, more importantly, to create aggregated plots of test set scores for further inference. The following diagram may also help to visualize our methodology - https://github.com/PlentyCoups/WB-article-extras/blob/master/methodology.png. 2.3.4 Results 2.3.4.1 Results processing It should be mentioned once again, that any differences in results are only due to a change of either the imputation method, the classification algorithm, or the dataset. Divisions into train and test sets for a given dataset were always identical. Because of that, even small scores differences should be taken into consideration. Another important fact is that we took into consideration only datasets on which all of the imputation methods worked, so that all of them may be compared fairly. Raw results were processed and assessed in the following way: Imputation times were compared in correspondence to each imputation method and dataset. Times logarithms had to be used instead of raw data, due to high differences. (Figure 1) Raw measures were compared in correspondence to each imputation method, classification algorithm, and dataset. (Figure 2) Rankings of measures were compared in correspondence to each imputation method, classification algorithm, and dataset. Each imputation method/classification algorithm combination was ranked three times on every dataset, once for each of the three used measures. The given combination gets a 1 if it gives the best result on a dataset, and a 6 if it gives the worst one. Then, such rankings were visualized through multiple boxplots with mean and median ranking calculated and marked on them. (Figure 3) Moreover, as it was already mentioned before, cross-validation scores were merged in a specialized report used for checking process correctness, that is available here - https://github.com/PlentyCoups/WB-article-extras/blob/master/report.html. 2.3.4.2 Results visualization Below figures are best to be viewed with a slight zoom. Figure 1: Imputation timesUpper plot:Imputation times for each imputation method are aggregated and visualized using boxplots.Vertical axis is scaled using logarithm base 10.Lower plot:Imputation times for each imputation method/dataset combination are visualized using scatter plots.Dots’ colors represent imputation methods, datasets are mapped to horizontal axis. The fastest method is basic (median/mode), followed by softImpute and VIM hotdeck. VIM knn and missRanger methods were much slower, and mice method was the slowest, achieving longest imputation time recorded during the experiment - 9 hours. Datasets cylinder-bands, SpeedDating and adult took the most CPU time to impute. Longest times recorder during the experiment were those achieved by mice, often followed by missRanger. Figure 2: Raw scoresClassification performance assesed with three measures is visualized for four classification algorithms using scatter plots.Major horizontal axis represents classification algorithms. Major vertical axis represents measures. Minor horizontal axes represent datasets. Minor vertical axes represent measures’ values.Dots’ colors represent imputation methods. For some datasets imputation method impact is negligible, and for others it is well visible. MCC is on average lower than AUC and BACC, it seems to be a more ‘strict’ measure. For some datasets choice of classification algorithm does not matter, and for others it is noticeably meaningful. For all datasets it is possible to choose at least one imputation method/classification algorithm combination for which results are acceptable, but some datasets prooved to be overall harder. Figure 3: Scores rankingsClassification performance assesed with three measures’ rankings is aggregated and visualized for four classification algorithms using boxplots.Major horizontal axis represents classification algorithms. Major vertical axis represents measures. Minor horizontal axes represent imputation methods. Minor vertical axes represent measures’ rankings.Single red dot represents given imputation method/classification algorithm ranking by given measure on single datasets. Ten datasets for each imputation method/classification algorithm give ten dots for each boxplot.Black lines represent median ranking and red lines represent mean ranking with labeled value. For all three measures best rankings were achieved by basic (median/mode)/kNN combination. For Naive Bayes, Ranger Random Forests, XGBoost algorithms lowest rankings were achieved using missRanger imputation method, with quite low variance of results. The imputer that turned out to be globally the worst (and also the slowest) was the one from mice package. The most important observation is that almost all imputation method/classification algorithm combinations ended first for some dataset. 2.3.5 Summary and conclusions After analyzing the results, especially the plots, it became clear, that it is not possible to pick the best imputation method/classification algorithm for all datasets. It is possible to pick globally the best methods, that had lowest mean rankings (basic for kNN or missRanger for XGBoost), but it does not mean these are the best choice for all datasets. As already said, most imputation method/classification algorithm combinations ended first on at least one dataset. Because of that, if the best possible prediction correctness is an absolute priority, it is always the best to consider and test all available options. It seems that it is a good idea to use basic median/mode imputation as a solid and fast to obtain starting point, and then, when possible and desirable, to evaluate more sophisticated methods, in search of even better ways. References "],
["various-data-imputation-techniques-in-r.html", "2.4 Various data imputation techniques in R", " 2.4 Various data imputation techniques in R Authors: Jan Borowski, Filip Chrzuszcz, Piotr Fic (Warsaw University of Technology) 2.4.1 Abstract There are many suggestions on how to deal with problem of missing values in data sets. Some solutions are offered in publicly available packages for the R language. In our study, we tried to compare the quality of different methods of data imputation and their impact on the performance of machine learning models. We scored different algorithms on various data sets imputed by chosen packages. Results summary presents packages which enabled to achieve the best models predictions metrics. Moreover, the duration of imputation was measured. 2.4.2 Introduction and motivation 2.4.2.1 Background and related work Missing observations in data sets is a common and difficult problem. In the field of machine learning, one of the key objects is the data set. Real-world data are often incomplete, which prevents the usage of many algorithms. Most implementations of machine learning models, available in popular packages, are not prepared to deal with missing values. Before creating a machine learning model, it is essential to solving the problem of missing observations. This requires user pre-processing of data. Some researches examined the similarity between original and imputed data, in terms of descriptive statistics (Musil et al. 2002). Missing data are common in medical sciences and the impact of different imputation methods on analysis was measured (Bono et al. 2007). Some studies show that imputation can improve the results of machine learning models and that more advanced techniques of imputation outperform basic solutions (Batista and Monard 2003) (Su, Khoshgoftaar, and Greiner 2008). 2.4.2.2 Motivation Various imputation techniques are implemented in different packages for the R language. Their performance is often analyzed independently and only in terms of imputation alone. Because of a variety of available tools, it becomes uncertain which one package and method to use, when a complete data set is needed for a machine learning model. In our study, we would like to examine, how methods offered by some popular packages perform on various data sets. We want to consider the flexibility of these packages to deal with different data sets. The most important issue for us is the impact of performed imputation on later machine learning model performance. We are going to consider one specific type of machine learning tasks: supervised binary classification. Our aim is a comparison of metrics scores achieved by various models depending on the chosen imputation method. 2.4.2.3 Definition of missing data In the beginning, clarifying the definition of missing data is necessary. Missing data means, that one or more variables have no data values in observations. This can be caused by various reasons, which we can formally define as follows, referring to DONALD B. Rubin (1976): MCAR (Missing completely at random) Values are missing completely at random if the events that lead to lack of value are independent both of observable variable and of unobservable parameters. The missing data are simply a random subset of the data. Analysis performed on MCAR data is unbiased. However, data are rarely MCAR. MAR (Missing at random) Missingness of the values can be fully explained by complete variables. In other words, missing data are not affected by their characteristic, but are related to some or all of the observed data. This is the most common assumption about missing data. MNAR (Missing not at random) When data are missing not at random, the missingness is related to the characteristic of the variable itself. 2.4.2.4 Techniques of dealing with missing data In case of preparing a data set for machine learning models, we can generally distinguish two approaches. The first method is omission. From the data set, we can remove observations with at least one missing value or we can remove whole variables where missing values are present. This strategy is appropriate if the features are MCAR. However, it is frequently used also when this assumption is not met. It is also useless when the percentage of missing values is high. The second approach is imputation, where values are filled in the place of missing data. There are many methods of imputation, which we can divide into two groups. Single imputation techniques use the information of one variable with missing values. A popular method is filling missings with mean, median or mode of no missing values. More advanced are predictions from regression models which are applied on the mean and covariance matrix estimated by analysis of complete cases. The main disadvantage of single imputation is treating the imputed value as true value. This method does not take into account the uncertainty of the missing value prediction. For this reason multiple imputation was proposed. This method imputes k values, which leads to creating k complete data sets. The analysis or model is applied on each complete data set and finally, results are consolidated. This approach keeps the uncertainty about the range of values which the true value could have taken. Additionally, multiple imputation can be used in both cases of MCAR and MAR data. 2.4.3 Methodology Experiment like this one can be performed involving many techniques we decide to divide our tests into 4 steps: Data Preparation, Data Imputation, Model Training, Model Evaluation. Below we will explain every step in detail. 2.4.3.1 Data Preperation For test purposes we used 14 data sets form OpenML library (Casalicchio et al. 2019). Every data set is designed for binary classification and most of them contain numerical and categorical features. Percentage of missing observations ranged from 0.7% to 35.8%. Most of the sets had a similar number of observations in both classes of target variable, but some of them were unbalanced. Before data imputation, each data set was prepared. Specific preparations were different for each data set, but we commonly did: Removing features which didn’t contain useful information for algorithms (for example all observation have the same value) Correcting typos and converting all strings to lower case to reduce the number of categories Converting date to more than one column (for example “2018-03-31” can be converted to three columns: year, month and day) Removing or converting columns with too many categories After cleaning data sets were transferred to the next step. 2.4.3.2 Data Imputation Clean data sets were split into two data sets, training and testing, in proportion \\(1/4\\) respectively. This split was performed randomly and only once for every data set. It means, every imputation technique used the same split. Imputation was performed separately for train and test sets. Before split, we also removed the target column to avoid using it in imputation. For our study, we decided to choose five packages designed for missing data imputation in the R language and one self-implemented basic technique: Mode and median: Simple technique of filling missing values with mode (for categorical variables) and median (for continuous variables) of complete values in a variable. Implemented with basic R language functions. Is a very simple method and it is used as a base result for more complex algorithms to compare. mice(van Buuren and Groothuis-Oudshoorn 2011b): Package allows to perform multivariate imputation by chained equations (MICE), which is a type of multiple imputation. The method is based on Fully Conditional Specification, where each incomplete variable is imputed by a separate model. Imputation method from mice package don’t require any form of help, because they can impute numeric and categorical features. missMDA(Josse and Husson 2016): Package for multiple missing values imputation. Data sets are imputed with the principal component method, regularized iterative FAMD algorithm (factorial analysis for mixed data). Firstly estimation of the number of dimensions for factorial analysis is essential. missFOREST(Stekhoven and Buehlmann 2012b): Package can be used for imputation with predictions of the random forest model, trained on complete observations. The package works on data with complex interactions and non-linear relations. Enables parallel calculations. Algorithm can be used on both numeric and categorical features and is capable of performing imputation without any help of other methods. softImpute(Hastie and Mazumder 2015b): Package for matrix imputation with nuclear-norm regularization. The algorithm works like EM, solving an optimization problem using a soft-thresholded SVD algorithm. Works only with continuous variables. SoftImpute package works only with numeric features. To compare it with other algorithms on the same data, we use softImpute for numeric variables and mode for categorical variables. Alternatively, it is possible to use softImpute for numeric features and different algorithms for categorical variables, but we decided that this approach may lead to unreliable results. VIM(Kowarik and Templ 2016c): Package for visualization and imputation of missing values. It offers iterative robust model-based imputation (IRMI). In each iteration, one variable is used as a response variable and the remaining variables as the regressors. This method additionally creates new columns with information whether the observation was imputed or not. We decided to do not use these columns, because other methods do not create them. After imputation, we add back target variable to both sets. All methods work on the same parameters for all data sets. Not all methods were able to impute all 14 data sets. This fact will be taken into account in the subsequent analysis of the results. 2.4.3.3 Model traing For classification task we use four classification algorithms: Extreme Gradient Boosting, Random Forest, Support Vector Machines, Logistic Regression All methods were implemented in mlr package (Bischl, Lang, et al. 2016b). For hyperparameters tuning, we also used methods from the same package. For all data sets, four classifiers were trained and tuned on the same train sets. To select parameters we used Grid Search. For all algorithms selected parameters were tuned (more information about parameters in package description): XGB (etc, gamma, max_depth, subsample) Random Forest (num.trees, min.node.size) SVM (gamma) Logistic Regression (alpha, nlambda) We will not focus on this part of the experiment. The most important part of this step is that every model training was carried out the same way. This means that differences in results can be caused only by the influence of previously used imputation technique. 2.4.3.4 Model evaluation After previous steps, we have got trained models and test sets. In the final step we evaluate models and imputations methods. For every imputation and algorithm we calculate F1 score expressed by formula \\(2\\frac{(precision)\\cdot(recall)}{precision + recall}\\) and accuracy. We chose accuracy as the basic and most popular metric, and F1 which is a better metric for unbalanced sets. A detailed discussion about results in the next section. 2.4.4 Results After the long and tedious process of data imputation, it is finally time to evaluate our methods of imputation. Methods were trained and tested on 14 data sets and evaluated strictly using 2 methods mentioned earlier. Before score analysis, it is worth mentioning that all algorithms were tested with optimal parameters, so the score should be quite meaningful. The Table 1 below presents average metrics achieved by models, depending on the imputation method. As described earlier, we decided to use two measures of algorithm effectiveness: F1 score Accuracy These two measures complement each other well because they allow us to measure well the effectiveness of our imputations and algorithms, on both balanced and unbalanced sets. Table 1 Method Accuracy F1 softImpute 0.86 0.75 mice 0.85 0.75 missForest 0.85 0.74 VIM 0.85 0.76 median 0.83 0.75 missmda 0.82 0.80 Our experiment did not find out the best imputation algorithm, but we can derive some interesting conclusions from it. First of all, we can think about the median/mode imputation as kind of baseline method. Surprisingly it seems to perform quite well, among the others, often quite sophisticated methods. ML models based on it achieved over 80% of accuracy on average. However, it is hard to decide whether it is a high score or not, because we are only able to compare algorithms between the others. To say anything more meaningful about our methods we shall look at the distribution of the scores to make a better analysis of performance. 2.4.4.0.1 Distributions of scores Taking a brief look at the distributions of accuracy score (Figure 1), it seems quite unclear which algorithm performs the best. All medians seem to be approximately on the same level and also the first and third quartile, of almost all scores distributions, have the same value. Only missMDA is a bit lower than the others, but this is too early to derive any conclusions. F1 scores (Figure 2) give us much more information. The first thing that becomes apparent after looking at that plot is the fact that we do have some very low values for every type of imputation. However, this is simply because some of our data sets were very small, saw neither of our algorithms was able to achieve recall above 0. Besides from that, once again scores of all algorithms were quite close to each other. It is also worth noting that not all algorithms managed to perform imputations on all data sets. For this reason, the analysis of only the means and distributions alone may not be fully objective. To create a fair comparison, we decided to use two different methods of comparing and classifying these algorithms, so that we will be able to choose our winner. 2.4.4.1 Median/mode baseline score To begin with, we decided to treat the median/mode as the basic form of imputation and we will compare it to all other methods. As a single measure of models, we understand the average F1 score made by all models on the specific imputation result. As a measure of imputation method, we calculate difference between measure of models on its result to measure on mode/median result. Table below presents how much the average measure calculated for every imputation method differed from the median/mode. Table 2 Method score softImpute 0.02 missForest 0.01 mice -0.02 missmda -0.11 VIM -0.11 Results achieved here (Table 2) are quite surprising. First of all the maximum gain in F1 score is only 0.02 and it is achieved by softImpute. But what can bewilder the most is the fact that only 2 out of 5 algorithms managed to perform better than the median/mode. Score achieved by SoftImpute may be in some way caused by the fact, that it is using mode imputation for categorical variables. For some data sets its imputation becomes very similar to mode/median, if there are many more categorical variables than numeric ones. These scores can raise questions about the whole purpose of using sophisticated algorithms, but of course, we are unable to say anything harsh about them yet. We can definitely note that we need to be we careful while using these algorithms, because they may lead to huge loss in scores, even up to -0.11, as here it is shown with missMDA. It is worth noting the order of the algorithms here, as we will use another method to compare the scores. 2.4.4.2 Second approach As a second approach, for each data set we decided to rank imputation methods according to the models scores, and then award points for each place. As a score of models for imputation method on the given data set, we understand as before the average F1 score made by all models on the imputation result. The best rank is 1 and it is given to the imputation method which had the highest average of models’ F1 scores. As a result, we sum up all of the points and the final score is the sum of the points across all data sets. Of course, the method with the lowest amount of points wins. This approach allows us to give the worse (highest) rank for the imputation methods which failed on a specific set and compare them fairly to others. Table 3 Method Score softImpute 38 median 39 missForest 48 mice 49 VIM 58 missmda 69 Scores achieved here (Table 3) resemble these achieved before. The order is not the same, but the winner remains the same. Once again median/mode imputation managed to perform very well, outperforming many complicated algorithms. We can say that our scores are stable, so conclusions taken from these scores can be taken seriously. 2.4.4.3 Times As a final tool of comparing algorithms, we decided to compare times of imputation. Obviously, even the algorithm with the best score, but with awful imputation time is considered useless, so this is important to take that factor to final evaluation. As we can see (Figure 3) times of imputations made by each algorithm can vary heavily. There is no point in creating rankings like we did to analyze scores achieved by algorithms, but it is simply worth noting that our previous winner - softImpute is very quick, its plot looks the same as the median/mode plot, which is quite an achievement. On the other side, there is Mice, which times can reach very high values. 2.4.5 Summary and conclusions Summing up all of our work we can say, that we managed to find plenty of interesting observations about all of these imputation algorithms and the way they deal with different data. However, it is hard to issue the final judgments, because we only had 14 data sets available. Despite that, we have several conclusions. First of all, it is worth noting how well simple methods of imputation have managed to perform. Median/mode imputation was a tough competitor and have been outperformed rarely in our tests. The second issue is the matter of choosing the optimal metric for checking the performance of imputation algorithms. We have shown 2 different approaches and scores they achieved differed slightly. Taking that into consideration we cannot say that we have a clear winner in our competition, but we can give some sort of advice for all interesting in imputing their data sets. The advice is quite simple, but what we have managed to show quite powerful. The case is to start imputing your data set with the most basic method, which is median/mode imputing, and then trying to beat its score with a different algorithm, for example softImpute, which in our tests managed to perform quite well. References "],
["imputation-techniques-comparison-in-r-programming-language.html", "2.5 Imputation techniques’ comparison in R programming language", " 2.5 Imputation techniques’ comparison in R programming language Authors: Mikołaj Jakubowski, Marceli Korbin, Patryk Wrona (Warsaw University of Technology) 2.5.1 Abstract Machine Learning (ML) models are nowadays routinely deployed in domains ranging from medicine to business. The majority of them do not accept missing values in a dataset and this issue has to be dealt with using imputation. To address the problem of choice of imputation technique, we compare some of the imputation algorithms, basing on ML models’ quality and considering different datasets’ sizes and missing values percentages. The imputation methods can either be easily implemented or are offered in public R packages; they differ in both algorithmic and programming complexity. Our results indicate that successful imputation does depend on used imputation methods or datasets we work with. 2.5.2 Introduction &amp; Motivation Nowadays, the problem of missing data has become ubiquitous in our everyday lives in each domain, for example medicine, education, criminology, politics or business affairs. There are three main problems that missing data might cause: a substantial amount of bias, more arduous handling and analysis of the data and reductions in efficiency. Moreover, many machine learning models cannot be used because of missing data. Imputation bases on replacing missing data with an estimated value based on other available information. There are numerous kinds of imputation methods which can be divided into simple methods as well as methods involving more sophisticated algorithms. Many programming and statistical environments support data imputation via libraries and packages. In the past, there was some evaluation and testing on available imputation methods. Such research has been already done by Fiona M. Shrive (2006), not using machine learning models, or by José M. Jerez (2010), using such models. Nevertheless, each research was conducted only on 1 dataset and using only 1 metric. One could say that the testing imputation techniques with machine learning models is an open domain for research. Effectively, data science and statistical modelling are both much younger domains than mathematics or physics. Besides, it is hard to say if an imputation algorithm’s output is the proper one, because there is no data to compare with. In order to assess an imputation method, one could make use of some statistical approach like evaluating machine learning predictions, changing data by intentionally removing a part of it, or combining both processes. In this work, we compare imputation techniques by using predictions of machine learning models, as well as confronting the amount of time each algorithm took to perform an imputation task. We have used RStudio due to its numerous packages giving access to implementation of various imputation methods. The whole evaluation is repeated on several datasets, varying by size and missing data percentage. All these datasets are available online and come from OpenML. Our goal was to find the best and the worst imputation method in function of input data and the amount of time to impute, that is, the imputation real-life circumstances. We evaluate the methods basing primarily on the quality of machine learning models, which is measured by means of two measurement methods. We used R language in RStudio environment because of packages and libraries having already implemented imputation algorithms as well as multiple machine learning models. 2.5.3 Methodology 2.5.3.1 Imputation methods and classification algorithms Throughout the whole time of carrying out the study, the following imputation methods were being used: IRMI: during the whole iteration, one variable is used as a response variable, whereas the rest are used as the regressors. The whole information is used to impute the response column (Matthias Templ 2011). hotDeck: replacing missing values in an instance with the values present in a similar row. The record to copy values from may be chosen randomly or deterministically (Rebecca R. Andridge 2010). k nearest neighbours: a point value is approximated by the values of the closest points, based on other variables (Obadia 2017). missForest: a random forest is used as a multiple imputation scheme composed by classification and regression trees. Its error estimates deliver the independent imputation error calculation, without a test set necessary (Daniel J. Stekhoven 2011). mean, median or dominant: a simple, easily implementable function, which replaces all the missing values in every column independently, the value choice method being based on the datatype. Continuous features (e.g. float values) have their NAs replaced by the mean of all the present values; categorical ordinal variables (e.g. integer values) are imputated by the median, while categorical nominal columns (character values) become completed with the dominant value among remaining ones. The first three were imported from the R package VIM, whereas missForest imputation is based on another R package of the same name. The machine learning algorithms we evaluated during the study are the following classification algorithms: logistic regression (package stats) naive Bayesian classificator (package e1071) binomial regression (package stats) random forests (package ranger) 2.5.3.2 Datasets We used the following seven datasets available in the OpenML package to carry out all the calculations. name number of features number of rows number of missing values labor 17 57 326 colic 20 368 1199 credit-approval 16 690 67 hepatitis 20 155 167 vote 17 435 392 eucalyptus 16 736 455 echoMonths 10 130 97 2.5.3.3 Algorithm Given all of the above tools and resources, we developed an algorithm in order to study the imputations’ effectiveness. Each imputation method is carried out on a given dataset. The operation is run in a given number of iterations, during each of which the length of imputation is measured through reading the current time with the R base function Sys.time, immediately before and after the imputation being performed. After all iterations, the mean time is calculated. In the next step, every machine learning algorithm is tested. The dataset is divided into training data and test data, with a size ratio 9:1. Then, the current model is trained on the former and evaluated on the latter, through the use of mlr functions: makeClassifTask, makeLearner, train and predict. Having the model evaluated on a dataset, we measure its performance by means of accuracy and F1 score. We chose these two measures as the most symmetric ones, while discarding widely used precision and recall, due to the F1 score being more concise and giving just as many useful information on the dataset balance as a harmonic mean of the former two. All the imputation time and model evaluation score measurements are systematically written down and eventually output in a list of three matrices. 2.5.4 Results 2.5.4.1 Performance for each dataset All charts are self-explanatory. We tried to visually separate all 5 imputation methods by using random jitter. We created plots for each respective dataset. Based on the results below, and knowing characteristics of a dataset, it should be easy to estimate which method of imputation is the best for a desired use. 2.5.4.1.1 labor: 17 features, 57 rows, 326 missing values For this dataset IRMI &amp; kNN are relatively good, except for naiveBayes model, where kNN was the worst. 2.5.4.1.2 colic: 20 features, 368 rows, 1199 missing values For colic dataset the results are similar - IRMI and kNN have the highest accuracy and F1 metrics’ values. 2.5.4.1.3 credit-approval: 16 features, 690 rows, 67 missing values This dataset was different - the best imputation methods are hotdeck and missForest. 2.5.4.1.4 hepatitis: 20 features, 155 rows, 167 missing values For this dataset the missForest imputation was irrevocably the best. Besides, mean/med/dom imputation method performed quite well. 2.5.4.1.5 vote: 17 features, 435 rows, 392 missing values The best imputation method for vote dataset is hard to discern. However, IRMI method seems to averagely perform in the best way. kNN and missForest were again the worst imputation methods for naiveBayes ML model. 2.5.4.1.6 eucalyptus: 16 features, 736 rows, 455 missing values There were cases where simple mean/med/dom imputation revealed to be much better than other imputation methods - it performed surprisingly fine. IRMI achieved high scores as well. 2.5.4.1.7 echoMonths: 10 features, 130 rows, 97 missing values For echoMonths dataset, the excellent performance of IRMI can be easily seen. After kNN imputations, the models achieved low accuracy and F1 scores comparing to other imputation methods. 2.5.4.1.8 Overall performance Considering all datasets and taking the average Accuracy and F1 metrics’ scores, one could deduce good performance of missForest algorithm as well as a surprisingly high scores of a simple mean/median/dominant method. On the other hand, kNN and IRMI achieved relatively poor scores. Nevertheless, there were such datasets such as a small dataset called labor where the latter two imputation methods performed much better. 2.5.4.2 Charts conclusions and average time measurements Taking a glance at all charts, we are able to conclude which methods work best in which instances. Note that this recipe for choosing imputation method is partially subjective. That is, only certain amount of datasets were measured and there might exist some out there not following the rules we provide. Having a small dataset with middling amount of missing data (~5%) - use IRMI. Having a small dataset with significant amount of missing data (over 15%) - use IRMI or kNN. For this instance they are both quite competitive. Having a medium dataset with middling amount of missing data (~5%) - use MissForest. Having a big dataset with significant amount of missing data (over 15%) - again, IRMI and kNN provide simmilar results. Having a big dataset with tiny amount of missing data (below 1%) - hotdeck and MissForest preform equally as good. Having a big dataset with middling amount of missing data (~5%) - once again, IRMI seems to be the safest choice. Wanting to choose the safest option regardless - MissForest is the safest option overall. Other than performance measurements, we also provide average time taken by each algorithm, whose computation has already been mentioned as a part of our methodology. We are excluding mean, median or dominant method as the only one not being the most advisable in any case, according to the aforementioned rules. ML algorithm average time IRMI 5,96s missForest 0,94s kNN 0,9s hotdeck 0,22s For the biggest explored dataset, the highest IRMI time was 17 seconds. Generally, none of these algorithms are highly time consuming. However, it may be worth to acknowledge that sometimes, when we are dealing with a huge dataset and do not dispose a lot of time, IRMI might not be a good choice - especially since missForest most often produces better results. 2.5.4.3 Ranking We also created a leaderboard of imputation methods in function of used machine learning model. We used 2 metrics separately to evaluate classification models - Accuracy and F1 Score. Imputation methods with the highest Accuracy and F1 Score obtain the lowest ranking - that is the best. We summed up the rankings when considering multiple datasets - drafts obtained the same that is a better ranking. The data used for classification was divided into small and big datasets to determine whether the imputation methods’ scores vary among the size of the dataset. All datasets have similar number of features, so we set the threshold of a big and small dataset as 10,000 values. In the result, as small datasets (with less than 10,000 values each) we chose labor, colic, hepatitis, vote and echoMonths: Accuracy ranking ML algorithm logreg naiveBayes binomial ranger IRMI 3 1 3 1 missForest 4 2 4 2 hotdeck 5 4 5 2 kNN 1 5 1 2 mean/median/dominant 2 3 2 5 F1 Score ranking ML algorithm logreg naiveBayes binomial ranger IRMI 3 1 3 1 missForest 3 1 3 2 hotdeck 5 5 5 5 kNN 1 4 1 3 mean/median/dominant 2 3 2 4 We can deduce from the above ranking that missForest beats other imputation methods when we deal with small datasets. Mean/median/dominant imputation is also convenable in this case - in exception of naiveBayes model. The worst model is kNN. Bigger datasets (with 10,000 or more values) are credit-approval and eucalyptus: Accuracy ranking ML algorithm logreg naiveBayes binomial ranger IRMI 3 1 3 1 missForest 4 2 4 2 hotdeck 5 4 5 2 kNN 1 5 1 2 mean/median/dominant 2 3 2 5 F1 Score ranking ML algorithm logreg naiveBayes binomial ranger IRMI 3 1 3 1 missForest 3 1 3 2 hotdeck 5 5 5 5 kNN 1 4 1 3 mean/median/dominant 2 3 2 4 IRMI and kNN revealed to be the most accurate as far as the bigger datasets are concerned. It is also important to notice that hotdeck imputation from VIM package was unanimously the worst independently of used model. All datasets: Accuracy ranking ML algorithm logreg naiveBayes binomial ranger IRMI 5 2 5 1 missForest 2 1 2 1 hotdeck 4 4 4 3 kNN 3 3 3 5 mean/median/dominant 1 5 1 3 F1 Score ranking ML algorithm logreg naiveBayes binomial ranger IRMI 3 2 3 2 missForest 2 1 2 1 hotdeck 5 5 5 5 kNN 3 3 3 4 mean/median/dominant 1 4 1 3 The above ranking shows that the missForest imputation algorithm from the package of the same name has achieved the best scores independently of dataset’s size. On the other side, hotdeck performed the worst and the bigger the dataset is, the lower its performance was. 2.5.5 Conclusions From the ranking, one can deduce that depending on dataset’s size, missForest, kNN and IRMI imputation achieved the best scores. missForest was the most appropriate to deal with small datasets, while kNN and IRMI were outstanding at bigger datasets’ imputation. Moreover, from the ranking we deduce that if we use random hotdeck imputation from VIM package, the bigger the dataset is, the lower this imputation method’s performance is. Enough to say that we gain time in exchange for imputation corectness but the difference is almost negligeable and requires future testing. An appropriate choice of imputation method is very important and should be made depending on the given task. Nevertheless, we are aware of the necessity of future tests on higher number of datasets. Our number of big datasets is 2 and that is why our reasoning was limited. References "],
["how-imputation-techniques-interact-with-machine-learning-algorithms.html", "2.6 How imputation techniques interact with machine learning algorithms", " 2.6 How imputation techniques interact with machine learning algorithms Authors: Martyna Majchrzak, Agata Makarewicz, Jacek Wiśniewski (Warsaw University of Technology) 2.6.1 Abstract Imputation of missing values is a common step in the machine learning process. Many real-life datasets contain incomplete observations and dealing with them is a key part of modelling as most of the algorithms provided by R packages require complete data. This report aims to measure the influence of five different imputation methods on the performance of selected classification models. Simple methods such as mean, median, and mode are compared with advanced imputation techniques from specialized R packages - mice, VIM, and softImpute. As tested algorithms, Recursive Partitioning And Regression Trees, Naive Bayes, Ranger (Random forest), Linear Discriminant Analysis, and k-Nearest Neighbours were chosen. Their prediction effectiveness is assessed by F1 score to provide a proper measure for both balanced and imbalanced data. 2.6.2 Introduction and Motivation Missing data occurring in datasets is a common and sometimes difficult problem. Many real-world datasets, such as, for instance, medical records, are often incomplete. Unfortunately, in the field of machine learning, this is an issue which must be handled because many algorithms cannot work with missing values. Therefore, dealing with them is an important part of reprocessing data in the machine learning process. The first important step is to identify the missing data pattern (or mechanism). We commonly distinguish 3, according to Donald B. Rubin (1976) : Missing Completely at Random (MCAR), Missing at Random (MAR) and Missing Not at Random (MNAR), however, we will not focus on them in this study. One way of getting rid of missing data is “omission” - removing observations or variables with at least one missing value. However, this strategy is frequently inappropriate because we can lose some important data or significantly reduce it if the percentage of missing values is high. Fortunately, imputation serves the need better. By definition, imputation is a process of replacing missing data with substituted values. We can distinguish 2 types of imputation. First one is single imputation - we the information from only one variable to impute missing values. It includes the basic methods such as imputing with mean or mode, which are fast and easy to implement, however they do not guarantee a very good performance. The second one is multiple imputations, in which we impute many values (perform imputation many times), leading to creating many complete datasets. This type includes more sophisticated strategies for instance the ones using tree-based models, which usually result in better models, but enhance the algorithm’s complexity and computational time. For the R language, there are implemented multiple imputations techniques in many different packages. It is often hard to choose the best approach. Our goal is to examine how these methods perform on different datasets and how they interact with different classification algorithms. In order to do that, we impute each dataset with five chosen imputations and then on each one we execute modelling with selected algorithms. We base our conclusions about imputations influence on the values of F1 measure and our own ranking. 2.6.3 Methodology Our work can be divided into 4 major stages: data preparation, imputation, classification models’ training, and finally, evaluation of their performance. All of them are outlined in detail in the following paragraphs. 2.6.3.1 Data preparation In the following table are presented the datasets used for this experiment, along with their OpenML ID number, name, number of instances, number of features and number of missing values. The datasets vary in size and number of missing values. Dataset ID Name Instances Features Missing Percentage of missing 1018 ipums_la_99-small 8844 56 34843 0,07 1590 adult 48842 13 6465 0,01 188 eucalyptus 736 16 455 0,04 23381 dresses-sales 500 13 955 0,15 27 colic 368 20 1199 0,16 29 credit-approval 690 16 67 0,01 38 sick 3772 28 2293 0,02 4 labor 57 17 326 0,34 40536 SpeedDating 8378 123 18570 0,02 41278 stem-okcupid 45907 20 139693 0,15 55 hepatitis 155 20 167 0,05 56 vote 435 17 392 0,05 6332 cylinder-bands 540 34 999 0,05 944 echoMonths 130 10 97 0,07 Tab. 1. OpenML datasets used in the study As seen in the table, we used 14 datasets from OpenML library, all of which are designed for binary classification. Before starting the experiment, each of them was individually preprocessed, what usually included reducing the number of categories (for instance by converting strings to lower case or dividing date column into more features) or simply removing features with too many of them, removing features which didn’t contain any useful information, and correcting some spelling mistakes. 2.6.3.2 Imputation strategies Each prepared dataset was split into train set (80% of observation) and test set (20% of observation). They are imputed separately, using the methods described below, to avoid data leakage. The imputations, that were performed and analyzed in our study include: mean/mode imputation One of the basic techniques replaces missing values with a mean (for continuous variables) and mode (for categorical variables) of complete values in the given variable. Implemented with basic R functions. mice (predictive mean matching) Performs multivariate imputation by chained equations, meaning it creates multiple imputations (replacement values) for multivariate missing data. Implemented with mice() function (with method parameter set to “pmm”) from mice package (S. V. Buuren 2020). k-nearest neighbours An aggregation of the non-missing values of the k nearest neighbours is used as an imputed value. The kind of aggregation depends on the type of the variable. Implemented with kNN() function from the VIM package (Matthias Templ 2020). hotdeck Each missing value is replaced with an observed response from a “similar” unit. Implemented with hotdeck() function from VIM package (Matthias Templ 2020). softImpute combined with median/mode imputation For numeric variables function softImpute() from softImpute package (Trevor Hastie 2015) is used, fitting a low-rank matrix approximation to a matrix with missing values via nuclear-norm regularization. For remaining variables, missing values are imputed with median or mode, which is implemented with impute() function from the imputeMissings() package (Matthijs Meire 2016). 2.6.3.3 Classification algorithms We chose five different algorithms: two linear classifiers (NB, LDA), two tree-based models (RPART, Ranger), and one kernel estimation (KNN). For each train set and for every imputation method all five classifiers were trained. The modelling was performed using the mlr3 package (Michel Lang 2020). RPART (Recursive Partitioning And Regression Trees) Based on decision trees, works by splitting the dataset recursively, until a predetermined termination criterion is reached or no improvement can be made. At each step, the split is made based on the independent variable that splits the data best (results in the largest possible reduction in the heterogeneity of the dependent (predicted) variable). NB (Naive Bayes) Technique based on an assumption of independence between predictors (every pair of features being classified is independent of each other) and their equal contribution to the outcome, then the probability of an event occurring given the probability of another event that has already occurred is calculated. Ranger (Random forest) Constructs a large number of individual decision trees that operate as an ensemble, each tree returns a class prediction and the class that is the mode of the classes (in case of classification) of the individual trees becomes our model’s prediction. LDA (Linear Discriminant Analysis) Based on the dimension reduction technique, it finds a new feature space to project the data to maximize class separability. More precisely, it finds a linear combination of features that characterizes or separates two or more classes of objects or events. The resulting combination is used as a linear classifier, which uses Bayes’ Theorem to estimate probabilities of belonging to respective classes. KNN (k-Nearest Neighbour) An object is classified by a majority vote of its neighbours, being assigned to the class most common among its k nearest neighbours. The neighbours are measured by a distance function. After the training stage, the time has come to check how selected imputations and algorithms interact with each other. Again, for each - in this case - test set and every imputation all five classifiers listed above make a prediction. Assessing their performance is outlined in the next paragraph. 2.6.3.4 Model evaluation The performance of algorithms is evaluated using F1 Measure - the harmonic mean of the precision and recall. \\[F_1=2*\\frac{precision*recall}{precision+recall}\\] Due to that measure, a ranking of imputations for each dataset is created. Imputations are given ranks from 1 to 5: if imputations have the same F1 measure, they receive the same rank if imputation method failed to impute the data, it receives rank 5 The overall measure of the imputation performance is a mean of its ranks from all datasets. 2.6.3.5 Summary of the process Before entering the results section, let’s summarise the work that has been done up to this stage. Firstly, all datasets were simply preprocessed (if needed) to make the imputation easier. Next, each of them was split into train and test dataset. The ratio was 80 to 20, the same for all datasets, with the same random seed. Each train and test set was imputed separately, with 5 different methods. Then, on each train set 5 different classification models were trained and following, the prediction was made on test sets. Finally, each combination of a test set, imputation method and classification algorithm was assessed by F1 score and proposed ranking. 2.6.4 Results A missing value NA in the results table implicates, either that an imputation failed on this particular dataset, leaving missing values in it (when there are a few of them in a column), or that the model didn’t manage to perform on the entire dataset, no matter what imputation (when there is a full row of missing values). Additionally, for each dataset, the best result is bolded. 2.6.4.1 Results of Rpart classification algorithm TABLE 2.6: Tab. 2. Results of Rpart classification algorithm. Dataset Insert.mean Mice_pmm VIM_knn VIM_hotdeck softImpute ipums_la_99-small NA NA NA NA NA adult 0.903 0.903 0.903 0.903 0.903 eucalyptus 0.881 0.906 0.793 0.875 0.895 dresses-sales NA NA NA NA NA colic 0.757 0.667 0.667 0.732 0.757 credit-approval 0.886 0.886 0.886 0.886 0.886 sick 0.985 0.984 0.987 0.989 0.99 labor 0.75 0.333 0.667 0.75 0.429 SpeedDating 1 1 1 1 1 stem-okcupid NA NA NA NA NA hepatitis 0.4 0.5 0.667 0.4 0.444 vote 0.965 0.966 0.965 0.966 0.965 cylinder-bands 0.732 0.722 0.732 0.732 0.732 echoMonths 0.774 0.812 0.774 0.774 0.774 Tab. 2. Results of Rpart classification algorithm. Rpart model failed on all imputations of 3 of the datasets. Within those 3 datasets, all imputations will receive rank ‘5’, and therefore this will not affect their overall score within the Rpart model but will affect Rpart as a whole compared to the other models. Most imputations received an F1 score within the range &lt;0.6-0.99&gt;. There are, however, two datasets that stand out - SpeedDating with score 1.0 (probable overfitting) and labor MICE pmm and softImpute are significantly lower. Fig. 1. Ranking for Rpart classification algorithm. Generally, VIM knn and MICE pmm imputations gave the best results when combined with the Rpart model. 2.6.4.2 Results of Naive Bayes classification algorithm TABLE 2.7: Tab. 3. Results of Naive Bayes classification algorithm. Dataset Insert.mean Mice_pmm VIM_knn VIM_hotdeck softImpute ipums_la_99-small 0.125 0.126 0.126 0.124 0.125 adult 0.887 0.887 0.887 0.887 0.887 eucalyptus 0.863 0.874 0.886 0.874 0.868 dresses-sales 0.672 0.667 0.698 0.645 0.704 colic 0.519 0.465 0.528 0.565 0.528 credit-approval 0.787 0.787 0.787 0.787 0.787 sick 0.948 0.945 0.948 0.945 0.943 labor 0.667 0.8 0.429 1 1 SpeedDating 0.886 0.885 0.887 0.888 0.886 stem-okcupid 0.844 0.858 0.843 0.857 0.844 hepatitis 0.6 0.6 0.6 0.545 0.6 vote 0.915 0.915 0.923 0.915 0.915 cylinder-bands 0.565 0.385 0.571 0.543 0.552 echoMonths 0.865 0.824 0.8 0.788 0.833 Tab. 3. Results of Naive Bayes classification algorithm. Naive Bayes was able to perform classification on all imputations of all the datasets. It is the only model to have done that. The F1 measure values vary a lot more than in the Rpart model, from around 0.12 in the ipums_la_99-small dataset to 1.0 in two imputations of the labor dataset. Fig. 2. Ranking for Naive Bayes classification algorithm. Generally, MICE pmm imputation gave the best results when combined with the Naive Bayes model. 2.6.4.3 Results of Ranger classification algorithm TABLE 2.8: Tab. 4. Results of Ranger classification algorithm. Dataset Insert.mean Mice_pmm VIM_knn VIM_hotdeck softImpute ipums_la_99-small 0.185 0.156 0.142 0.155 0.156 adult 0.915 0.916 0.916 0.916 0.915 eucalyptus 0.908 0.908 0.918 0.908 0.902 dresses-sales 0.758 0.715 0.723 0.723 0.763 colic 0.667 0.688 0.765 0.667 0.611 credit-approval 0.881 NA 0.881 0.887 0.881 sick 0.986 0.985 0.987 0.985 0.986 labor 0.857 NA 0.857 0.857 0.857 SpeedDating 0.996 0.995 0.996 0.995 0.996 stem-okcupid 0.885 0.885 0.889 0.885 0.883 hepatitis 0.667 0.667 0.75 0.5 0.444 vote 0.974 0.983 0.974 0.966 0.974 cylinder-bands 0.776 NA 0.794 0.8 0.765 echoMonths 0.812 NA 0.75 0.812 0.788 Tab. 4. Results of Ranger classification algorithm. The only missing values are for the MICE pmm imputation on 4 datasets, on which this imputation method failed, leaving missing values in the dataset. Those will all receive rank 5, decreasing the overall score of the MICE pmm imputation. The other F1 measure values are mostly between 0.6 and 0.99, with some outliers like the ipums_la_99-small dataset with values around 0.15. Fig. 3. Ranking for Ranger classification algorithm. Generally, softImpute and VIM hotdeck imputations gave the best results when combined with the Ranger model. 2.6.4.4 Results of LDA classification algorithm TABLE 2.9: Tab. 5. Results of LDA classification algorithm. Dataset Insert.mean Mice_pmm VIM_knn VIM_hotdeck softImpute ipums_la_99-small 0.286 0.317 0.291 0.24 0.286 adult 0.897 0.898 0.899 0.898 0.897 eucalyptus 0.9 0.905 0.894 0.894 0.906 dresses-sales NA NA NA NA NA colic 0.571 0.457 0.634 0.634 0.611 credit-approval 0.874 NA 0.874 0.872 0.859 sick 0.977 0.977 0.978 0.976 0.977 labor 1 NA 0.8 1 0.857 SpeedDating 0.995 0.995 0.995 0.994 0.995 stem-okcupid NA NA NA NA NA hepatitis 0.6 0.6 0.545 0.545 0.6 vote 0.965 0.965 0.965 0.965 0.965 cylinder-bands 0.648 NA 0.676 0.638 0.638 echoMonths 0.865 NA 0.865 0.865 0.865 Tab. 5. Results of LDA classification algorithm. The LDA model failed to perform on 2 datasets. Besides that, there are 4 missing values in the second column, because this imputation method failed, leaving missing values in the dataset. Those will all receive rank 5, decreasing the overall score of the MICE pmm imputation. The F1 score values are mostly between 0.6 and 0.99, with some outliers like the ipums_la_99-small dataset with values around 0.28. Fig. 4. Ranking for LDA classification algorithm. Generally, VIM hotdeck imputation gave the best results when combined with the LDA model. 2.6.4.5 Results of KKNN classification algorithm TABLE 2.10: Tab. 6. Results of KKNN classification algorithm. Dataset Insert.mean Mice_pmm VIM_knn VIM_hotdeck softImpute ipums_la_99-small 0.221 0.201 0.242 0.21 0.221 adult 0.887 0.889 0.889 0.89 0.887 eucalyptus 0.857 0.852 0.852 0.857 0.857 dresses-sales 0.742 0.742 0.742 0.742 0.713 colic 0.565 0.571 0.605 0.578 0.511 credit-approval 0.851 NA 0.851 0.851 0.843 sick 0.98 0.978 0.98 0.978 0.978 labor 0.667 NA 0.857 0.8 1 SpeedDating 0.914 0.916 0.916 0.915 0.915 stem-okcupid 0.883 0.883 0.883 0.883 0.883 hepatitis 0.667 0.857 0.667 0.667 0.667 vote 0.949 0.957 0.948 0.957 0.949 cylinder-bands 0.763 NA 0.685 0.676 0.784 echoMonths 0.778 NA 0.765 0.788 0.778 Tab. 6. Results of KKNN classification algorithm. The KNN model managed to perform on all dataset, except for the MICE pmm imputations of 4 datasets, which failed, leaving missing values in the dataset. Those will all receive rank 5, decreasing the overall score of the MICE pmm imputation. The F1 score values are mostly between 0.65 and 0.99, with some outliers like the ipums_la_99-small dataset with values around 0.28 and colic dataset with values around 0.57. Fig. 5. Ranking for Rpart classification algorithm. Generally, mean/mode imputation gave the best results when combined with the Weighted KNN model. VIM knn and softImpute were second best. 2.6.4.6 Comparing Models The heatmap below presents the mean from ranks of given imputation on all the datasets, combined with each classification model. Fig. 6. Heatmap of the ranking of all classification algorithms and imputations. It clearly shows, that no imputation performs the best with every model. Every model has one or two imputations, that interact with it better than the others, but those are different for every classifier. The efficiency of the model combined with a particular imputation depends on numerous factors. In the next chapter the impact of one possible factor - the percentage of missing values in the dataset before the imputation - will be discussed. 2.6.4.7 The impact of the percentage of missing values During the study, it was brought to the researchers’ attention, that some of the unusual results appeared for the datasets, that originally had a higher than average percentage of missing values. Therefore, an analogical chart was created, using only the following 4 datasets with the highest percentage of missing values. Dataset ID Name Instances Features Missing Percentage of missing 23381 dresses-sales 500 13 955 0,15 27 colic 368 20 1199 0,16 4 labor 57 17 326 0,34 41278 stem-okcupid 45907 20 139693 0,15 Tab. 7. Datasets with a high percentage of missing data Fig. 7. Heatmap of the ranking of all classification algorithms and imputations for selected datasets with a high percentage of missing data. The results suggest, that the Weighted KNN model generally performs better than other classification models on those datasets, no matter what imputation method it is combined with. Similar results are achieved for the Ranger model combined with VIm hotdeck, softImpute and mean/mode imputations and Naive Bayes model combined with VIM knn, MICE pmm and mean/mode imputation. The appearance of such dependence may suggest, that number of missing values in a dataset may be a contributing factor to the efficiency of an imputation method combined with a classification model. 2.6.5 Summary and conclusions The conclusions from this study can be divided into those, that apply to choose the best imputation method if the model is imposed, and those which can help to choose a classification model in the first place. 2.6.5.1 Choosing an imputation for a model The choice of the best imputation technique for a particular classification model is not a simple task and it should be made considering a multitude of factors. From this study it seems that the best methods for the following models are: Rpart - VIM knn Naive Bayes - MICE pmm Ranger - VIM hotdeck and softImpute LDA - VIM hotdeck KKNN - mean/mode imputation This results may be affected by the specificity of the datasets and should not be treated as final recommendations. However, when reliability is needed, the only method we do not recommend is MICE pmm - it failed to impute 4 of the 14 datasets. 2.6.5.2 Choosing a model It is worth noting, that from all the models, only Rpart and Naive Bayes managed to perform on those datasets, that MiCE failed to impute, despite the fact, that they still had missing values. Besides, Naive Bayes was able to perform on every single one of the 14 datasets, which makes it the champion of this study, as far as the reliability of the model is concerned. When choosing a classification model for a dataset with the percentage of missing values, Weighted KNN, Ranger and Naive Bayes may provide better results than Rpart and LDA models. 2.6.5.3 Further reseach There probably is no one imputation method that exceeds all the others in the matters discussed in this study. Although, further research, based on more datasets and imputation techniques, could help to provide more clearer guidelines for when to choose which imputation method, taking more than just the classification model used into consideration and when to choose which classification model. References "],
["interpretability.html", "Chapter 3 Interpretability", " Chapter 3 Interpretability Interpretability "],
["building-an-explainable-model-for-ordinal-classification-on-eucalyptus-dataset-meeting-black-box-model-performance-levels-.html", "3.1 Building an explainable model for ordinal classification on Eucalyptus dataset. Meeting black box model performance levels.", " 3.1 Building an explainable model for ordinal classification on Eucalyptus dataset. Meeting black box model performance levels. Authors: Karol Saputa, Małgorzata Wachulec, Aleksandra Wichrowska (Warsaw University of Technology) 3.1.1 Abstract It is popular nowadays to use black box models, which are complex and often unintuitive e.g. neural networks, in order to achieve high model performance without having to go in-depth into domain knowledge. However in some fields, e.g. banking or medicine it is crucial to understand not only what was the model outcome but also how was it obtained and why something was recommended - in other words we want to be able to explain models’ decisions. This is possible when using simpler models such as linear models or decision trees. In this article we argue that by using exploratory data analysis, domain knowledge, variable selection and other model improvements, it is possible to build simpler and easily interpretable models that are as accurate as black box models. For a specific task of ordinal classification and exemplary dataset of Eucalypti, we provide a model (with AUC metric better than considered black box model) with an explanation of techniques applied. 3.1.2 Introduction and Motivation In the classification problems, the main goal is to map inputs to a categorical target variable. Most machine learning algorithms assume that the class attribute is unordered. However, there are many problems where target variable is ranked, for example while predicting movie ratings. When applying standard methods to such problems, we lose some informaton, which could improve our model performance. Main goal of machine learning models is predicting target with accuracy as high as possible. There are many complex ML models which, after sufficient amount of time, will give us very good predictions. Such models are called ‘black boxes’. However, there are many situations when we want to know why each prediction was made. ‘Black box’ models usually do not give us such information. Hence, there is necessity of constructing simple, explainable machine learning models, which could be easily interpreted by non-technical person. This paper presents various methods to make an explainable machine learning model for ordinal classification problem. The aim is to achieve better results than ‘black box’ model does. We will test some existing approaches to ordinal classification and take advantage of analysis and imputation of missing data, feature transformation and selection, as well as knowledge from exploratory data analysis. Our experiments are based on ‘eucalyptus’ dataset from OpenML [7]. The dataset’s objective is to find the best seedlot for soil conservation in seasonally dry hill country. Predictions are made depending on features such as height, diameter and survival of the plants. Target variable is ordered - it is represented by values ‘low’, ‘average’, ‘good’ and ‘best’. This article is structured as follows: Section 2 provides a brief description of avaliable ordinal classification methods and existing black box to explainable model comparisons. Section 3 explaines the methodology used throughout our experiment. Section 4 shows the results obtained for various models and section 5 containes an explanation of the best achieved explaianble model. Section 6 sums up our findings. At the end, in section 7 there is a list of references we used in our research. 3.1.3 Related Work 3.1.3.1 Ordinal classification as a regression task As described in [1], one of the most fundamental techniques is to cast target labels to a sequence of numbers. For example \\(\\{low, good, best\\}\\) changes to \\(\\{0,1,2\\}\\). Then, standard regression can be applied. There is additional information about the classes in comparison to the usual nominal classification. Also, a metric used is different than in a classification task - mean square error used in a regression task takes into account similarities between two labels when a conversion is applied. 3.1.3.2 Transformation to multiple binary classification problems Ordinal classification taxonomy given in [1] specifies decompositions of classification task into binary problems as a separate technique to cope with ordinal classification. A more detailed algorithm and performance test are presented in [2] which suggests better scores obtained using binary decomposition than using a single model during a test on multiple datasets. Based on [2], as a binary decomposition we define a conversion of a target classes \\(\\{low, good, best\\}\\) into pairs of binary classes (True/False) \\(\\{\\{False, False\\}, \\{True, False\\}, \\{True, True\\}\\}\\) determining whether outcome is better than \\(low\\) and better than \\(good\\). After classification is made, probabilites for original classes are calculated as explained in [1], [2]. 3.1.3.3 Comparing black boxes and white boxes As presented in the Introduction and Motivation section, we aim to achieve good results in a machine learning task using an explainable and interpretable model (white box). A black box model is used as a baseline model. Detailed justification of that approach can be found in [3]. Herein our work further develops this issue by presenting a model with the possibility to extract, understand and reuse of its decision logic for a specific task of ordinal classification. 3.1.4 Methodology The aim of this article is to build the best interpretable model for an ordinal classification problem and comparing it to a black box model. First undertaken step was dividing the data into training and test sets, consisting of 70% and 30% of all data, respectively. Since the considered dataset has a categorical target variable, the division was done using random sampling within each class of the target variable in an attempt to balance the class distributions within the splits. Since results obtained by the models were dependent on the random seed that was used when dividing the sets, the results compared thoughout this article are averaged out over 100 different random seeds, assuring a valid comparison could be done. 3.1.4.1 Initial preprocessing In order to get a legitimate comparison, the data was initially preprocessed in such a way as to assure that both models’ performances are compared on the same test set. This initial preprocessing included: deleting the observations with ‘none’ value in the target variable from both the training set and the test set; deleting observations with missing values from test set, resulting in a 6% decease of the test set observations. It is important to note that missing values are still present in the training set. The reason why the missing data is deleted from the test set is that many of the explainable models cannot be run with missing data present. This means that the missing values will have to either be deleted or imputed later on. This leads to a possibility that the explainable model will impute missing data differently than the black box model, resulting in two different tests sets. And, if - instead of imputing missing data - we decide to delete it in order to make running explainable model possible, then the obtained test sets will differ in number of rows, making it impossible to draw any meaningful conclusions. Hence the missing data were deleted from the test set. 3.1.4.2 Running the black box model The black box model chosen for comparison is an extreme gradient boosting model. After the initial preprocessing the xgboost model was trained on the training set and used to predict results on the test set. As this model can only deal with numerical data, categorical (factor) variables were transformed using one hot encoding. The training proces and prediction were done using the mlr [5] package in R, and the exact model specifications were the following: Parameters of extreme gradient boosting model Category Specification Learner classif.xgboost from package xgboost Type classif Name eXtreme Gradient Boosting; Short name: xgboost Class classif.xgboost Properties twoclass,multiclass,numerics,prob,weights,missings,featimp Predict-Type response Hyperparameters nrounds=200,verbose=0,objective=multi:softmax The quality of prediction was measured using the AUC (area under ROC curve) measure. We have also used other metrics suitable for ordinal classification task [4] and the results of all of the models are presented in table in the Results section of this article. However, AUC is the measure that we used to assess and compare each considered model. The AUC measure achieved by the black box is the one we tried to top using a simpler explainable model. 3.1.4.3 Running the basic version the explainable model We have chosen a tree model to be the considered explainable model, its exact specifications were the following: Parameters of decision tree model Category Specification Learner classif.rpart from package rpart Type classif Name Decision Tree; Short name: rpart Class classif.rpart Properties twoclass,multiclass,missings,numerics,factors,ordered,prob,weights,featimp Predict-Type response Hyperparameters xval=0 As this model cannot be run with missing data, they were deleted from the training set before training the model. Another step was deleting one from each of the one-hot-encoded variables (the default function transforms variable with n factor levels into n columns, but n-1 columns are sufficient as the n-th column is a linear combination of the remaining n-1 columns). This model performed worse than the black box model - the outcomes are presented in the Results section of this article. The AUC measure achieved by this model provides the base for this research, to which other models’ results will be compared to. 3.1.4.4 Improving the explainable model In the study, the explainable model was enhanced by applying existing approaches to ordinal classification, feature transformation and selection and missing data imputation. The refinement process consisted of, but was not limited to, the following: Splitting a multiclass classification problem into 3 binary classification problems - binary decomposition with 3 rpart models (as described in section 2.2) Changing the levels of the target variable:“low”, “average”, “good”, “best” into numeric values: 1, 2, 3, 4, respectively and running a regression rpart model. Imputing missing data in the training set. Selecting variables: deleting the site names and specific location tags. Transforming Latitude variable from factor to numeric. The fourth step has a scientific justification. The experiment for which the data was collected was focused on finding the best seedlot for soil conservation in seasonally dry hill country. All the data in this dataset comes from New Zealand, but there is a chance that the results of such experiment would be used for other geographical regions. So far our model was making the prediction based also on specific flat map coordinates and site names, that are present both in the training and the test set. This means it would be impossible to use this model for judging seedlots of eucalypti planted outside of New Zealand. To make this possible, we have decided to take away all the variables that give away the exact position of the seedlots, leaving features such as latitude and the characteristics of plants and their habitat. After each improvement the model was retrained and the results obtained on the test set were saved and compared with the previous version of the model. If the new change has improved the model’s performance on the test set then it became the base for further development. Instead, if it has not improved the model’s performance, the previous version of the model was being further developed. 3.1.5 Results Results obtained for each model are shown in the tables below: Explainable models: Metrics results for white-box models tested Model AUC MSE ACC ACC1 Percent Basic Explainable Model’s AUC Basic rpart 0.8259 0.5284 0.5835 0.9797 100.00% Three binary rparts 0.8430 0.5393 0.5816 0.9827 102.07% Regression rpart 0.8611 | 0. 0.4995 0.5815 | 0.93 0.9321 104.27% | Regression rpart with imputation 0.8598 0.5038 | 0.5798 | 0.930 0.9307 | 104.1 104.10% | Regression rpart with no location 0.8613 0.4996 0.5815 0.9323 | 104. 104.28% | Regression rpart with no location and numeric lattitide 0.8612 0.4993 0.5816 | 0.9323 | 104.2 104.28% | Black box models: Metrics results for black-box models tested Model AUC MSE ACC ACC1 Percent Basic Explainable Model’s AUC Xgboost with nrounds=5 0.8405 0.4998 0.6044 0.9830 101.77% Xgboost 0.8590 0.4467 0.6248 0.9873 104.00% We have visualised the AUC measure improvements, in comparison to the basic rpart (decision tree) model, on the graph below. As shown on the graph, after adding the improvements, the explainable model was able to outperform the black box model, which was the point of this study. The best model is a decision tree, which is defined for a regression task, and which includes variable selection (taking away specific location tags as explained in the methodology section). Results of different techniques in improving model performance in comparison to black box. The basic classification decision tree was used as a baseline. 3.1.6 Model explanantion Our final explainable model is a simple regression decision tree. The interpretation of tree-based models is very simple: we start from the root node and go to the next nodes via the edges, basing on the decisions made by model. Subsequent decisions are connected by ‘AND’. When we reach the leaf, we can read predicted value of the target. We can also easily compare the prediction of various “what if” scenarios. Moreover, there is no need to transform features used in model - there is no point in neither taking the logarithm of a variable nor normalize them in any other way. As described in [6], one of main advantages of decision trees is natural visualization: we create a graph based on choosen model. The graph is presented below. Depth of this model is 6. First divisions are made based on Vigour and Crown variables. We can conclude that small values of vigour leads to “low” or “average” utility feature. Graph of a decision tree structure of our model Looking into model features’ importance we can extract variables used by our model. The most important features are: vigour (73.3%) stem form (7.9%) height (7.6%) seedlot number (4.1%) survival (2.6%) crown form (2.3%) year of planting (2.2%) 3.1.7 Summary and conclusions In the study, three approaches to ordered classification were considered. On the studied Eucalyptus dataset, the ordinal classification as a regression task approach has given the highest AUC value. This could be due to the fact that the classes in the target variable were not equally distributed, and so the regression has performed better than multiclass classification or a set of binary classification problems. The final explainable model is a fairly simple decision tree, defined for a regression task, which includes variable selection. The variables excluded were mainly specific location tags, and deleting them means this model could be used for judging Eucalypti in other regions, not only the ones coming from New Zealand, where all the data comes from. Our goal was to build an explainable model that achieves results similar to a black box model. In fact, the model constructed in this article uses domain knowledge and application of different preprocessing and machine learning techniques and, as a result, performs better than a black box model. This shows that our goal was achievable and that explainable models can give comparable and sometimes even better results than black box models, in addition to a smaller computational cost and possibility to understand and explain decisions made to solve a problem. Different methods, as in [1] could also be applied in the future, to further improve the model built in this article. 3.1.8 References P. A. Gutiérrez, M. Pérez-Ortiz, J. Sánchez-Monedero, F. Fernández-Navarro and C. Hervás-Martínez, “Ordinal Regression Methods: Survey and Experimental Study,” in IEEE Transactions on Knowledge and Data Engineering, vol. 28, no. 1, pp. 127-146, 1 Jan. 2016, doi: 10.1109/TKDE.2015.2457911. Frank, Eibe &amp; Hall, Mark. (2001). A Simple Approach to Ordinal Classification. Lecture Notes in Computer Science. 2167. 145-156. 10.1007/3-540-44795-4_13 Rudin, C. Stop explaining black box machine learning models for high stakes decisions and use interpretable models instead. Nat Mach Intell Gaudette L., Japkowicz N. (2009) Evaluation Methods for Ordinal Classification. In: Gao Y., Japkowicz N. (eds) Advances in Artificial Intelligence. Canadian AI 2009. Lecture Notes in Computer Science, vol 5549. Springer, Berlin, Heidelberg mlr package: Machine Learning in R. Accessed 17 June 2020 from https://mlr.mlr-org.com/ Molnar, C. Interpretable Machine Learning. A Guide for Making Black Box Models Explainable. OpenML. Accessed 17 June 2020 from https://www.openml.org/d/188 "],
["predicting-code-defects-using-interpretable-static-measures-.html", "3.2 Predicting code defects using interpretable static measures.", " 3.2 Predicting code defects using interpretable static measures. Authors: Wojciech Bogucki, Tomasz Makowski, Dominik Rafacz (Warsaw University of Technology) 3.2.1 Abstract In building machine learning models, sometimes more important than effectiveness is their easy explanation. In the case of data on defects in the code that this article concerns, explainability helps to justify decisions on whether or not to accept a given piece of work. In this project, we are looking at the code data from NASA systems to see if measures that are transformations of simple variables in combination with a white-box model can be effective enough to achieve comparable results to black-box models. We iteratively build the white-box models by applying different methods to them and preserving those changes that improve the score relative to the black-box random forest model. The comparison is made using crossvalidation. The most important modifications among those that we consider are adding the measurements both proposed by McCabe and Halstead as well as those generated by us. We managed to build a decision tree model better than the black-box one, but contrary to expectations, neither the generated measures nor the original ones bring almost any improvement to the models’ results. The tree has been heavily tuned, which makes it still interpretable, but not in so obvious way. Great improvement in score was provided by adding synthetical data to the minority class, which may also raise doubts about the interpretability. We conclude that the data from this set does not sufficiently describe the complexity of the problem, and as a consequence, the measurements themselves, although they may give some indication, are also not sufficient. 3.2.2 Introduction and Motivation Since the very beginning of the computer revolution there have been attempts to increase efficiency in determining possible defects and failures in the code. An effective method to do so could bring many potential benefits by identifying such sites as early as at the code development stage and eliminating costly errors at the deployment stage. McCabe and Halstead proposed a set of measures that are based on static properties of the code (including basic values, e.g. number of lines of code or number of unique operators, as well as transformations of them, (1976) (1977)). In their hypotheses, they argue that these measures can significantly help to build models that predict the sensitive spots in program modules. However, it can be argued that the measures they propose are artificial, non-intuitive, and above all, not necessarily authoritative, not taking into account many aspects of the written code and program (Fenton and Pfleeger 1997). To support their hypotheses with, McCabe and Halstead collected information about the code used in NASA using scrapers and then used machine learning algorithms. In this article we use the above data sets to build a model that best predicts the vulnerability of the code to errors. We check whether static code measures (being transformations of basic predictors) significantly improve prediction results for the so-called white-box models (e.g. trees, linear regression and k nearest neighbors algorithm). Our goal is to build, using simple data transformations and easily explainable methods, such model that will achieve results comparable to the black-box model (such as neural networks or gradient boosting machines) used on data without advanced measures. We also want to compare the effectiveness of the measures proposed by McCabe and Halstead and compare them with the measures we have generated. 3.2.3 Dataset Our dataset comes from the original research of Halstead and McCabe. We obtain it by combining the sets from OpenML (Vanschoren et al. 2013) and supplementing them with data from the PROMISE repository (Sayyad Shirabad and Menzies 2005). It contains data collected from NASA systems written in C and C++ languages. The data is in the form of a data frame containing more than \\(15000\\) records. Each record describes one “program module”. – with this generic term, the authors defined the simplest unit of functionality (in this case, these are functions). Each record is described with a set of predictors, which can be divided into several groups: Basic measures (such as number of lines of code, number of operands, etc.). McCabe’s measures how complex the code is in terms of control flow and cross-references . Halstead’s measures for general code readability. Target column (1 if module contains defects, 0 if not). Source column we added, specifying from which subsystem the module came (the original 5 datasets came from different systems). The dataset is slightly imbalanced – about \\(20\\%\\) of records are classified as having defects. In order to verify our hypotheses, we decide at the beginning to remove the Halstead’s measures (which were transformations of the basic measures) from the collection to see if we are able to build an effective black-box model without them. We also wanted to remove McCabe’s measurements, but the basic measurements that he used to calculate his measurements are not preserved in the dataset, so we decide to keep them. There are not many records with openly missing data in the set (\\(&lt; 1\\%\\)), however, the values of some columns raise doubts – in the column containing information about the number of lines of code of a given module in many cases there is a value \\(0\\), which is not reliable. However, it turned out during the preliminary analysis that deleting those records significantly worsens the model score, so we decided to keep them. 3.2.4 Methodology Our research consist of the following stages: Data exploration. Initial data preparation. Building of black-box and white-box models and comparing them against the relevant measurements. Repeating the cycle: Improvement of white-box models by modifying their parameters or data. Measuring the effectiveness of the models built. Analysis of the resulting models. Keeping or rejecting changes for further work. Selection of the best white-box model and final comparison with the black-box model. During the step 4. in some cases we decide to take a step back and use other similar transformation or other order of transformations if we suspect that it can yield a better result. We use R programming language and popular machine learning project management packages – mlr (Bischl, Lang, et al. 2016c) and drake (Landau 2018). 3.2.4.1 Data exploration At this stage, we take a closer look at what the data looks like and we are analyzing their distributions, gaps, correlations and simple relationships. 3.2.4.2 Initial data preparation This stage consists mainly of merging the data sets, as mentioned earlier, and adding a source column (in fact, we add five indicator columns, which contain one-hot-encoded value, as models generally do not cope well with character columns). Since there is not much missing data, we impute them with the median, because this method is effective and fast. Imputation is necessary from the very beginning, as many models cannot cope with missing values. Since there were very few missing values, it does not affect significantly the result of those models that would still work. We do not carry out further transformations at this stage because we do not want to disturb the results of the next stage. 3.2.4.3 Starting models We build models on this almost unaltered data. We use one poorly interpretable model (black-box) – random forest, specifically ranger package (Wright and Ziegler 2017), because it is fast and low-effort. Among well interpretable models (white-boxes) used in our work there are: logistic regression (lm), decision tree (rpart), k-nearest neighbors algorithm (kknn). We train the models into data that we have divided into five folds with a similar distribution of the decision variable, on which we will perform cross-validation. Then we compare the results using commonly used measure – AUC (Area Under Curve) (Flach, Hernandez-Orallo, and Ferri 2011), which not only assesses whether the observations are well classified, but also takes into account the likelihood of belonging to a class. AUC is not the best measure to be used on imbalanced dataset. However, the unbalance here is not big enough to make this choice unreliable. We use AUC as the main comparative criterion of the models also in the further part of our work. 3.2.4.4 Improving white-boxes This is a key part of our work. In the iterative cycle we use different methods to improve the quality of the white-box models. After applying each of these methods, we check whether it has improved our score and possibly analyze the model, using statistical methods (residuals analysis) and explanatory machine learning (DALEX package (Biecek 2018)), to draw indications of what should be done next. We are trying the following methods: Tuning hyperparameters of models – Default hyperparameters for models are generally good, but in specific cases using specific hyperparameters may yield in better results, so we use model-based optimization for tuning those parameters (Bischl, Richter, et al. 2017). Reducing outliers – For each variable a two-value vector that indicates the thresholds for which the values are considered as outliers is generated. Then all outliers are changed to the nearest value of obtained earlier vector. Logarithmic and exponential transformations of individual variables – So that linear relationships can be better captured and to reduce the influence of outliers, we transform variables using exponential and polynomial functions. Discretization of continuous features – Some variables do not have a linear effect on the response variable, even if they are transformed by simple functions like exponential function, sometimes there are clear thresholds – so we can replace the variable with indexes of individual segments. The SAFE algorithm helps with this (Gosiewska et al. 2019). Generating new columns as functions of other columns – There may be interactions between variables that cannot be captured by linear models. In order to take them into account, we generate new columns, applying to the rest of them various transformations – we take their inverses, products, quotients, elevations to power, and so on. As a result of these operations, a lot of new measures, potentially simillar to those proposed by McCabe and Halstead, are created, which we later evaluate. We also analyze their interpretability, i.e. to what extent they are translatable into an intuitive understanding of such a measure. At this point we also consider Halstead and McCabe’s measurements. A model with thousands of variables is not well interpretable, so we need to select meaningful measures. We do this by training rpart and ranger models with these additional features and we use DALEXto select the significant ones. We do it twice so at the and we had around 10 of the most important features. Oversampling – On the basis of the data set, we generate more observations from the minority class using the SMOTE algorithm (Hu and Li 2013) so that the model more emphasizes the differences in characteristics of individual classes. In order to avoid model overfitting, we generate data within each fold separately and during crossvalidation we use 4 folds with synthetic data as a train set and the 5th fold without synthetic data as a test set. Our goal is to beat black-box model. In our case we chose random forest model from package ranger. As a white-box model we used logistic regression. Results were tested on dataset with different transformations. 3.2.4.5 Selecting the best model At the end of the process, we select the model that has the highest AUC score for crossvalidation on our dataset. 3.2.5 Results Our base black-box model result is \\(0.792\\). The results of individual models after applying transformations are shown in the Table 1. Order Applied operation logreg kknn rpart Kept? - Base \\(0.735\\) \\(0.728\\) \\(0.500\\) - 0 rpart tuning \\(0.735\\) \\(0.728\\) \\(0.737\\) yes 1a Normalization \\(0.735\\) \\(0.727\\) \\(0.737\\) no 1b Outlier reduction \\(0.743\\) \\(0.732\\) \\(0.739\\) no 1c Logarithm \\(0.744\\) \\(0.718\\) \\(0.725\\) no 2a Outlier reduction and normalization \\(0.743\\) \\(0.732\\) \\(0.739\\) yes 2b Logarithm and outlier reduction \\(0.744\\) \\(0.717\\) \\(0.725\\) no 3a Gain-ratio discretization \\(0.743\\) \\(0.732\\) \\(0.739\\) no 3b rSAFE \\(0.744\\) \\(0.718\\) \\(0.734\\) no 4a New features selected by ranger \\(0.747\\) \\(0.729\\) \\(0.733\\) no 4b New features selected by rpart \\(0.745\\) \\(0.731\\) \\(0.739\\) no 4c Halstead’s measures \\(0.745\\) \\(0.731\\) \\(0.738\\) no 5a SMOTE with new features by ranger \\(0.749\\) \\(0.737\\) \\(0.800\\) no 5b SMOTE with new features by rpart \\(0.747\\) \\(0.736\\) \\(0.793\\) no 5c SMOTE without new features \\(0.745\\) \\(0.736\\) \\(0.804\\) yes Table 1: AUC in white-box models. Each row describes one of the operation that we apply in order to obtain improvement and results of individual models. The first column indicates the order in which we used these transformations. The letters “a, b, c” indicate that we used different transformations with “backtracking”, i.e. we applied them parallel to the same model as they are similar. The last column informs if we decided to keep the change or reject it. Since at the beginning rpart had the AUC value of \\(0.5\\), firstly we tuned its hyperparameters and there was significant improvement in result. Then as step 1a-1c we tried 3 basic transformations of data as normalization of all columns, outlier reduction and logarithm of nearly all columns (without one hot encoded source). As we can see the best improvement was achieved by using outliers reduction so in step 2a we tried to add normalization to that and in step 2b we tried to reduce outliers after applying logarithm. Only outliers reduction and normalization were good enough to be used in the following experiments. We were considering also keeping the logarithmic transformation as it improves logreg result, but we rejected it because it results in drop in the results of other models. Next we used discretization of some columns. Unfortunately, any of the gain-ratio method from funModeling (3a) and rSAFE (3b) did not make statistically important better result. Using partial dependency plot on the black-box model do not show that there is variable which can be discretized. Figure 1: A partial dependence plot depicts effect that one feature has on the prediction outcome. On X axis there is a value of certain feature and on Y axis there is a probability of defect prediction for this value. Then we tried to create new measures based on basic Halstead measures. We selected best measures by DALEX variable importance on one of two learners: ranger or rpart. Then on we test the models after adding the best 10 measures to the dataset, but the results were not promising except from the logistic regression in 4a. Use of the SMOTE algorithm gave a huge improvement especially in decision tree and the best result is in dataset with only outliers reduction and normalization without any new features. 3.2.6 Summary and conclusions Figure 2: The graph shows the improvement of the AUC compared to the difference between the original black-box model and the white-box model. \\(0\\) represents the result of the rpart model, \\(100\\) represents the result of the ranger model. Although we achieved the main goal and created the interpretable model that is better than the black-box the way to achieve that is not satisfying. The most improvement was gained from just hyperparameters tuning of decision tree and oversampling minority class, but the addition of new synthetical data to the minority class is not a questionable method of improving the model. The thing we really trusted that can make improvement was Halstead and McCabe’s measures but any of them do not give any improvement in the model. So we can conclude that the hypotheses of both of them are wrong. Probably the features proposed in original datasets are not sufficient in this problem because there is hardly any improvement with the manipulation of these features. The most informative measure is just a number of code lines. Another possibility is that the code scrapers to create data had some mistakes and that concludes the low quality of data. We visualize the best model based on SMOTE. Since we use the decision tree and in the last model do not create any new measures so it is an interpretable model. It is obvious how it works and why it predicts results because we can show the decision points. Unfortunately, this model is not very trivial. Its depth is usually around 6 nodes and in total it has around 50 nodes. That is why we do not include it in the article - it will not be readable. The created plot, as well as the code, can be found on the GitHub repository (Bogucki, Makowski, and Rafacz 2020). References "],
["using-interpretable-machine-learning-models-in-the-higgs-boson-detection-.html", "3.3 Using interpretable Machine Learning models in the Higgs boson detection.", " 3.3 Using interpretable Machine Learning models in the Higgs boson detection. Authors: Mateusz Bakala, Michal Pastuszka, Karol Pysiak (Warsaw University of Technology) 3.3.1 Abstract This article focuses on comparing efficiency of explainable and black-box AI models in detection of the Higgs boson. Furthermore, it discusses possible modification to data, aiming to improve the performance of explainable models including both manual and automated methots, the latter represented by the rSAFE package. It concludes that some improvement is possible, but limited without obscuring variables used to explain model, thus reducing explainability. It also leads to a remark that logistic regression is the most sensitive model to variable manipulation, allowing achieving scores comparable with black-box models with some data engineering. 3.3.2 Introduction and Motivation ‘We fear that, which we do not understand’. This principle has often been used to portray artificial intelligence in popular culture. The notion that it is something beyond human comprehension became so common, that it started to affect the way we perceive computer algorithms. Because of that, it comes as no surprise that people become wary, hearing that the decisions which affect them directly were made by an AI. Machine Learning models, regardless of the algorithm used, are often treated as a black box that takes some data as an input and returns a prediction based on said data. This approach is most often seen from people who are unfamiliar with the methods used. In many fields, where the only concern is achieving results that are as accurate as possible, this is not a concern. There are however situations, where the risk of unpredictable or unjustified results is unacceptable. This includes uses in medical and judicial systems, where an incorrect decision may have severe consequences. Artificial intelligence could find plenty of uses in those areas, but it would require creating models, which make decisions based on a transparent set of rules. Those include algorithms such as decision trees and linear regression. That raises another problem. Such models often underperform in comparison to more advanced algorithms, notably artificial neural networks and ensemble based solutions, which are more capable at detecting nonlinearities and interactions in the data. Being able to achieve results comparable with said models, while retaining explainability would allow to increase the role of machine learning in fields where transparency is required. Furthermore, it would provide tools suitable for scientific analysis of relations between complex phenomena. Because of that, instead of using one of the popular medical datasets, we decided to test the viability of our approach on a set from a different field of science. The methods used will be tested on the Higgs (Baldi, Sadowski, and Whiteson 2014) dataset described later in the article. The dataset comes from the OpenML (Vanschoren et al. 2013) website. The problem we study tackles methods of improving the predictions of simple models while retaining their transparency. In the rest of the paper we will describe methods of transforming the data to increase the quality of predictions. Most notably we will focus on a tool named rSAFE (Gosiewska et al. 2019), which allows us to extract nonlinearities in the data, based on predictions of a so called ‘surrogate’ model. We will also try to improve predictions based on our knowledge about the data. 3.3.3 Related Work Our work is based on earlier one focused on feasibility of usage of neural networks in this very problem (Baldi, Sadowski, and Whiteson 2014). However, it only concentrates on this question, testing various sizes and architectures of neural networks, but not in the context of interpretability of machine learning models. Thus we built our analysis on what was already achieved. 3.3.4 Methodology 3.3.4.1 Data Modern high energy physics is being done in colliders, of which the best known is Large Hadron Collider near Geneva. Here, basic particles are accelerated to collide at almost speed of light, resulting in creation of other, searched for particles. However, not every collision leads to desired processes and even then target particles may be highly unstable. One of such tasks is producing Higgs bosons by colliding two gluons. If a collision is successful, two gluons fuse into electrically-neutral Higgs boson, which then decays into a W boson and electrically-charged Higgs boson, which in turn decays into another W boson and the light Higgs boson. The last one decays mostly into two bottom quarks. However, if a collisions isn’t successful, two gluons create two top quarks, decaying then into a W boson and a bottom quark each, resulting in the same end-products, but without a Higgs boson in an intermediate state. The first process is called ‘signal’, while the second – ‘background’. Our dataset is a dataset called ‘higgs’, retrieved from OpenML database. It contains about 100 thousand rows of data generated with an event generator using Monte Carlo method and is a subset of a HIGGS dataset from UCI Machine Learning Repository. The purpose of this data is to be used as a basis for approximation of a likelihood function, which in turn is used to extract a subspace of high-dimensional experiment data where null hypothesis can be rejected, effectively leading to a discovery of a new particle. Each row describes event, which must satisfy a set of requirements. Namely, exactly one electron or muon must be detected, as well as no less than four jets, every of these with the momentum transverse to the beam direction of value \\(&gt; 20 GeV\\) and an absolute value of pseudorapidity less than \\(2.5\\). Also, at least two of the jets must have b-tag, meaning than they come from bottom quarks. Each event is described by its classification, 1 if a Higgs boson took part in the process or 0 if not. This column is our target, so it’s a two-class classification problem. There are 21 columns describing various low-level features, amongst which there are 16 describing momentum transverse to the beam direction (jetXpt), pseudorapidity (jetXeta), azimuthal angle (jetXphi) and presence of b-tag (jetXb-tag) for each of the four jets, as well as three first of these for the lepton (that is, electron or muon) and missing energy magnitude and its azimuthal angle. There are also 7 columns describing reconstructed invariant mass of particles appearing as non-final products of the collision event. Names like m_jlv or m_wbb mean that the mass reconstructed is of a particle which products consist of jet, lepton and neutrino or a W boson and two bottom quarks respectively. In this case, these particles are namely top quark and electrically-charged Higgs boson. These columns are so-called top-level features. 3.3.4.2 Measures The first measure that we will use is a simple accuracy. It can be misleading, especially when the target variable is unbalanced, but it is very intuitive and in our dataset the target variable is well-balanced. Second measure is the AUC score, which stands for Areas Under Curve of Receiver Operating Characteristic. It requires probabilities as an output of the model. We create the curve by moving the threshold of a minimal probability of a positive prediction from 0 to 1 and calculating the true positive rate to false positive rate ratio. This measure is much less prone to give falsely high scores than accuracy. The worst score in the AUC is \\(0.5\\). We want to get AUC score as close as possible to \\(1\\), however, close to \\(0\\) is not bad, we just must invert the labels and we will get a close to \\(1\\) AUC score. Our last measure is Mean Misclassification Error (mmce). It is defined as number of misclassified cases divided by total number of cases, that is \\(1-accuracy\\). 3.3.4.3 Models The main goal of this research is to find a way of enhancing the performance of interpretable models, so now we will choose algorithms for this task. For the first test algorithm we chose the logistic regression. It is one of the simplest classification methods. Like in the linear regression algorithm we fit a function to the training data, just instead of a linear function we use a logistic function for the logistic regression algorithm. So the interpretability amounts to the understanding of how points are spread throughout the space. The second algorithm that we will use is a decision tree from the package rpart (Therneau and Atkinson 2019b). It’s structure is very intuitive. We start our prediction from the root of a tree. Then we go up to the leaves basing our path on conditions stated in particular nodes. It is similar to how humans make their decisions. If we have to make one big decision we divide it into many smaller decisions which are much easier to make and after answering some yes/no questions we reach the final answer to the question, so to interpret this model we have to understand how these small decisions in every node are made. Black box models usually have very complex structure, which gives them advantage in recognising complex structures of the data. On the other hand, they are very hard to interpret and explain compared to white box models. There is a package in the R language named rSAFE that meets halfway between black box models and white box models. It is a kind of a hybrid that uses black box models for extracting new features from the data and then using them to fit interpretable models. We will use it for enhancing our interpretable model and compare them to pure white box models. To have some perspective of what level of quality of predictions we can reach we will test some black box models. The black box model that we will use is ranger (Wright and Ziegler 2017). This is a fast implementation of random forests. This is a light and tunable model, so we will try to reach the best performance as we can with this algorithm. As a second black box for comparison we used eXtreme Gradient Boosting (Chen and Guestrin 2016). For easily comparable results we will use exactly the same training and testing dataset. However, we do not limit our feature engineering to only one process for all models, because every algorithm can perform better with different features than the rest. What is more, we will focus on augmenting data in a way that will enable us to get as much as we can from simple, interpretable models. 3.3.5 Results Our original scores were as follows: model Accuracy AUC MMCE ranger 0.7220879 0.8004664 0.2779121 rpart 0.6531224 0.6703391 0.3468776 xgboost 0.6781099 0.7387571 0.3218901 logreg 0.6410977 0.6823442 0.3589023 Initial model performance. We could not illustrate scores with all three measures at once, so we decided to use AUC in our barplots. It makes no difference, though, as every measures behaved similarly, that is, whenever model A had better score in metric X than model B, the same was true for metrics Y and Z. While it certainly isn’t an universal behaviour, it was so prevalent in our results that we gladly embraced it. We noticed several patterns in data and modified features accordingly. Firstly, four jets are indistinguishable from each other, or at least we weren’t told they are. Thus our models should work equally well regardless of how we permute the order of jets in our data. However, this means that the coefficients should be equal, and that’s a serious limitation in our search space. Instead, we propose that each data point should be transformed to order jets by their \\(p_T\\), transverse momentum. Another thing we noticed relates to jets’ b-tags. One thing is that they are categorical values with three levels, where \\(0\\) means background process unrelated to bottom quarks and is usually written down as \\(-1\\), highest value means signal process, described as \\(1\\), and that value in between is probably something like ‘unknown’, especially given the fact that it is depicted by \\(0\\). Another important remark is that at most two of jets for given collision have b-tag value other than ‘background process’, sometimes even none of them. With these points in mind, our proposition is to replace four b-tags columns with a set of six columns. Four of them would be logical vectors, determining whether b-tag is present on given jet, denoted as jetXb.tag, where X is between \\(1\\) and \\(4\\). Remaining two would be named b.tagYdet, where Y is either \\(1\\) or \\(2\\) and also logical vector, where TRUE means that b-tag value is either ‘background process’ or ‘signal process’, while FALSE means, well, the opposite and is used to describe \\(0\\), ‘unknown’ value. Note that in this proposition no b-tag can be safely marked by setting FALSE for all jetXb.tag columns. To avoid ambiguity, we propose that b-tags should be ordered by \\(p_T\\) of their respective jets, which would be achieved by default, as jets would be already ordered at this point. model Accuracy AUC MMCE ranger 0.7185489 0.7947530 0.2814511 rpart 0.6422300 0.6516345 0.3577700 xgboost 0.6740201 0.7351031 0.3259799 logreg 0.6379972 0.6760226 0.3620028 Performance of models after proposed transformations. Applied transformations did not produce favourable results. Actually, every model performed slightly worse. Because of that, we decided to apply an automated method of transforming data using the rSafe package. As the ranger model performed significantly better than the others, we used it as a surrogate model to create new transformations. After that, we tested our models using the complete resulting data set, including both original variables and their transformations. The results are presented in the table. model Accuracy AUC MMCE rpart 0.6488898 0.6705438 0.3511102 logreg 0.6805985 0.7481106 0.3194015 Performance of interpretable models after applying rSAFE transformations. Although the scores of rpart dropped, we achieved a significant improvement in the performance of logistic regression. It was still below the level of the ranger algorithm, but it managed to surpass xgboost. Increasing the number of variables lead, hovewer, to a decrease in interpretability. rSAFE includes a function, that selects the best subset of variables from it’s resulting dataset. We applied it to our data and trained the models once again. model Accuracy AUC MMCE rpart 0.6366817 0.6593928 0.3633183 logreg 0.6638212 0.7279496 0.3361788 Performance of interpretable models after applying rSAFE transformations and subsetting variables. As it turns out, removing variables lead to a decrease in scores. Logistic regression still performed better than on original data, but it no longer surpassed any of the blackbox models. After that, we decided to generate new variables as transformations of the existing ones. We tested raising numeric variables to different powers, and applying logarithms. While power transformations did not affect results, we noticed an improvement after adding logarithms in the performance of logistic regression. model Accuracy AUC MMCE ranger 0.7256474 0.8031399 0.2743526 rpart 0.6530306 0.6689441 0.3469694 xgboost 0.6768758 0.7388668 0.3231242 logreg 0.6690531 0.7270340 0.3309469 Performance of models after adding logarithm transformations. Finally, we applied rSafe transformations as before, now on the dataset with new variables. model Accuracy AUC MMCE rpart 0.6332038 0.6384015 0.3667962 logreg 0.6900427 0.7582915 0.3099573 Performance of models after adding logarithm and rSAFE transformations. This way we achieved additional performance increase for logistic regression, surpassing its previous scores. Similarly to before, removing excess variables negatively affected performance of logistic regression. This time it didn’t really affect rpart model, but it performed badly, so it was out of the competition already. model Accuracy AUC MMCE rpart 0.6368142 0.6438115 0.3631858 logreg 0.6812818 0.7479311 0.3187182 Performance of models after adding logarithm and rSAFE transformations and subsetting variables. 3.3.6 Summary and conclusions Our study shows that popular explainable machine learning models are significantly behind their black-box counterparts on prediction accuracy. Unfortunately, the latter are not of much use for physicists. We managed to improve our explainable models by creating new variables, but they were still falling behind the best black-boxes, while their explainability suffered. We suppose that achieving a score comparable to the best black-box models would involve generating a lot more artificial variables. However, it would have an unwelcome side effect of obscuring the data and hampering the explainability of the model even further. Thus, expert domain knowledge should be deemed necessary – either this, or more effective XAI has to be developed. References "],
["can-automated-regression-beat-linear-model.html", "3.4 Can Automated Regression beat linear model?", " 3.4 Can Automated Regression beat linear model? Authors: Bartłomiej Granat, Szymon Maksymiuk, (Warsaw University of Technology) 3.4.1 Abstract Health care is a subject everyone is familiar with, therefore it is clear we want to constantly improve our knowledge about appliances of today’s science in medicine. However there is a strong belief that models built for that purpose should be interpretable one like linear models are. The goal of this paper is to combine knowledge about linear models, with possibilities of black-box to create an interpretable model no one will argue with. We propose a solution called Automated Regression, that will create a linear model based on a given black-box using Explainable Artificial Intelligence methods and standard feature engineering. The whole process is divided by steps and fully automated to allow fast and smooth technique of creating decent linear models. 3.4.2 Introduction and Motivation Health-related problems have been a topic of multiple papers throughout the years and machine learning brought some new methods to modern medicine.(Obermeyer and Emanuel 2016), (“Ascent of Machine Learning in Medicine” 2019). We care so much about our lives that every single algorithm and method eventually gets tested on some medical data. What is unique about health data is that it requires caution to use black-box models with them, as the process that stays behind its decision is unclear. They were attempts to explain such models(Holzinger et al. 2019), but some people do not agree with that approach. Almost always doctors know whether a patient is sick or not. What is important to them is the reason why he is sick. That’s why explainable machine learning is the key to make all of us healthier. Explainable Artificial Intelligence (XAI) term refer to an area of science where various methods and techniques are used to make Artificial Intelligence human-understandable. One of its applications is Interpretable Machine Learning where researchers put their effort to explain decisions made by any type of black-box predictive model clear and present reasoning for it. However XAI methods are not very well defined yet and they lack strong statistical background like tests, etc. That caused strong opposition of Explainable Machine Learning to rise(Rudin 2019). Unfortunately making a good explainable model for health data might be close to impossible. Medical problems of all kinds can be very unique, complex, or completely random. That’s why researchers spend numerous hours on improving their explainable models and that’s why we decided to test our approach on liver disorders dataset with help. We will try to improve the linear model results with the help of the AutoML black-box model. There is a big discussion in the scientific world about “what does it mean for a model to be interpretable?”(Lipton 2016). Unfortunately there is no clear answer to that question and often it is necessary to specify what is supposed to be meant as interpretable. In this paper we assume that the model is interpretable if it is possible to explicitly track down what contributed to model answer and to do it in a finite time. It means that all transformation of variables or even concatenations are possible as soon as a used type of model is simply structured like a linear model is. However, we understand other’s opinions and we will present also results with only simple transformations and without controversial concatenations. The liver-disorders dataset is well known in the field of machine learning(McDermott and Forsyth 2016) and that’s exactly the reason why we chose it. It is described in the next chapter. Our goal was to find a relatively clean dataset with many models already done by other researchers. Another advantage is the availability of the dataset. It is published on the OpenML repository(Vanschoren et al. 2013) and therefore everyone can give a shot to that problem. We don’t want to show that properly cleaned data gives better results but to achieve, an explainable model found after a complex analysis that we want to test. In this paper we do a case study on liver disorders dataset and want to prove that by using automated regression it is possible to build an easy to understand prediction that outperforms black-box models on the real dataset and at the same time achieve similar results to other researchers. By automated regression we understand a process when we search through space of available dataset transformation and try to find the one for which linear regression model is best using earlier defined loss function. 3.4.3 Data The dataset we use to test our hypothesis is a well-known liver-disorders first created by ‘BUPA Medical Research Ltd.’ containing a single male patient as a row. The data consists of 5 features which are the results of blood tests a physician might use to inform diagnosis. There is no ground truth in the data set relating to the presence or absence of a disorder. The target feature is attribute drinks, which are numerical. Some of the researchers tend to split the patients into 2 groups: 0 - patients that drink less than 3 half-pint equivalents of alcoholic beverages per day and 1 - patients that drink more or equal to 3 and focus on a classification problem. All of the features are numerical. The data is available for 345 patients and contains 0 missing values. The dataset consists of 7 attributes: mcv - mean corpuscular volume alkphos - alkaline phosphatase sgpt - alanine aminotransferase sgot - aspartate aminotransferase gammagt - gamma-glutamyl transpeptidase drinks - number of half-pint equivalents of alcoholic beverages drunk per day selector - field created by the BUPA researchers to split the data into train/test sets For further readings on the dataset and misunderstandings related to the selector column incorrectly treated as target refer to: “McDermott &amp; Forsyth 2016, Diagnosing a disorder in a classification benchmark, Pattern Recognition Letters, Volume 73.” 3.4.4 Methodology For the beginning we would like to introduce the notation that we are going to use in this paper. We take AutoMl Model \\(M_{aml}\\) and the dataset \\(D\\) that consists of \\(D_{X} = X\\) which is set of independent variables and \\(D_{y} = y\\) - dependent variable (ie. target). We assume that the AutmoML Model \\(M_{aml}\\) is an unknown function \\(M_{aml}: \\mathbb{R}^{p} \\to \\mathbb{R}\\), where p is a number of features (independent variabes) in the \\(D\\) Dataset. This function that satisfies \\(y_{i} = M_{aml}(X_{i}) + \\epsilon_{i}\\) where \\(\\epsilon\\) is an error vector so it transforms observation into predicted value with additional error value. Proposed technique - Automated Regression - constructs known function \\[G_{AR} : \\mathbb{R}^{n \\times p} \\to \\mathbb{R}^{n \\times p_{1}}\\] where \\(n\\) is a number of observations in original dataset. It purpose is to transform original dataset using known functions. Keep in mind that \\(p_{1}\\) does not have to equal \\(p\\) since we allow conncatenations of variables. Then we fit a linear regression model using those transformated data, therfore we can say that our goal is to solve linear regression equation, so find function \\(G_{AR}\\) and \\(\\beta\\) vector that satisfies \\[y = G_{AR}(X)\\beta + \\epsilon\\]. To find the parameters mentioned before it is necessary to put accurate constraints. First of all we want it to minimize one of the two loss functions \\(L: \\mathbb{R}^{n} \\to \\mathbb{R}\\). First of them is \\[L_{R} : \\frac{\\sqrt{\\sum_{i=1}^{n}(y_{i}-\\hat{y_{i}})^{2}}\\sum_{i=1}^{n}(y_{i}-\\bar{y_{i}})^{2}}{\\sum_{i=1}^{n}(\\hat{y_{i}}-\\bar{y_{i}})^{2}}\\] which can be interpreted as Root Mean Square Error divided by the R-squared coefficient of determination. R-squared stands as a measure of variance explained by the model (Fan, Ong, and Koh 2006) and therefore may be useful to find the best explanation of interactions met in the data. Obviously, a high coefficient does not always mean an excellent model, furthermore even low values of it not always inform us about the uselessness of found fit. Therefore Automated Regression will be performed independently using second measure \\[L_{0} : \\sqrt{\\sum_{i=1}^{n}(y_{i}-\\hat{y_{i}})^{2}}\\] which is Root of the Mean Squared Error (RMSE) and is widely used loss function for regression task. On top of that we also put constraints on the domain of valid transformations of particular variables, which will be divided into four stages conducted one by one. For a given dataset, described in the previous paragraphs we decided to use: Stage 1: Feature selection XAI feature Importance Stage 2: Continuous transformation Polynomial transformation Lograthmic transformation Stage 3: Discrete transformation SAFE method Stage 4: Feature concatenation Multiplication of pair of features. XAI related methods are conducted using AutoML Model. We’ve decided to omit data imputation as an element of valid transformations set because the liver-disorders dataset does not meet with the problem of missing values. The optimization process is conducted based on Bayesian Optimization and the backtracing idea. As was pointed out earlier, each element of the domain of valid transformations is a particular stage in the process of creation \\(G_{AR}\\) function. Within each stage, Bayesian optimization will be used to find the best transformation. During further steps, if any of transformation did not improve model, ie. \\(L\\) function was only growing, the algorithm takes second, the third, etc. solution from previous steps according to backtracking idea. If for no of \\(k\\) such iterations, where k is known parameter, a better solution is found, step is omitted. 3.4.5 Results Dataset was split on the train and test dataset, and that split will be the same for all experiments covered in this section. To begin with we need to introduce the AutoML model that was used as a goal to beat. H2O AutoML(H2O.ai 2017) was used to find the best black-box solution over the provided training dataset. As interpretation R-squared coefficient of determination is not the same for linear models and black-box, we will present a comparison of the found solution to the AutoML model using only \\(L_{0}\\) measure. The table below shows parameters of the back-box model which appears to be Gradient Boosting Machine. Parameters of Gradient Boosting Machine which is a black-box H2O AutoML Model. RMSE calculated using the test dataset. Num. of trees Max depth Min depth Min leaves Max leaves Test RMSE 30 8 6 12 18 2.73 After each step of the procedure shown in 3.4.5 we would like to present the score best model had achieved. In this way we can make an assumption about what action is the most crucial for a given dataset. As was mentioned before, to face the problem of interpretability and what does it mean, there will be four final models presented. For each approach, ie. with and without feature concatenation step, two models are found using both \\(L_{R}\\) and \\(L_{0}\\) measures. Despite multiple approaches, all of them share some common properties. First of all, it turns up that the linear model with all features as active predictors scores 2.68 RMSE and 0.17 R-squared. It means we are already on the level of the black-box model. What is worrisome though, is a small fraction of variance explained by the model. Therefore, the goal of our further research is to beat the black-box even more. Next, XAI permutation feature importance (Fisher, Rudin, and Dominici 2018) selected 3 features that may be insignificant. Due to that fact and relatively small dataset we were able to check all combinations of including them into the model and what’s interesting, different features were chosen depending on whether \\(L_{0}\\) or \\(L_{R}\\) measures were used. About the second step, polynomial transformation, including the Box-Cox transformation of the target value, made the model fit to data way worse. As a result the only transformation that indeed improved both loss function was log-transformation and discrete transformation using the SAFE method(Gosiewska et al. 2019). Exact results after each step for both loss functions. In all columns beside R squared value of Root Mean Square Error over test dataset is shown. Model Baseline Stage 1 Stage 2 Stage 3 Stage 4 Black-Box R squared \\(L_{0}\\) | With concat. 2.680 2.660 2.613 2.594 2.533 2.727 0.193 \\(L_{R}\\) | With concat. 2.680 2.660 2.613 2.670 2.734 2.727 0.231 \\(L_{0}\\) | Without concat. 2.680 2.660 2.613 2.594 - 2.727 0.171 \\(L_{R}\\) | Without concat. 2.680 2.660 2.613 2.670 - 2.727 0.193 Figure 3.1 shows how each step contributed to the final output. For the model described by the red line which is supposed to reach the lowest possible value of RMSE we can spot that the most significant is the last step, which is the concatenation of features. It lowers RMSE in comparison to the previous step by 0.06 which is the highest witnessed growth. The model found a very sophisticated variable which is the multiplication of discretized alkphos and sgot. One the contrary final RMSE score of model shown using a blue line, which used concatenations of features with a \\(L_{R}\\) loss function was slightly higher than AutoML one and way higher than the result of the most model. The inspection of each step shows that although the first two steps were very similar, after the third and fourth R squared coefficient inflated with the cost of higher RMSE. It means that the model explains more variance of the plain, but returns worse predictions on the average. FIGURE 3.1: RMSE of models according to the phase of training. The orange line indicates Root Mean Square Error of the black-box model. The dashed vertical line shows a cutoff point for models without concatenation of features. The output of \\(G_{AR}\\) function ie. the variables that are presnt in our model: \\(L_{R}\\) loss function \\(\\text{SAFE(log(sgot))}\\) \\(\\text{SAFE(log(gammagt))}\\) \\(\\text{SAFE(sgpt)}\\) \\(\\text{SAFE(alkphos)}\\) \\(\\text{mcv}\\) \\(\\text{(SAFE(alkphos)}:\\text{mcv)}\\) \\(L_{0}\\) loss function \\(\\text{SAFE(sgpt)}\\) \\(\\text{log(sgot)}\\) \\(\\text{alkphos}\\) \\(\\text{log(gammagt)}\\) \\(\\text{mcv}\\) \\(\\text{(log(gammagt)}:\\text{mcv)}\\) 3.4.6 Summary and conclusions To summarize all, feature selection and feature transformation steps can be conducted with maintaining a balance between better RMSE and R squared. Moreover, the results are very similar no matter what loss function was used. Things are getting a bit more complicated when we start to discretize variables or, optionally, to merge them. In that cause better coefficient of determination is not being followed up by better Room Mean Square Error. However, we consider proposed \\(L_{R}\\) loss functions as promising, and are looking forward to improving its behavior in the future making the border between its elements smoother. Automated Regression turned up to be successful for the given task. We were able to significantly improve the model and beat black-box and are more than happy looking at the result. One of the advantages of the proposed method is flexibility. Putting harsher constraints on a set of allowed transformations does not cause any problems. It was shown when we had to stop the training stage after three out of four steps in order to obtain more interpretable results. The effectiveness of the proposed method is the highest for small and medium-sized datasets, but we look forward to a large datasets extension. References "],
["interpretable-non-linear-feature-engineering-techniques-for-linear-regression-models-exploration-on-concrete-compressive-strength-dataset-with-a-new-feature-importance-metric-.html", "3.5 Interpretable, non-linear feature engineering techniques for linear regression models - exploration on concrete compressive strength dataset with a new feature importance metric.", " 3.5 Interpretable, non-linear feature engineering techniques for linear regression models - exploration on concrete compressive strength dataset with a new feature importance metric. Authors: Łukasz Brzozowski, Wojciech Kretowicz, Kacper Siemaszko (Warsaw University of Technology) 3.5.1 Abstract In this article we present and compare a number of interpretable, non-linear feature engineering techniques used to improve linear regression models performance on Cocrete compressive strength dataset. To assert their interpretability, we introduce a new metric for measuring feature importance, which uses derivatives of feature transformations to trace back original features’ impact. As a result, we obtain a thorough comparison of transformation techniques on two black-box models - Random Forest and Support Vector Machine - and three glass-box models - Decision Tree, Elastic Net, and linear regression - with the focus on the linear regression. 3.5.2 Introduction and Related Works Linear regression is one of the simplest and easiest to interpret of the predictive models. While it has already been thorougly analysed over the years, there remain some unsolved questions. One such question is how to transform the data features in order to maximize the model’s effectiveness in predicting the new data. An example of a known and widely used approach is the Box-Cox transformation of the target variable, which allows one to improve the model’s performance with minimal increase in computational complexity (Sakia 1992). However, the choice of the predictive features’ transformations is often left to intuition and trial-and-error approach. In the article, we wish to compare various methods of features’ transformations and compare the resulting models’ performances while also their differences in feature importance. Many black box regression models use various kinds of feature engineering during the training process. Unfortunately, even though the models perform better than the interpretable ones, they do not provide information about the transformations used and non-linear dependencies between variables and the target. The goal we want to achieve is extracting features and non-linearities with understandable transformations of the training dataset. To measure the improvement of used methods we will compare their performance metrics with black box models’ as a ground truth. This will allow us to effectively measure which method brought the simple linear model closer to the black box. Moreover, we will take under consideration the improvement of black box model performance. Thanks to this, our article will not only present the methods for creating highly performant interpretable models, but also improvement of the results of black box model. A similar research may be found in the following articles: (Patricia Arroba 2015) - the authors present automatic transformations of features with a genetic approach. It provides a way to feature engineering without researcher’s expertise. (Melo] and Banzhaf 2018) - similarly to the previous article, this paper provides a way to automate feature engineering using evolutionary computation to make a hybrid model - the final model is simple linear regression, while its features are found by a more complex algorithm. 3.5.3 Methodology The main goal of our research is to compare various methods of transforming the data in order to improve the linear regression’s performance. While we do not aim to derive an explicitly best solution to the problem, we wish to compare some known approaches and propose new ones, simultaneously verifying legitimacy of their usage. The second goal of the research is to compare the achieved models’ performances with black box models to generally compare their effectiveness. An important observation about the linear regression model is that once we transform features in order to improve the model’s performance, it strongly affects its interpretability. We therefore propose a new feature importance measure for linear regression and compare it with the usual methodes where possible. 3.5.3.1 Transformation methods The four methods of feature transformation compared in the article include: By-hand-transformations - through a trial-and-error approach we derive feature transformations that allow the linear regression models to yield better results. We use our expertise and experience with previous datasets to get possibly best transformations which are available though such process. They include taking the features to up to third power, taking logarithm, sinus, cosinus or square root of the features. Brute Force method - this method of data transformation generates huge amount of additional features being transformations of the existing features. We allow taking each feature up to the third power, taking sinus and cosinus of each feature and additionaly a product of each pair of features. The resulting dataset has 68 variables including the target (in comparison with 9 variables in the beginning). Bayesian Optimization method (Bernd Bischl 2018) - we treat the task of finding optimal data transformation as an optimization problem. We restrict the number of transformations and their type - we create one additional feature for each feature present in the dataset being its \\(x\\)-th power, where \\(x\\) is calculated through the process of Bayesian optimization. It allows us to keep the dimension of the dataset relatively small while improving the model’s performance. We allowed the exponents to be chosen from interval \\((1, 4]\\). We will refer to these transformations as bayesian transformations. One of our ideas is to use Genetic Programming (GP) to find the best feature transformations. Our goal is to find a set of simple feature transformations (not necessarily unary) that will significantly boost the ordinary linear regression model performance. We will create a set of simple operations such as adding, multiplying, taking a second power, taking logarithm and so on. Each transformation is based only on these operations and specified input variables. Each genetic program tries to minimize MSE of predicting the target, thus trying to save as much information as possible in a single output variable constructed on a subset of all the input features. We will use a variation of the genetic algorithms to create an operation tree minimizing our goal. Before we fit the final model, that is the ordinary linear regression, first we select the variables to transform. The resulting linear regression is calculated on the transformed and selected variables at the very end. To summarize, each operation tree calculates one variable, but this variable may not be included at the end. To avoid strong correlation we decided to use similar trick to the one used in random forest models. Each GP is trained only on some subset of all variables. Bootstraping seems also to be an effective idea, however, we did not implement it. In the case of small number of features in the data, such as in our case, one can consider using all possible subsets of variables of fixed size. Otherwise, they can be chosen on random. This process results in a number of output variables, so we choose the final transformed features using LASSO regression or BIC. This method should find much better solutions without extending dimensionality too much nor generating too complex transformations. The general idea is to automate feature enginnering done traditionally by hand. Another advantage is control of the model’s complexity. We can stimulate how the operation trees are made, e.g. how the operations set looks like. There is also a possibilty of reducing or increasing complexity at will. Modification of this idea is to add a regularization term decreasing survival probability with increasing complexity. At the end, the model could also make a feature selection in the same way - then one of possible operations in the set would be dropping. We will refer to these transformations as genetic transformations. 3.5.3.2 Feature importance metric Feauture importance is a metric indicating which predictors (columns) are the most important to the model’s final prediction. The metric becomes more and more significant, as machine learning algorithms play more vital role in almost any decision making process from banking to medical areas. A more precise description of Feature Importance may be found in (Biecek 2018). The most natural feature importance metric for linear models, such as linear regression and GLM, are the absolute values of coefficients. Let \\(x_1, \\dots, x_n\\) denote the features (column vectors) and \\(\\hat{y}\\) denote the prediction. In linear models we have: \\[c + \\sum_{i=1}^n a_i \\cdot x_i = \\hat{y}\\] where \\(c\\) is a constant (intercept) and \\(a_i\\) are the coefficients of regression. Formally, the Feature Importance mesaure value of the \\(i-\\)th feature (\\(FI_i\\)) measure is given as: \\[FI_i = |a_i|\\] We may also notice that: \\[FI_i = \\Big|\\frac{\\partial \\hat{y_i}}{\\partial x_i}\\Big|\\] We wish to generalize the above equation and, in doing so, create a new Feature Importance mesaure applicable to transformed data which will be be able to provide Feature Importance of features before any transformations. Transforming the data in the dataset to improve the model’s performance may be expressed as generating a new dataset, where each column is a function of all the features present in the original dataset. If the new dataset has \\(m\\) features and the new prediction vector is given as \\(\\dot{y}\\), we then have: \\[d + \\sum_{i=1}^m b_i \\cdot f_i (x_1, \\dots, x_n) = \\dot{y}\\] where \\(d\\) is the new intercept constant and \\(b_i\\) are the coefficients of regression. We therefore may use the formula above to derive a new Feature Importance Measure (Derivative Feature Importance, \\(DFI\\)) as: \\[DFI_{i, j} = \\Big|\\frac{\\partial \\dot{y_i}}{\\partial x_{i, j}}\\Big| = \\Big|b_i \\cdot \\sum_{k=1}^m \\frac{\\partial f_k}{\\partial x_{i, j}}\\Big|\\] The above measure is calculated separately for each observation \\(j\\) in the dataset - we will refer to it as local \\(DFI\\). However, due to the additive properties of derivatives, we may calculated Derivative Feature Importance of the \\(i\\)-th feature as: \\[DFI_i = \\frac{1}{n}\\sum_{j=1}^n DFI_{i, j}\\] which is then a global measure of Feature Importance of the \\(i\\)-th feature (a global \\(DFI\\)). A question arises why such measure is necessary. Let us assume that we have columns \\(x\\) and \\(y\\) in our dataset. After the data transformation we may end up with a dataset containing columns such as \\(\\sin{x}\\), \\(x^2\\) or even \\(\\sqrt{x + y}\\). Naturally, the new linear model’s coefficients standing by the new features are unable to provide any information about the input variable’s importance, as we cannot deduce the \\(x\\)’s importance knowing the importance of \\(\\sqrt{x+y}\\). However, the \\(DFI\\) measure allows us to reverse the transformations and calculate the \\(x\\)’s and \\(y\\)’s Feature Importance if we know exactly what transformations were used, and that knowledge comes naturally from the sole process of feature engineering. We may therefore calculate a local \\(DFI\\) for a chosen observation - which will tell us which input features were the most important in the prediction of the given sample - or we may take an average \\(DFI\\) of all observations - the global \\(DFI\\) - to calculate the overall importance of each feature. 3.5.3.3 Dataset and model performance evaluation The research is conducted on Concrete_Data dataset from the OpenML database [link]. The data describes the amount of ingredients in the samples - cement, blast furnace slag, fly ash, water, coarse aggregate and fine aggregate - in kilograms per cubic meter; it also contains the drying time of the samples in days, referred to as age. The target variable of the dataset is the compressive strength of each sample in megapascals (MPa), therefore rendering the task to be regressive. The dataset contains 1030 instances with no missing values. There are also no symbolic features, as we aim to investigate continuous transformations of the data. Due to the fact, that we focus on the linear regression model, the data is reduced prior to training. We remove the outliers and influential observarions based on Cook’s distances and standardized residuals. We use standard and verified methods to compare results of the models. As the target variable is continuous, we may calculate Mean Square Error (MSE), Mean Absolute Error (MAE), and R-squared measures for each model, which provide us with proper and measurable way to compare the models’ performances. The same measures may be applied to black box models. The most natural measure of feature importance for linear regression are the coefficients’ absolute values after training the model - however, such easily interpretable measures are not available for black-box models. We therefore measure their feature importance with permutational feature importance measure and caluclating drop-out loss, easily applicable to any predictive model and therefore not constraining us to choose from a restricted set. In order to provide unbiased results, we calculate the measures’ values during cross-validation process for each model, using various number of fold to present comparative results. 3.5.4 Results 3.5.4.1 Transformed dataset First we wish to present explicitly the datasets acquired with the transformations and the plain data as a reference: The plain Concrete_data dataset containes features \\(\\text{Cement}\\), \\(\\text{Slag}\\), \\(\\text{Ash}\\), \\(\\text{Water}\\), \\(\\text{Superplasticizer}\\), \\(\\text{CoarseAggregate}\\), \\(\\text{FineAggregate}\\), \\(\\text{Age}\\) and the target compressive strength shortened to \\(\\text{CS}\\). The dataset after by-hand transformations contains the columns: \\(\\log(\\text{Age})\\) \\(\\sin{\\Big(2\\pi\\frac{\\text{Water}}{\\max{\\text{Water}}}\\Big)}\\) \\(\\sin^2{\\Big(2\\pi\\frac{\\text{Water}}{\\max{\\text{Water}}}\\Big)}\\) \\(\\text{FineAggregate}^2\\) \\(\\text{Superplasticizer}^2\\) \\(\\frac{\\text{Ash}}{\\text{Cement}}\\) \\(\\text{FineAggregate}^2 \\cdot \\text{Cement}\\) features \\(\\text{Cement}\\), \\(\\text{Slag}\\), \\(\\text{Ash}\\), \\(\\text{Water}\\), \\(\\text{CoarseAggregate}\\) from the plain dataset. The dataset after brute force transformations contains the columns: all the features from the plain dataset, Second power of each feature from the plain dataset, Third power of each feature from the plain dataset, Sinus of each feature from the plain dataset, Cosinus of each feature from the plain dataset, Multiplication of each pair of distinct features from the plain dataset. The dataset after bayesian transformations contains the columns: all the features from the plain dataset, \\(\\text{Cement}^{1.166}\\) \\(\\text{Slag}^{3.120}\\) \\(\\text{Ash}^{3.161}\\) \\(\\text{Water}^{3.657}\\) \\(\\text{Superplasticizer}^{1.110}\\) \\(\\text{CoarseAggregate}^{2.182}\\) \\(\\text{FineAggregate}^{1.789}\\) \\(\\text{Age}^{1.100}\\) The dataset after genetic transformations contains the columns: \\(\\sqrt{2\\cdot \\text{Cement}}+\\text{Superplasticizer}\\) \\(\\sqrt{\\text{Superplasticizer}\\cdot\\text{Age}+\\text{CoarseAggregate}+\\text{Age}}\\) \\(\\sqrt{2\\cdot \\text{Age}} + \\sqrt{\\text{Age} + \\text{FineAggregate}}\\) \\(\\sqrt{\\log(\\text{Cement}+\\text{Slag})}\\) \\(\\sqrt{\\text{Cement}+\\text{Superplasticizer}}+\\sqrt{\\text{Cement}+\\text{Slag}}\\) \\(\\sqrt{\\text{FineAggregate}} + \\sqrt{\\text{Superplasticizer}\\cdot \\text{Age}}\\) \\(\\sqrt{\\text{Cement} + \\text{Slag} + \\text{CoarseAggregate}}\\) \\(\\text{Superplasticizer}+\\sqrt{\\text{CoarseAggregate}-\\text{Ash}}\\) Note, that the transformations used in results section are only examples of the presented methods and various realisations may result in various final measures’ values. Considerably, the trial-and-error approach is quite individual and the results may differ depending on the experimentator. This non-automatic method should be treated as something that can be achieved after numerous trials. 3.5.4.2 Models’ performance FIGURE 3.2: MAE of the DT, LR, GLM, optimized GLM, SVM and RF models on the plain Concrete_Data dataset and after transformations. The lower the value, the better the model’s performance. FIGURE 3.3: MSE of the DT, LR, GLM, optimized GLM, SVM and RF models on the plain Concrete_Data dataset and after transformations. The lower the value, the better the model’s performance. The plots presented above show the values of MAE and MSE achieved by each model on the datasets after the mentioned transformations. We may observe that the linear models have significantly reduced their error values after the transformations, while the brute force method yielded best results. However, brute force method generates much more features increasing the resulting dimension of the dataset, thus increasing the time complexity and reducing the interpretability. The three remaining methods, that is: the Bayesian optimization, the trial-and-error method and the genetic modifications - provided much improvement in comparison with the models’ performance on the plain dataset as well. In this case the final space has much lower dimension. Bayesian optimization results in 17 features, the trial-and-error in 13 features and the genetic modifications result in 9 features, in comparison to 68 features after brute force transformations (all numbers include the target variable). All of these methods have very similar quality of predictions, considering both MAE and MSE. The remaining non-linear white-box model, namely the Decision Tree Regressor, seems to be rather unaffected by any transformations of the dataset. In comparison, both black the boxes: Random Forest and SVM with gaussian kernel, are strongly influenced, though it is hard to say when black box’s prediction quality increases and when decreases. 3.5.4.3 Feature Importance comparison We present the comparison of Feature Imprortance values between permutational Feature Importance calculated on the black-box models - Random Forest and SVM and the local and global Derivative Feature Importance calculated on the datasets after brute force transformations and after the Bayesian optimization. FIGURE 3.4: Feature Importance of RFR and SVM on the untransformed Concrete_Data dataset. The longer the bar, the more important the feature is. FIGURE 3.5: Local (obs. 573) and global DFI of optimized GLM model on the Concrete_Data dataset after brute force transformations. The longer the bar, the more important the feature is. FIGURE 3.6: Local (obs. 458) and global DFI of optimized GLM model on the Concrete_Data dataset after Bayesian transformations. The longer the bar, the more important the feature is. The figure 3.3 present permutational Feature Importance of the black-box models, and the figures 3.4 and 3.5 present \\(DFI\\) values - for one observation and calculated globally, respectively. The local \\(DFI\\) were calculated on the observations nr \\(573\\) and \\(458\\). We may notice high resemblance of the black-box Feature Importance to the local \\(DFI\\) after Brute Force trtansformations, as well as similarities between all the Feature Importance calculations overall. The order of features in \\(DFI\\) is not precisely equal to the order of feature after permutational Feature Importance of the black-boxes; however, the order still makes sense as far as we can say based on our little knowledge of materials. Moreover, the deviations of Feature Importance measures between various models is quite common in such research. To summarize, although the deviations between black-box and glass-box models’ Feature Importance are present, we conclude that \\(DFI\\) may provide a new way of calculating Feature Importance for linear models. Its efficiency shall be further investigated in another research. When it comes the presented comparison, the majority of important variables were detected by the \\(DFI\\) method. 3.5.5 Summary and conclusions Each of the four methods leads to significant improvement in Linear Regression. All of them are based on completely different ideas and their effectivenes may vary for different tasks. It needs noting that the presented research is conducted on a dataset containing only numerical variables, so similar research for transformations of categorical variables remains to be yet conducted. However, we have presented numerous ways to transform the dataset improving the linear models while at least partially maintaining its interpretability. The black box models in this case were unsurpassed, though we achieved highly comparable results. The greatest advatange of white box usage is that every person can understand their mechanics and predictions. Therefore, the presented methods may serve as an efficient solution, when one needs to retain simplicity while also offering an improvement to predictions. Our new metric - \\(DFI\\) - can be used to compare simple (differentiable) feature transformations in linear regression. The analytical deduction suggests, that its performance is accurate, but it shall be investigated further before being applied in general. In the end, the obtained results are satisfying and should encourage to put more effort into research about new transformation methods and interpretability metrics. The next thing to do is putting more effort into researching the new \\(DFI\\) metric, to improve interpretability of the extended regression models. We hope that our article will inspire a number of interested readers to conduct such research. References "],
["surpassing-black-box-models-performance-on-unbalanced-data-with-an-interpretable-one-using-advanced-feature-engineering.html", "3.6 Surpassing black box model’s performance on unbalanced data with an interpretable one using advanced feature engineering", " 3.6 Surpassing black box model’s performance on unbalanced data with an interpretable one using advanced feature engineering Authors: Witold Merkel, Adam Rydelek, Michał Stawikowski (Warsaw University of Technology) 3.6.1 Abstract Explainability is the most talked about topic of modern predictive models. The article touches on the topic of such models and their benefits. The main focus is to prove that even on complicated data, explainable models can achieve comparable performance to the best black-box models. Not only are there described strategies allowing better results but also greater explainability. The dataset used in experiments is the adult dataset from OpenML which is from Census database. During the experiments there are multiple processing techniques used, SAFE and different imputation methods among others. Every tool used is explained and the results gained from each part are shown and explained. Thanks to the fact that adult dataset is vastly unbalanced there is a perfect opportunity to present techniques which can be used to handle such tasks. All those methods combined allow for a presentation of a clear workflow enhancing explainable models performance with emphasis on decision tree models. The best results we achieved with decision tree model using methods mentioned above. However at first the best score was achieved by logistic regression, which from the start beat the black boxes. On the other hand it was not possible to tune it, to get it any better. For this reason, our final model is a decision tree, that despite starting as one of the worst surpasses all of the other boxes white and black. This shows that everything can be accomplished with adequate feature engineering, while keeping them explainable. 3.6.2 Introduction and Motivation Recently, an increase in demand of interpretable models can be seen. Machine learning models have gained in popularity in recent years among many fields of business science, industry and also more and more often in medicine. Interpretability is a quickly growing part of machine learning, and there have been many works examining all of the aspects of interpretations. (Murdoch 2018) The problem, however, turned out to be blackbox models, which did not provide sufficient information about the motivation in making specific decisions by the models. Some of machine learning models are considered as black boxes. This means that we can get accurate predictions from them, but we give up the ability of clearly explaining or identifying the logic behind these decisions. (Pandey 2019) Interpretability of models is a desirable feature among specialists in fields other than machine learning, it helps them make better decisions, justify their choices, and combine expert knowledge with the model’s indications. Many works concerning Explainable Artificial Intelligence have arose during the last few years as the topic got more and more popular. [Explainable Artificial Intelligence (XAI): Concepts, Taxonomies, Opportunities and Challenges toward Responsible AI] (Alejandro Barredo Arrieta 2019) is a paper about XAI in general and many challenges concerning the topic. The article addresses all kinds of easily explainable models which set our focus on enhancing kNN and decision tree based models. Humans and computers work differently in how they sense, understand and learn. Machines deals with large volume of data and finding hidden patterns in it and people are better at seeing the bigger picture and finding high-level patterns. (accenture 2018) Trust and transparency are also demanded. There are many methods that can help us create an interpretable model. One of the ways to achieve interpretability is to use only a certain subset of algorithms that create interpretable models. Some of the algorithms considered to be interpretable are: linear regression, logistic regression, decision trees or K Nearest Neighbours (KNN). (Molnar 2019a) Another way may be to use blackboxes to create an interpretable model. They can help us during transformation of the original data set or, for example, in selecting variables. [SAFE ML: Surrogate Assisted Feature Extraction For Model Learing] (Gosiewska et al. 2019) focuses on using Black Box models as surrogate models for improving explainable models with rSAFE which is going to be an important part of the workflow. In this article, we will discuss the process of creating an interpretable model whose target effectiveness will be comparable to blackbox models. We will present the whole workflow, during which we will get acquainted with the dataset with which we will work, we will use advanced feature engineering methods and compare the results obtained during all phases of process. An additional problem we will face during work will be unbalanced data and creating a model that will take them into account during prediction. We will use machine learning tools and frameworks available in R and Python. 3.6.3 Data The dataset used is the adult dataset from OpenML. The original data comes from UCI and was extracted by Barry Becker from the 1994 Census database. The task is to predict whether a given adult makes more than $50,000 a year based attributes such as: Name | M Median/Mode age | 3 37 weekly work hours | 4 40 capital gain | 0 0 capital loss | 0 0 fnlwgt | 1 178145 race | w white sex | m male native country | U USA work class | p private relationship | h husband martial status | m married occupation | o other education | h high school graduate Table 3.6.1 - data introduction For numerical variables median is presented and for categorical it is mode. All variables expect fnlwgt, capital-gain and capital-loss are self explanatory. Fnlwgt is an approximation for the demographic background of the people which assigns similar weights to people with similar demographic characteristic. Capital gain (loss) is a rise (fall) in the value of a capital asset that gives it a higher (lower) worth than the purchase price. It concerns only business owners, which is why the median is zero. In the data we can observe a problem with target class distribution which is vastly unbalanced. The ratio of positive and negative values is around one to four. The dataset has overall of more than forty eight thousand observations and fifteen features, some of which are scarce. By observing the distribution of variables in the dataset, there is more information to be gained about the data structure. Figure 3.6.1 - distribution of numeric variables from dataset Some of the variables presented on figure 3.6.1 are normally distributed but there are also highly skewed variables such as capital-gain and capital-loss. From the age variable we can see that most of the data describes young people, which is consistent with the distribution of working people in most western countries. From further analysis we discovered that aged people tend to earn more than youth, so age is important for classification. Capital-gain and capital-loss are centered at zero and show little variation. For this reason they will be of lesser relevance for the model. From further analysis we discovered that education-num is highly correlated with target, higher educated individuals usually earn more than $50000. From observation density exploration it can be see that most of the people in the dataset are young with secondary education. Fnlwgt plot is right skewed, which will be dealt with during initial data preparation. From hours-per-week it can be observed that the most popular working time is 40 hours per week. From further analysis of the dataset it can be observed that only self-employed work class has a higher percentage of positive target value (earning more than $50000) than negative. Figure 3.6.2 - missing data pattern. The bottom plot shows relation between missing variables, when a certain variables is absent in correspondence to other. The upper part of the figure depicts the missing data percentage. Columns of the bottom part of figure 3.6.2 presents a pattern in which the missing data occurs. Each column stands for one pattern, which describes records containing missing data in red color and complete ones in blue. For example the most popular ones are those in which there is no missing data. The second most popular pattern is one with missing values in columns occupation and work class. Missing data percentage is low, but imputation should improve the results a little bit. Numeric variable are generally weakly correlated with each other, the highest dependance can be observed between hours-per-week and education. People with education greater than secondary tend to work more than 40 hours per week. Capital-gain is also correlated with education - the higher the education the bigger the capital-gain. The rest of numeric variables are independent of each other. All of this information combined provides a complete description of the dataset. 3.6.4 Methodology As mentioned before we are going to work on an unbalanced dataset. In order to handle this issue and achieve the best possible results during our future work we are going to use two measures: AUPRC and AUC. The former one is designed to take the lack of balance into account. The dataset will be divided into two partitions using stratification in order to handle scarce factor levels. The training part of the dataset is going to be used to compare the effects of many processes used to enhance the results. We are going to use five fold cross-validation. The final results are going to be presented using the test dataset. 3.6.4.1 Initial Data Preparation The first part concerned is Initial Data Preparation which focuses on imputing the missing data. The side tasks are handling outliers and transformation of skewed variables using logistic functions. A few most popular imputation methods will be compared. Four different methods were used. The first one was mean and mode imputation - replacing missing data with mean for numeric variables and mode for categorical. The next one was more advanced - KNN imputation - calculates the mean or mode from k nearest neighbours in training data. The third one was MICE imputation - the most advanced one, predictive mean matching. The last one was simply removing the rows containing missing data. After the imputation, the problem with skewness of fnlwgt distribution presented in figure 3.6.1 was handled. To deal with it a simple logarithmic transformation was conducted. The transformation was successful and the new feature has an appropriate distribution. 3.6.4.2 Feature Engineering and Tuning Firstly we are going to compare a few most popular Machine Learning models on our initially prepared dataset. We picked three popular explainable models: KNN, Decision Tree and Logistic Regression. During the Feature Engineering we will utilize strategies such as transforming and extracting features using the SAFE algorithm mentioned in the article above. The SAFE ML algorithm uses a complex model as a surrogate. New binary features are created on the basis of surrogate predictions. These new features are used to train a simple refined model. (…) method that uses elastic black-boxes as surrogate models to create a simpler, less opaque, yet still accurate and interpretable glass-box models. New models are created on newly engineered features extracted/learned with the help of a surrogate model. (Gosiewska et al. 2019) To extract new variables we will use rSAFE, which will depend on two different Black Box models: Random Forest and Adaboost. Those models were tuned to increase the final result. Based on the initial variables and the new ones created by the rSafe algorithm, we will choose the combination that will be the best for this problem. We will achieve this by analyzing various values of the “regularization penalty” parameter in the data extraction algorithm in the rSafe package and choosing the one that gives us the best result. The consecutive step is changing the classification task to regression which is not a common practice, but based on our previous experience it proves to be successful for decision tree models created on unbalanced data. The complexity does suffer from the increase in leaves, but the model retains it’s interpretability which was crucial for the task. Additionally we carried out a variable selection on those that are currently used. It was based on feature importance from a random forest model. We used ten most important variables from fifteen that were being utilised before. The last phase of this section was tuning our decision tree model. We based it on mlrMBO, but our method is adjusted to the task. The optimized measure is AUPRC, this proces is slightly different from optimizing AUC. Before using MBO we select the optimal range of parameters like: “minsplit” - the minimum number of observations that must exist in a node in order for a split to be attempted and “minbucket” - the minimum number of observations in any terminal node. After selecting ten optimal values of the first one we select ten optimal for the second. For each of those hundred pairs we use MBO to find the optimal “cp” - complexity parameter. Any split that does not decrease the overall lack of fit by a factor of cp is not attempted. 3.6.5 Results 3.6.5.1 Initial Data Preparation and Model Comparison Imputation type | AUPR AUPRC | A AUC | none | 0.64 0.6457237 | 0 0.8357408 | Mean and mode basic imputation | 0.64 0.6472785 | 0 0.8362379 | KNN imputation | 0.65 0.6547221 | 0 0.838437 | MICE imputation | 0.64 0.6452505 | 0 0.834577 | Missing data removal | 0.65 0.6515376 | 0 0.8355305 | Table 3.6.2 - results of different imputation measured in AUC and AUPRC From table 3.6.2 we can observe that the best results were achieved using KNN imputation so from now on this will be the used method. This is not surprising, because it is an advanced mean and mode imputation method. Difference between methods is little and even the KNN method improved AUPRC by less than one percent. Reason behind this is probably the low amount of missing data. Figure 3.6.3 - first results. The comparison of models performance by AUPRC. From figure 3.6.3 we can see that the best result was achieved by logistic regression which was surprising and shows how capable explainable models can be even on complex data. Although being the best on raw data, none of the later modifications had any impact on the model so we decided to exclude it from further considerations. The next best explainable model ranked by AUPRC was KNN and Decision Tree. Due to the fact that logistic regression obtained a better result than black boxes at the very start, and the KNN model is not globally interpretable, we will focus on the decision tree, which is a popular and easily interpretable model. That is the reason why we are going to work mainly with this model and initially with KNN for comparison in our goal to achieve similar results to Black Box models using it, but our final model will be a decision tree. The best Black boxes were Random Forest and Adaboost, because of that in the later phase of our project we will depend on them to make our results better. 3.6.5.2 Feature Engineering and Tuning Model | From From AUPRC | To AUPRC | Fr From AUC | To To AUC | decision tree | 0.655 0.655 | 0. 0.674 | 0.84 0.842 | 0.851 | knn | 0.698 0.698 | 0. 0.706 | 0.85 0.859 | 0.868 | Table 3.6.3 - base models performance before and after applying rSAFE measured in AUC and AUPRC From table 3.6.3 it can be observed that this method gave us a significant improvement in terms of AUC and AUPRC. New variables that were extracted and chosen with penalty regularization are: “age_new” - new levels: (-Inf, 32] and (32, Inf), “fnlwgt_new” - new levels: (-Inf, 11.70863] and (11.70863, Inf), \"hours.per.week_new - new levels: (-Inf, 40] and (40, Inf), “race_new” - new levels: White_Asian-Pac-Islander and Amer-Indian-Eskimo_Other_Black. After using rSAFE we conducted the rest of the workflow on decision tree models because non of the later modifications had any impact on the KNN model so we decided to exclude it from further considerations. Stage From AUPRC To AUPRC From AUC To AUC Change to regression 0.674 0.716 0.851 0.865 Variable selection 0.716 0.720 0.865 0.871 Tuning 0.720 0.794 0.871 0.912 Table 3.6.4 - model performance before and after each stage measured in AUC and AUPRC Changing the task improved the performance vastly and so we considered it worth it despite the added complexity. The improvement of variable selection on the other hand is slight, but because of this operation we are using a less complicated model, so we decided to use it and try to minimize the complexity gain generated by chaning the task type. Tuning was the most impactful part which surged the performance, topping all tested black boxes and logistic regession. 3.6.6 Final Results Figure 3.6.4 - improvement in AUPRC of decision tree model for each stage of workflow compared to other considered models From figure 3.6.4, we can see that thanks to our actions the performance of a fully interpretable model surpassed all basic black boxes and logistic regression, which from the start had really high results. Each stage improved the performance, but the most significant part was variable selection and tuning. Changing task to regression gave a surprisingly impactful upswing in AUPRC despite being the easiest one to conduct. On the other hand rSAFE and imputation were the most demanding, but provided the least improvement. 3.6.7 Conclusions Summarizing the work on adult dataset, we were able to create an explainable decision tree surpassing any other model using various data preparation and tuning methods. We were not required to take any compromises concerning explainability to achieve the best performance. Methods used such as rSAFE, imputation, or hyperparameter tuning do not affect the interpretability. The final results prove that it is possible to achieve comparable or even better results with interpretable models than black boxes even on unbalanced datasets, such as “adult”. It requires some work, but there are a lot of methods that make it possible to improve our performance. Some of them may be based on very well-functioning black box models. Probably in comparison with tuned black box model, we would achieve lesser results, but interpretable models have an advantage, people can understand why some choices were made and can be safely use when transparency and ease of understanding are needed. Using models such as logistic regression or decision trees make it possible. In this paper we showed that models built using these algorithms after some modifications are able to get similar results comparable to such powerful models as random forest or adaboost, and even artificial deep neural networks. References "],
["which-neighbours-affected-house-prices-in-the-90s.html", "3.7 Which Neighbours Affected House Prices in the ’90s?", " 3.7 Which Neighbours Affected House Prices in the ’90s? Authors: Hubert Baniecki, Mateusz Polakowski (Warsaw University of Technology) 3.7.1 Abstract The house price estimation task has a long-lasting history in economics and statistics. Nowadays, both worlds unite to exploit the machine learning approach; thus, achieve the best predictive results. In the literature, there are myriad of works discussing the performance-interpretability tradeoff apparent in the modelling of real estate values. In this paper, we propose a solution to this problem, which is a highly interpretable stacked model that outperforms the black-box models. We use it to examine neighbourhood parameters affecting the median house price in the United States regions in 1990. 3.7.2 Introduction Real estate value varies over numerous factors. These may be obvious like location or interior design, but also less apparent like the ethnicity and age of neighbours. Therefore, property price estimation is a demanding job that often requires a lot of experience and market knowledge. Is or was, because nowadays, Artificial Intelligence (AI) surpasses humans in this task (Conway 2018). Interested parties more often use tools like supervised Machine Learning (ML) models to precisely evaluate the property value and gain a competitive advantage (Park and Bae 2015; Rafiei and Adeli 2015; Liu and Liu 2019). The dilemma is in blindly trusting the prediction given by so-called black-box models. These are ML algorithms that take loads of various real estate data as input and return a house price estimation without giving their reasoning. Black-box complex nature is its biggest strength and weakness at the same time. This trait regularly entails high effectiveness but does not allow for interpretation of model outputs (Baldominos et al. 2018). Because of that, specialists interested in supporting their work with automated ML decision-making are more eager to use white-box models like linear regression or decision trees (Selim 2009). These do not achieve state-of-the-art performance efficiently, but instead, provide valuable information about the relationships present in data through model interpretation. For many years houses have been popular properties; thus, they are of particular interest for ordinary people. What exact influence had the demographic characteristics of the house neighbourhood on its price in the ’90s? Although in the absence of current technology, it has been hard to answer such question years ago (Din, Hoesli, and Bender 2001), now we can. In this paper, we perform a case study on the actual United States Census data from 1990 (“Census dataset” 1996) and deliver an interpretable white-box model that estimates the median house price by the region. We present multiple approaches to this problem and choose the best model, which achieves similar performance to complex black-boxes. Finally, using its interpretable nature, we answer various questions that give a new life to this historical data. 3.7.3 Related Work The use of ML in the real estate domain is a well-documented ground (Conway 2018) and not precisely a topic of this contribution. We relate to the works that aim to use Interpretable ML techniques (Molnar 2019b) to interpret models predictions in the house price estimation problem. The state-of-the-art approach to house price estimation is to combine linear and semi-log regression models with the Hedonic Pricing Method (Garrod and Willis 1992; Randeniya, Ranasinghe, and Amarawickrama 2017), which aims to determine the extent that environmental or ecosystem factors affect the price of a good. (Özalp and Akinci 2017) deliberately seeks to interpret the outcome and provide information about the parameters that affect property value. There are also comparisons between the linear white-box and black-box models (Selim 2009; Baldominos et al. 2018) which showcase the performance-interpretability tradeoff (Gosiewska and Biecek 2020). Nature of the topic might entail that data is place-specific; therefore, part of the studies focus on a single location with the use of geospatial data. The case study on London (Law 2017) links the street network community structure with house price, which takes into consideration the topology of the city. In contradiction, (Heyman and Sommervoll 2019) uses the Oslo city data to explore the differences between the relative and absolute location attributes. Applying data like the distance to the nearest shop and transportation is place-agnostic. One of the new ideas is to utilize location data from multiple sources in a Multi-Task Learning approach (Gao et al. 2019). It also studies the relationships between the tasks, which gives an extensive insight on prediction attributions. In this paper, we partially aim to enhance the use of regression decision tree models, which had been utilized to estimate the house prices based on their essential characteristics in (Fan, Ong, and Koh 2006). 3.7.4 Data For this case study we use the house_8L dataset crafted from the data collected in 1990 by the United States Census Bureau. Each record stands for a distinct United States state while the target value is a median house price in a given region. The variables are presented in Table 3.1. TABLE 3.1: Description of variables present in the house_8L dataset. Original.name New.name Description Median price price median price of the house in the region 33100 P3 house_n total number of households 505 H15.1 avg_room_n average number of rooms in an owner-occupied Housing Units 5.957 H5.2 forsale_h_pct percentage of vacant Housing Units which are for sale only 0.148 H40.4 forsale_6mplus_h_pct percentage of vacant-for-sale Housing Units vacant more then 6 months 0.500 P11.3 age_25_64_pct percentage of people between 25-64 years of age 0.483 P16.2 family_2plus_h_pct percentage of households with 2 or more persons which are family households 0.714 P19.2 black_h_pct percentage of households with black Householder 0.003 P6.4 asian_p_pct percentage of people which are of Asian or Pacific Islander race 0.002 Furthermore, we will apply our Metodology (Section 3.7.5) on a corresponding house_16H dataset, which has the same target but a different set of variables. More correlated variables of a higher variance make it significantly harder to estimate the median house price in a given region. Such validation will allow us to evaluate our model on a more demanding task. The comprehensive description of used data can be found in (“Census dataset” 1996). 3.7.5 Methodology In this section, we are going to focus on developing the best white-box model, which provides interpretability of features. Throughout this case study, we use the Mean Absolute Error (MAE) measure to evaluate the model performance, because we focus on the residuals while the mean of absolute values of residuals is the easiest to interpret. 3.7.5.1 Exploratory Data Analysis Performing the Exploratory Data Analysis highlighted multiple vital issues with the data. Firstly, Figure 3.7 presents the immerse skewness of the target, which usually leads to harder modelling. However, we decided not to transform the price variable because this might provide less interpretability in the end. Secondly, there are 46 data points with unnaturally looking target value. We suspect that the target value of 500001 is artificially made, so we removed these outliers. Finally, there are six percentage and two count variables, which indicates that there are not many possibilities for feature engineering. FIGURE 3.7: (L) Histogram of the target values shows that the distribution is very skewed. (R) Exemplary variable correlation with the target shows that there are few data points with the same, unnaturally high target value. It is also worth noting that there are no missing values and the dataset has over 22k data points which allow us to reliably split the data into train and test subsets using the 2:1 ratio. Throughout this case study, we use the Mean Absolute Error (MAE) measure to evaluate the model performance, because we later focus on the residuals while the mean of absolute values of residuals is the easiest to interpret. 3.7.5.2 SAFE The first approach was using the SAFE (Gosiewska et al. 2019) technique to engineer new features and produce a linear regression model. We trained a well-performing black-box ranger (Wright and Ziegler 2017) model and extracted new interpretable features using its Partial Dependence Profiles (Friedman 2000). Then we used these features to craft a new linear model which indeed was better than the baseline linear model by about 10%. It is worth noting that both of these linear models had a hard time succeeding because of the target skewness. 3.7.5.3 Divide-and-conquer In this section, we present the main contribution of this paper. The divide-and-conquer idea has many computer science applications, e.g. in sorting algorithms, natural language processing, or parallel computing. We decided to make use of its core principles in constructing the method for fitting the enhanced white-box model. The final result is multiple tree models combined which decisions are easily interpretable. The proposed algorithm presented in Figure 3.8 is: Divide the target variable with k middle points into k+1 groups. Fit a black-box classifier on train data which predicts the belonging to the i-th group. Use this classifier to divide the train and test data into k+1 train and test subsets. For every i-th subset fit a white-box estimator of target variable on the i-th train data. Use the i-th estimator to predict the outcome of the i-th test data. FIGURE 3.8: Following the steps 1-5 presented in the diagram, the divide-and-conquer algorithm is used to construct an enhanced white-box model. Such a stacked model consists of the black-box classifier and white-box estimators. The final product is a stacked model with one classifier and k+1 estimators. The exact models are for engineers to choose. It is worth noting that the unsupervised clustering method might be used instead of the classification model. 3.7.6 Results 3.7.6.1 The stacked model For the house price task, we chose k = 1, and the middle point was arbitrary chosen as 100k, which divides the data into two groups in about a 10:1 ratio. We used the ranger random forest model as a black-box classifier and the rpart (Therneau and Atkinson 2019b) decision tree model as a white-box estimator. The ranger model had default parameters with mtry = 3. The parameters of rpart models were: maxdepth = 4 - low depth reassures the interpretability of the model cp = 0.001 - lower complexity helps with the skewed target minbucket = 1% of the training data - more filled tree leaves adds up to higher interpretability Figure 3.9 depicts the tree that estimates cheaper houses, while Figure 3.10 presents the tree that estimates more expensive houses. FIGURE 3.9: The tree that estimates cheaper houses. Part of the stacked model. FIGURE 3.10: The tree that estimates more expensive houses. Part of the stacked model. Interpreting the stacked model presented in Figures 3.9 &amp; 3.10 leads to multiple conclusions. Firstly, we can observe the noticeable impact of features like the total number of households or the average number of rooms on the median price of the house in the region, which is compliant with basic intuitions. It is also evident that the bigger percentage of people between 25-64 years of age the higher the prices. Finally, we can observe the impact of critical features. The percentage of people which are of Asian or Pacific Islander race divides the prices in an opposing direction to the percentage of households with black Householder. The corresponding tree splits showcase which neighbours, and in what manner, affected house prices in the ’90s. Whether it is a correlation or causality is a valid debate that could be further investigated. 3.7.6.2 Comparison of the residuals In this section, we compare our stacked model with baseline ranger and rpart models, respectively referred to as black-box and white-box. Our solution achieves competitive performance with interpretable features. The main idea behind the divide-and-conquer technique was to minimize the maximum value of the absolute residuals, which reassures that no significant errors will happen. Such an approach may inevitably lead to minimizing the sum of the absolute residual values (MAE). In Figure 3.11, we can see that the targets mentioned above were indeed met. The stacked model not only has the lowest maximum error value but also has the best performance on average, as the red dot highlights the MAE score. FIGURE 3.11: Boxplots of residuals for the stacked model compared to black-box and white-box models. The plot is divided for cheaper and more expensive houses. The red dot highlights the MAE score. Figure 3.12 presents a more in-depth analysis of model residuals. In the top, we can observe that the black-box model has the lowest absolute residual mode (tip of the distribution), but the stacked model lays more in the centre (base of the distribution), which leads to more even spread of residuals. In the bottom, we can observe that the black-box model tends to undervalue house prices, while our model overestimates them. Looking at the height of the tip of the distribution and its shape, we can conclude that the stacked model provides more reliable estimations. FIGURE 3.12: Density of residuals for the stacked model compared to black-box and white-box models. The plot is divided for cheaper and more expensive houses. 3.7.6.3 Comparison of the scores Finally, we present the comparison of MAE scores for all of the used models in this case study in Table 3.2. There are two tasks with different variables, complexity and correlations. We calculate the scores on the test subsets. We can see that the linear models performed the worse, although the SAFE approach noticeably lowered the MAE. Then there is a decision tree which performed better but not so on the more laborious task. Both of the black-box models did a far better job at house price estimation than interpretable models. Finally, our stacked model is a champion with the best performance on both of the tasks. TABLE 3.2: Comparison of the MAE score for all of the used models on test datasets. Light colour highlights white-box models, while the dark colour is for black-box models. Dataset (test) Model house_8L house_16H linear model 23.1k 24.1k SAFE on ranger 21.4k 22.6k rpart 19.2k 22.1k xgboost 16k 16.5k ranger 14.8k 15.6k stacked model 14.6k 15.3k 3.7.7 Conclusions This case study aimed to provide an interpretable machine learning model that achieves state-of-the-art performance on the datasets crafted from the data collected in 1990 by the United States Census Bureau. We not only provided such a model but also examined its decision-making process to determine how it estimates the median house price in the United States regions. The stacked model had prominently shown the impact of neighbours’ age and race in predicting the outcome. We believe that the principles of the divide-and-conquer algorithm can be successfully applied in other domains to neglect the performance-interpretability tradeoff apparent while using machine learning models. In further research, we would like to generalize this approach into a well-defined framework and apply it to several different problems. References "],
["explainable-computer-vision-with-embedding-and-k-nn-classifier.html", "3.8 Explainable Computer Vision with embedding and k-NN classifier", " 3.8 Explainable Computer Vision with embedding and k-NN classifier Authors: Olaf Werner, Bogdan Jastrzębski (Warsaw University of Technology) 3.8.1 Abstract Interpretability of machine learning models become in recent years even more important. The need for it raised a wave of criticism towards deep neural networks (DNNs), as they are not interpretable. However, DNN can give much better results than other classifiers for different tasks, e.g. image recognition, and therefore there is a trade-off between interpretability and robustness. In this work, we investigate further the idea of Deep k-Nearest Neighbors (DkNN), simplifying the previous algorithm. We present an easy augmentation technique of k-NN classifier, that can significantly boost its performance in image recognition using autoencoders or other unsupervised learning algorithms. We obtain a classifier that is interpretable in a sense, that we can provide explanatory training set observations and at the same time much more robust. This technique, however, as presented here, produced worse results than a convolutional neural network, and therefore there is still a room for improvement. 3.8.2 Introduction Artificial neural networks are widely used in computer vision. Convolutional neural networks [3], in particular, can achieve very high performance on this task and surpass other popular classifiers like SVM, logistic regression, and decision trees. However, the increase in performance is associated with a loss of interpretability. Artificial neural networks are infamous for their complexity and lack of interpretability and are often criticized for it. K-nearest neighbours classifier (k-NN) one of the simplest classification algorithms, yet it is interpretable and highly complex. The idea behind it is simple. To predict the class of a new observation, e.g. an image, we calculate the measure of similarity between it and all observations in collected training data. Then we find an arbitrarily chosen number of the most similar observations. We base our prediction on their labels. There are several problems with k-NN. Its prediction depends on the choice of the mentioned measure of similarity. The optimal measure can be very complex and non-trivial. Let’s take the problem of image recognition as our example. Two images that represent the same object on a different background can have the majority of the values of pixels different. A measure that treats each pixel independently, e.g. euclidean distance, would lead to an incorrect conclusion, that those images display very different objects. The second problem with the k-NN is that it scales poorly with the size of the training dataset, as every time we make a prediction, we must measure the similarity between a new observation and all observations in our training dataset, which can become very time-consuming. Interpretability of the k-NN classifier is in two things: interpretability of the measure of similarity we use and in that, with each prediction, we can provide the most similar observations from the training data set used to make the prediction. We argue that the latter is more important and that we can still call a model interpretable if the measure of similarity is not. The idea is not new. It has been previously investigated by Nicolas Papernot and Patrick McDaniel from Pennsylvania State University [6]. In our approach, we simplify their algorithm. We propose to use data embedding techniques to produce more meaningful spaces, where the use of p-norms as similarity measures is more justified. In this article, we will show that it is possible to increase significantly the performance of the k-NN using this technique. 3.8.3 Methodology The simplest and one of the most robust classifiers is k-NN. It doesn’t generalize information, instead, it saves training dataset and during prediction, it finds the most similar historical observations and predicts the label of a new observation based on their labels. However, it not only can’t distinguish important features from not important ones but also to find more complex interactions between variables. One of the simplest, yet very robust classification algorithms is k-NN. It does not generalize knowledge, instead, it finds the most similar observations from the training data set to new observation and predicts its class based on theirs. However, it’s not only unable to distinguish important from irrelevant features, but also find interactions between variables. A way to improve classification results with the k-NN algorithm is to determine a better measure of similarity than the p-norm. In this paper, we propose a measure of similarity of the following form: \\[ dist(x,y) := \\| Emb(x) - Emb(y) \\|_p \\] where \\(Emb: \\mathbb{R}^n \\rightarrow \\mathbb{R}^m\\), \\(\\mathbb{R}^n\\) is the space of observations and \\(\\mathbb{R}^m\\) is the space of embedding (n &gt; m). Note, that \\(dist\\) is not a metric. An embedding can be done made in various ways. In this article, we will explore different embedding techniques, including: SVD embedding Convolutional Autoencoder K-means embedding 3.8.3.1 Data To present our results, we are going to use the data set Fashion-Mnist. Fashion-MNIST is a dataset of Zalando’s article images, consisting of a training set of 60,000 examples and a test set of 10,000 examples. Each example is a 28x28 grayscale image, associated with a label from 10 classes. We have the following classes : T-shirt/top Trouser Pullover Dress Coat Sandal Shirt Sneaker Bag Ankle boot. 3.8.3.2 Autoenconders and Embedding In the theory of information and statistics, we distinguish data from information. Data is a carrier of information. Information is something in data, that is meaningful to us. For instance, consider two highly correlated random variables \\(X\\) and \\(Y\\). Knowing the value of one gives us great knowledge of the value of the other. Hence, if we have prior knowledge about the way in which those variables are connected: \\(2X + 3 \\approx Y\\), there is little difference between knowing the value of one of them: \\(X = 23.3454\\), and both values:\\(X = 23.3454, Y = 48.9877\\), because we can roughly guess value of one based on the other: \\(X = 23.3454, 2X + 3 \\approx Y \\models Y \\approx 49.6908\\). The relation between variables can be very complex. We still lose some information though. In statistics and machine learning, we try to represent data in a meaningful way, in that sense, representation gives us a lot of information in very little data. In other words, we try to find a way to compress data. One way to do so is to use autoencoders. An autoencoder is a parametric function composed of two parts: the encoder part and the decoder part. \\[Autoenc = Decoder \\circ Encoder\\] where \\(Encoder: \\mathbb{R}^n \\rightarrow \\mathbb{R}^m\\), \\(Decoder: \\mathbb{R}^m \\rightarrow \\mathbb{R}^n\\) and \\(n&gt;m\\). We want our autoencoder to compress and decompress data i.e. firstly shrink data to some representation and then recreate observation based on this low-dimensional representation. We want our recreated data to be as close to original observation as possible, for instance in mean square error (MSE): \\[min_\\theta \\frac{1}{n}\\sum_{i=1}^{n}\\|(Decoder_\\theta \\circ Encoder_\\theta)(x_i)-x_i \\|^2\\] 3.8.3.3 The use of standard interpretable models in computer vision We often think of logistic regression or decision tree classifiers as being interpretable. Their interpretability, however, relies strongly on the interpretability of observation features. When features are hardly interpretable, the information that a particular feature has been used is meaningless. For instance, if we know, that at some point the classifier makes a choice based on the age of the patient, it is a piece of meaningful information. On the other hand, if we know, that at some point the classifier makes a choice based on one of a million pixels, this information might be meaningless to us. It gives very little explanation, why a particular pixel has been used. In this section, we will explore the use of standard interpretable models and we will try to argue, that they are not useful when it comes to computer vision. Fig. 1: An example of logistic regression weights: In the picture we can see a visualisation of the weights of linear regression model (bright pixel values mean large weight and dark - small). This picture shows, that it is impossible to interpret those weights. 3.8.3.3.1 Logistic Regression Logistic regression[1] is a basic classification model. We get the probability of belonging to a given class by: \\[{\\displaystyle p={\\frac {e^{\\beta _{0}+\\beta _{1}x_{1}+...+\\beta _{n}x_{n}}}{e^{\\beta _{0}+\\beta _{1}x_{1}+...+\\beta _{n}x_{n}}+1}}}\\] where \\(\\beta _{0},\\beta _{1},...,\\beta _{n}\\) are coefficients of logistic regression. We obtain coefficients using gradient descent. Because we have multiple labels, we train 10 different logistic regression models and use softmax function to normalize probabilities of belonging to any particular class. We then visualize coefficients as images with bright spots indicating that they are important. The result is depicted in fig 1. As you can see, it doesn’t convey the information about, how does the model make a prediction. This shows, that linear regression might not be useful for interpretation in image recognition. 3.8.3.3.2 Decision Trees Decision trees are very useful classifiers that can be interpreted, however, they are suitable when we have very few meaningful dimensions. The tree that at a time makes a choice based on individual pixels is not a good classifier and it’s explanation provides little knowledge about, why the selected pixels have been chosen. Therefore, we will not examine the use of this class of classifiers. 3.8.3.4 Our Approach In this section we will show an alternative to logistic regression and decision trees, that is more interpretable and at the same time can obtain significantly better results. 3.8.3.4.1 The k-NN Classifier k-NN (k nearest neighbours) is a classifier, that doesn’t generalize data. Instead, we keep the training dataset, and every time we make a prediction, we calculate the distance (for instance euclidean distance) between our new observation and all observations in the training dataset to find k nearest. Prediction is based on their labels. k-NN is a robust classifier, that copes with highly nonlinear data. It’s also interpretable because we can always show k nearest neighbours, which are an explanation by themselves. It, however, is not flawless. It for instance poorly scales with the size of the training dataset, while at the same time it needs, at least in some domains, a very big training dataset, as it doesn’t generalize any information. We can significantly improve its performance by introducing complex similarity functions. If the similarity function is interpretable, we obtain a highly interpretable classifier. If not, we get semi interpretable classifier, where we cannot tell, why observations are similar according to the model, however, we can at least show similar training set examples, based on which prediction has been made. This complex distance functions can be made in many different ways. In this paper, we explore functions of a form \\[dist(Img1, Img2) = d_e(Emb(Img1), Emb(Img2))\\] where \\(d_e\\) is the euclidean distance, so we simply compute the euclidean distance between embeddings of images. The scheme of our augmented k-NN classifier is displayed in fig. 1. As we can see, a new image firstly gets embedded and then a standard classification with k-NN is made. This type of architecture allows us to create a robust and interpretable classifier. Fig. 2: The k-NN Classifier Architecture: In the picture we can see the k-NN classifier architecture. Firstly an image get’s embedded by the embedder and then, combined with embedded training dataset, form prediction via k-NN. 3.8.3.4.2 Embedding techniques In this section, we will explore different embedding techniques and their potential use. 3.8.3.4.2.1 K-means We use the K-means algorithm [2] (also known as the Lloyd algorithm) to find subclasses in every class. Algorithm: Initiate a number of random centroids For every observation find the nearest centroid Calculate the average of observations in every group found in point 2 These averages become new centroids Repeat points 2 to 4 until all new centroids are at the distance less than \\(\\epsilon\\) from old centroids We use euclidean distance. Prediction for every new observation is simply class of nearest centroid. The algorithm is even more interpretable because we can visualize centroids as images. Thanks to using K-means to find subclasses our images are not blurry. Also because the number of all subclasses is much lower than the number of records in data set using k-NN only on centroids is much faster. Consider the dataset showed in fig. 3 a). In fact, a good subset of the training data set is enough to create a very good classifier. For instance, 5 points depicted on fig. 3 b), approximate sufficiently training data distribution. Notice, that we, in fact, don’t have to choose particular observations. We can instead choose points in observation space that are similar to observations. This is what the k-means algorithm does and so, we can obtain good training data approximation using k-means. An example of a centroid image is showed in fig. 4. Fig 3: K-means embedding example. In the picture a) we can see original set and in the picture b) - the same set with it’s representation Fig 4: An example of a centroid image. A centroid in observation space is just an image. Here we can see, that this centroid is close to the “shoes cluster”. 3.8.3.4.2.2 SVD SVD is a standard method of dimensionality reduction [4]. It is rewriting \\(m\\times n\\) matrix \\(M\\) as \\(U\\Sigma V^T\\) where \\(U\\) is \\(m\\times m\\) orthonormal matrix, \\(V^T\\) is \\(n\\times n\\) orthonormal matrix and \\(\\Sigma\\) is \\(m\\times n\\) rectangular diagonal matrix with non-negative real numbers on the diagonal. We assume that singular values of \\(\\Sigma\\) are in descending order. Now by taking the first columns of \\(V^T\\) we get vectors that are the most relevant. Let \\(V_n\\) be a matrix, whose columns are \\(V\\) columns with n greatest eigenvalues. Such a matrix is a linear transformation matrix, that turns observations into their embedding. It can be shown, that \\(V_n\\) is the best transformation in \\(L_2\\) norm sense. The SVD autoencoder scheme is depicted in fig. 5. Visualisation of an eigenvector, an image from dataset and the same image with an eigenvector used as a filter is showed in the picture 6. Fig. 5: SVD autoencoder diagram: In this picture we can see SVD autoencoder diagram. Fig. 6: Most important eigenvector. In the picture a) we can see an eigenvector, b) an image from dataset and in c) two images combined using a) as a filter and b) as a background. Because there in the picture c) there is still great bright area (intersection of a) and c) is significant), value in representation associated with this eigenvector will be larger. 3.8.3.4.2.3 Convolutional Autoencoder We can create a semi-interpretable model by training a convolutional autoencoder and then creating a k-NN classifier on pre-trained embedding. As mentioned previously, it has several advantages over k-NN, because it uses \\(L^2\\) distance in more meaningful space. Embedder is not interpretable, but our classifier can at least show us historical observations, that had an impact on prediction, which sometimes is good enough, especially when it can be easily seen why two images are similar and we only want a computer to do humans work. For instance, if we provide 5 images of shoe that caused that our image of the shoe has been interpreted as shoe, we maybe don’t know, why those images are similar according to our classifier, however, we can see, that they are similar, so a further explanation of a model is not required. This model, again, is not fully interpretable. Our implementation of convolutional autoencoder consists of the following layers: Conv2d: Input Channels: 1 Output Channels: 50 filter size: 5 Conv2d: Input Channels: 50 Output Channels: 50 filter size: 5 Conv2d: Input Channels: 50 Output Channels: 10 filter size: 5 Conv2d: Input Channels: 10 Output Channels: 10 filter size: 5 Conv2d: Input Channels: 10 Output Channels: 1 filter size: 5 Conv2d: Input Channels: 1 Output Channels: 10 filter size: 5 Conv2d: Input Channels: 10 Output Channels: 10 filter size: 5 Conv2d: Input Channels: 10 Output Channels: 50 filter size: 5 Conv2d: Input Channels: 50 Output Channels: 50 filter size: 5 Conv2d: Input Channels: 50 Output Channels: 1 filter size: 5 along with pooling and unpooling between. Fig. 10: Architecture of the convolutional autoencoder: composed of several convolution layers. 3.8.3.5 Black-Box Convolutional Neural Networks The classical approach in computer vision is to use convolutional neural networks [3]. A standard artificial neural network sees all variables as being independent of each other. It doesn’t capture the same patterns across image space, nor it recognizes, that two pixels next to each other are somehow related. Shifted image is something completely different to a standard neural network from it’s original. Therefore, ANN won’t produce good results. There is, however, a smarter approach that can cope with those problems: convolutional neural networks. A convolutional neural network is an artificial neural network, that tries to capture spacial dependencies between variables, for instance, dimensions of pixels that are close to each other. It does that via introducing convolution. The easiest interpretation of what a convolutional neural network is doing is that instead of training big network that uses all variables (in our case all pixels), we train smaller transformation with a smaller number of variables (a smaller subset of pixels close to each other), that we use in many different places on the image. In some sense, we train filters. Every filter produces a corresponding so-called “channel”. After the first layer, we can continue filtering channels using convolutional layers. We place a dense layer (or a number of them) at the end and its result is our prediction. For further reading, please see [3]. Having a very good performance, they are impossible to explain. There are some techniques of visualizing filters [8], however more complex networks are generally uninterpretable. Along with standard artificial neural network, we will use it as an instance of robust classifier for comparing results. Our implementation of convolutional neural networks consist of the following layers: Conv 2d: Input Channels: 1 Output Channels: 50 filter size: 5 Max Pool: Size: 2 Conv 2d: Input Channels: 50 Output Channels: 70 filter size: 5 Max Pool: Size: 2 Conv 2d: Input Channels: 70 Output Channels: 100 filter size: 5 Max Pool: Size: 2 Conv 2d: Input Channels: 100 Output Channels: 150 filter size: 5 Linear: Input_size: 1350 Output_size: 500 Linear: Input_size: 200 Output_size: 10 Here’s architecture’s visualization: Fig. 11: The architecture of the Convolutional classifier: there are two section to this classifier. The first is convolutional layers section and the second is dense layers section. 3.8.4 Results Model ACC Black-Box Convolutional 0.941 k-NN Convolutional 0.923 k-NN base 0.8606 k-NN K-means 0.8512 Logistic regression 0.847 k-NN SVD 0.8001 We use accuracy because classes are balanced and any measure which accounts for similarities between classes (for example classifying Sandal as Ankle boot is worth more than classifying Sandal as Bag) seemed arbitrary to us. As expected Convolutional Neural Network is by far the best model here. However, we still achieved something. While using embedders such as K-means and SVD resulted in worse results, the accuracy was not that much worse and we reduced dimensionality at least a thousand folds, which resulted in much faster calculations. Using Convolutional embedder got us second-best results and while it is semi interpretable it is still better than base K-NN and Logistic regression which turned out to be not interpretable. 3.8.5 Discussion and Conclusion In this article, we proposed an easy way of getting interpretable computer vision using convolutional embedder. While it is not fully interpretable, explanations are not made post factum as in traditional neural networks and it is achieving better results than less robust classifiers. Interpretability of a model in complex problems, such as image recognition is difficult to achieve. In computer vision, standard interpretable models not only don’t give satisfying results but also fail to provide meaningful explanations due to high dimensionality of data and general lack of meaningfulness of individual variables (values of the brightness of pixels). While interpretability of decision trees or linear regression depends directly on the interpretability of variables, in k-NN we explain predictions by providing the most similar observations from training data set. In the case of computer vision, k-NN doesn’t give as good results as deep neural networks. However, it’s performance can be improved via more complex similarity measures. As we saw, measure composed of L2 distance measured between learned representations via autoencoders can increase the accuracy of a k-NN model. In addition, especially when it comes to images, if done correctly, explanation of the way the similarity of two images is calculated is almost always unnecessary, as the similarity is visible. The technique presented in this paper is not only simple to implement, but also very general, as there exists a great variety of embedding techniques that can be used. Linear embedding techniques, such as SVD embedding turned out to be worse than a standard k-NN. Also, k-means encoding didn’t improve classification accuracy. However, convolutional autoencoder, which is highly non-linear and can model a great variety of functions (instead of k-means) achieved much better results than a standard k-NN. Those results were not as good as the results of the black-box model but comparable. In the future, it’s worth to explore the use of different non-linear embedding techniques, as they are promising. This approach is very general and can be applied not only to computer vision, and so, the benefits of the use of it in different domains are also yet to be discovered. 3.8.6 Bibliography Park, Hyeoun-Ae. “An Introduction to Logistic Regression: From Basic Concepts to Interpretation with Particular Attention to Nursing Domain”, College of Nursing and System Biomedical Informatics National Core Research Center, Seoul National University, Seoul, Korea, 2013. Accessed at https://pdfs.semanticscholar.org/3305/2b1d2363aee3ad290612109dcea0aed2a89e.pdf?fbclid=IwAR2AEWs_oTJsGldDkTNdu5oDuwqMRG9URpYFTYg4ONEdxUSTbXS2AntHLNM Huda Hamdan Ali, Lubna Emad Kadhum. “K-Means Clustering Algorithm Applications in Data Mining and Pattern Recognition”, International Journal of Science and Research (IJSR), 2015. Accessed at https://pdfs.semanticscholar.org/a430/da239982e691638b7193ac1947da8d0d241b.pdf?fbclid=IwAR33LLbo0m9qcyayoI3qj1tJsnB8YzYehzFK7VUGz4tkH_IlATvhknPKOuk Keiron O’Shea1, Ryan Nash. “An Introduction to Convolutional Neural Networks”, Department of Computer Science, Aberystwyth University, Ceredigion, SY23 3DB, 2015. Accessed at https://www.researchgate.net/publication/285164623_An_Introduction_to_Convolutional_Neural_Networks?fbclid=IwAR35OQjrXNAm5549CX0-LkkdjppnNZIlnKfnFkHUcHsUZ_G-wdYDZ0v6SVY Carla D. Martin, Mason A. Porter. “The Extraordinary SVD”. Accessed at https://people.maths.ox.ac.uk/porterm/papers/s4.pdf?fbclid=IwAR2rC7ho-hLqtyR0eY5KqlYV_DbaKk7KcyE9PtT4hx1MkbXtnG04fe71uEo Gongde Guo, Hui Wang, David Bell, Yaxin Bi, Kieran Greer. “KNN Model-Based Approach in Classification”, School of Computing and Mathematics, University of Ulster. Accessed at http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.2.815&amp;rep=rep1&amp;type=pdf&amp;fbclid=IwAR0qK9dIhhmuj4-0V98Tn6dKzjKvivmfmucJVjDqV319eW_BJfWkt92Cy5E Nicolas Papernot, Patrick McDaniel. “Deep k-Nearest Neighbors: Towards Confident, Interpretable and Robust Deep Learning”, Department of Computer Science and Engineering, Pennsylvania State University, 2018. Accessed at https://arxiv.org/pdf/1803.04765.pdf?fbclid=IwAR2D5gqQf9SL0xRWBctEVrUCL9uUiIf9lZrpPN83YZYbiCGdLAlMlhhaVns Dor Bank, Noam Koenigstein, Raja Giryes. “Autoencoders”, 2020. Accessed at https://arxiv.org/pdf/2003.05991.pdf D. Erhan, Y. Bengio, A. Courville, and P. Vincent. “Visualizing higher layer features of a deep network”, University of Montreal, vol. 1341, no. 3, p. 1, 2009. "],
["acknowledgements.html", "Chapter 4 Acknowledgements", " Chapter 4 Acknowledgements This project is inspired by a fantastic book Limitations of Interpretable Machine Learning Methods created at the Department of Statistics, LMU Munich. We used the LIML project as cornerstone for this reopsitory. "],
["references.html", "References", " References accenture. 2018. “UNDERSTANDING Machines: EXPLAINABLE Ai,” 19. https://www.accenture.com/_acnmedia/pdf-85/accenture-understanding-machines-explainable-ai.pdf?fbclid=IwAR0ZtyDNzHR8dMUJHPwa0CkuQXgOOE68UQV4JCcBxXudO3dlm14LjqX-B8g. Alejandro Barredo Arrieta, Javier Del Ser, Natalia Díaz-Rodríguez. 2019. “Explainable Artificial Intelligence (Xai): Concepts, Taxonomies, Opportunities and Challenges Toward Responsible Ai,” 67. https://arxiv.org/abs/1910.10045. Anda, Bente, Dag Sjøberg, and Audris Mockus. 2009. “Variability and Reproducibility in Software Engineering: A Study of Four Companies That Developed the Same System.” Software Engineering, IEEE Transactions on 35 (July): 407–29. https://doi.org/10.1109/TSE.2008.89. Anderson, Christopher, Joanna Anderson, Marcel van Assen, Peter Attridge, Angela Attwood, Jordan Axt, Molly Babel, et al. 2019. “Reproducibility Project: Psychology.” https://doi.org/10.17605/OSF.IO/EZCUJ. Ardia, David, Lennart F. Hoogerheide, and Herman K. van Dijk. 2009. “AdMit.” The R Journal 1 (1): 25–30. https://doi.org/10.32614/RJ-2009-003. “Ascent of Machine Learning in Medicine.” 2019. Nature Materials 18 (5): 407–7. https://doi.org/10.1038/s41563-019-0360-1. Balan, Theodor, and Hein Putter. 2019. “FrailtyEM: An R Package for Estimating Semiparametric Shared Frailty Models.” Journal of Statistical Software, Articles 90 (7): 1–29. https://doi.org/10.18637/jss.v090.i07. Baldi, P., P. Sadowski, and D. Whiteson. 2014. “Searching for Exotic Particles in High-Energy Physics with Deep Learning.” Nature Communications 5 (1): 4308. https://doi.org/10.1038/ncomms5308. Baldominos, Alejandro, Iván Blanco, Antonio Moreno, Rubén Iturrarte, Óscar Bernárdez, and Carlos Afonso. 2018. “Identifying Real Estate Opportunities Using Machine Learning.” Applied Sciences 8 (November): 2321. https://doi.org/10.3390/app8112321. Batista, Gustavo E. A. P. A., and Maria Carolina Monard. 2003. “An Analysis of Four Missing Data Treatment Methods for Supervised Learning.” Applied Artificial Intelligence 17 (5-6): 519–33. https://doi.org/10.1080/713827181. Bernd Bischl, Jakob Bossek, Jakob Richter. 2018. “MlrMBO: A Modular Framework for Model-Based Optimization of Expensive Black-Box Functions,” 23. https://arxiv.org/abs/1703.03373. Biecek, Przemyslaw. 2018. “DALEX: Explainers for Complex Predictive Models in R.” Journal of Machine Learning Research 19 (84): 1–5. http://jmlr.org/papers/v19/18-416.html. Biecek, Przemyslaw, and Marcin Kosinski. 2017. “archivist: An R Package for Managing, Recording and Restoring Data Analysis Results.” Journal of Statistical Software 82 (11): 1–28. https://doi.org/10.18637/jss.v082.i11. Bischl, Bernd, Giuseppe Casalicchio, Matthias Feurer, Frank Hutter, Michel Lang, Rafael Mantovani, Jan van Rijn, and Joaquin Vanschoren. 2017. “OpenML Benchmarking Suites and the Openml100,” August. Bischl, Bernd, Michel Lang, Lars Kotthoff, Julia Schiffner, Jakob Richter, Erich Studerus, Giuseppe Casalicchio, and Zachary M. Jones. 2016a. “Mlr: Machine Learning in R.” Journal of Machine Learning Research 17 (170): 1–5. http://jmlr.org/papers/v17/15-066.html. ———. 2016b. “Mlr: Machine Learning in R.” Journal of Machine Learning Research 17 (170): 1–5. http://jmlr.org/papers/v17/15-066.html. ———. 2016c. “mlr: Machine Learning in R.” Journal of Machine Learning Research 17 (170): 1–5. http://jmlr.org/papers/v17/15-066.html. Bischl, Bernd, Jakob Richter, Jakob Bossek, Daniel Horn, Janek Thomas, and Michel Lang. 2017. “MlrMBO: A Modular Framework for Model-Based Optimization of Expensive Black-Box Functions.” arXiv Preprint arXiv:1703.03373. Boehmke, Bradley, and Jason Freels. 2017. “LearningCurve: An Implementation of Crawford’s and Wright’s Learning Curve Production Functions.” The Journal of Open Source Software 2 (May). https://doi.org/10.21105/joss.00202. Bogucki, Wojciech, Tomasz Makowski, and Dominik Rafacz. 2020. “Feature-Engineering.” GitHub Repository. https://github.com/DominikRafacz/feature-engineering; GitHub. Bono, Christine, L. Ried, Carole Kimberlin, and Bruce Vogel. 2007. “Missing Data on the Center for Epidemiologic Studies Depression Scale: A Comparison of 4 Imputation Techniques.” Research in Social &amp; Administrative Pharmacy : RSAP 3 (April): 1–27. https://doi.org/10.1016/j.sapharm.2006.04.001. Boughorbel S., El-Anbari M., Jarray F. 2017. “Optimal Classifier for Imbalanced Data Using Matthews Correlation Coefficient Metric.” PLOS One. https://doi.org/https://doi.org/10.1371/journal.pone.0177678. Broatch, Jennifer, Jennifer Green, and Andrew Karl. 2018. “RealVAMS: An R Package for Fitting a Multivariate Value- added Model (VAM).” The R Journal 10 (1): 22–30. https://doi.org/10.32614/RJ-2018-033. Brown, Eric. 2019. “Tacmagic: Positron Emission Tomography Analysis in R.” The Journal of Open Source Software 4 (February): 1281. https://doi.org/10.21105/joss.01281. Brown, Patrick, and Lutong Zhou. 2010. “MCMC for Generalized Linear Mixed Models with glmmBUGS.” The R Journal 2 (1): 13–17. https://doi.org/10.32614/RJ-2010-003. Burns, David M., and Cari M. Whyne. 2018. “Seglearn: A Python Package for Learning Sequences and Time Series.” Journal of Machine Learning Research 19 (83): 1–7. http://jmlr.org/papers/v19/18-160.html. Buuren, Stef Van. 2020. Mice: Multivariate Imputation by Chained Equations. https://cran.r-project.org/web/packages/mice/index.html. Buuren, Stef van, and Karin Groothuis-Oudshoorn. 2011. “Mice: Multivariate Imputation by Chained Equations in R.” Journal of Statistical Software 45 (3): 1–67. https://www.jstatsoft.org/article/view/v045i03. ———. 2020. Mice: Multivariate Imputation by Chained Equations. https://CRAN.R-project.org/package=mice. Casalicchio, Giuseppe, Bernd Bischl, Dominik Kirchhoff, Michel Lang, Benjamin Hofner, Jakob Bossek, Pascal Kerschke, and Joaquin Vanschoren. 2019. OpenML: Open Machine Learning and Open Data Platform. https://CRAN.R-project.org/package=OpenML. “Census dataset.” 1996. http://www.cs.toronto.edu/~delve/data/census-house/censusDetail.html. Chen, Tianqi, and Carlos Guestrin. 2016. “XGBoost: A Scalable Tree Boosting System.” CoRR abs/1603.02754. http://arxiv.org/abs/1603.02754. Coeurjolly, J.-F., R. Drouilhet, P. Lafaye de Micheaux, and J.-F. Robineau. 2009. “asympTest: A Simple R Package for Classical Parametric Statistical Tests and Confidence Intervals in Large Samples.” The R Journal 1 (2): 26–30. https://doi.org/10.32614/RJ-2009-015. Computing Machinery, Association for. 2018. “Artifact Review and Badging.” https://www.acm.org/publications/policies/artifact-review-badging. Conway, Jennifer. 2018. “Artificial Intelligence and Machine Learning : Current Applications in Real Estate.” PhD thesis. https://dspace.mit.edu/bitstream/handle/1721.1/120609/1088413444-MIT.pdf. Coyle, Jeremy, and Nima Hejazi. 2018. “Origami: A Generalized Framework for Cross-Validation in R.” The Journal of Open Source Software 3 (January): 512. https://doi.org/10.21105/joss.00512. Daniel J. Stekhoven, Peter Bühlmann. 2011. “MissForest—Non-Parametric Missing Value Imputation for Mixed-Type Data.” Bioinformatics 28 (1): 112–18. https://academic.oup.com/bioinformatics/article/28/1/112/219101. Din, Allan, Martin Hoesli, and André Bender. 2001. “Environmental Variables and Real Estate Prices.” Urban Studies 38 (February). https://doi.org/10.1080/00420980120080899. Dramiński, Michał, and Jacek Koronacki. 2018. “Rmcfs: An R Package for Monte Carlo Feature Selection and Interdependency Discovery.” Journal of Statistical Software, Articles 85 (12): 1–28. https://doi.org/10.18637/jss.v085.i12. Dray, Stéphane, and Anne-Béatrice Dufour. 2007. “The Ade4 Package: Implementing the Duality Diagram for Ecologists.” Journal of Statistical Software, Articles 22 (4): 1–20. https://doi.org/10.18637/jss.v022.i04. Drummond, Chris. 2012. “Reproducible Research: A Dissenting Opinion.” In. Eisner, D. A. 2018. “Reproducibility of Science: Fraud, Impact Factors and Carelessness.” Journal of Molecular and Cellular Cardiology 114 (January): 364–68. https://doi.org/10.1016/j.yjmcc.2017.10.009. Elmenreich, Wilfried, Philipp Moll, Sebastian Theuermann, and Mathias Lux. 2018. “Making Computer Science Results Reproducible - a Case Study Using Gradle and Docker,” August. https://doi.org/10.7287/peerj.preprints.27082v1. Escobar, Modesto, and Luis Martinez-Uribe. 2020. “Network Coin Cidence Analysis: The netCoin R Package.” Journal of Statistical Software, Articles 93 (11): 1–32. https://doi.org/10.18637/jss.v093.i11. Fan, Gang-Zhi, Seow Eng Ong, and Hian Koh. 2006. “Determinants of House Price: A Decision Tree Approach.” Urban Studies 43 (November): 2301–16. https://doi.org/10.1080/00420980600990928. Fenton, N. E., and S. L. Pfleeger. 1997. Software Metrics: A Rigorous &amp; Practical Approach. International Thompson Press. Fern’andez, Daniel M’endez, Daniel Graziotin, Stefan Wagner, and Heidi Seibold. 2019. “Open Science in Software Engineering.” CoRR abs/1904.06499. http://arxiv.org/abs/1904.06499. Fernández, Daniel Méndez, Daniel Graziotin, Stefan Wagner, and Heidi Seibold. 2019. “Open Science in Software Engineering.” ArXiv abs/1904.06499. Fiona M. Shrive, Hude Quan, Heather Stuart. 2006. “Dealing with Missing Data in a Multi-Question Depression Scale: A Comparison of Imputation Methods.” BMC Medical Research Methodology 6 (57). https://link.springer.com/article/10.1186/1471-2288-6-57. Fisher, Aaron, Cynthia Rudin, and Francesca Dominici. 2018. “All Models are Wrong, but Many are Useful: Learning a Variable’s Importance by Studying an Entire Class of Prediction Models Simultaneously.” arXiv E-Prints, January, arXiv:1801.01489. http://arxiv.org/abs/1801.01489. Flach, Peter, Jose Hernandez-Orallo, and Cèsar Ferri. 2011. “A Coherent Interpretation of Auc as a Measure of Aggregated Classification Performance.” In Proceedings of the 28th International Conference on Machine Learning, ICML 2011, 657–64. Flach Peter, Ferri, Hernandez-Orallo Jose. 2011. “A Coherent Interpretation of Auc as a Measure of Aggregated Classification Performance.” Proceedings of the 28th International Conference on Machine Learning. https://icml.cc/2011/papers/385_icmlpaper.pdf. Fomel, Sergey, Paul Sava, Ioan Vlad, Yang Liu, and Vladimir Bashkardin. 2013. “Madagascar: Open-Source Software Project for Multidimensional Data Analysis and Reproducible Computational Experiments.” Journal of Open Research Software 1 (November): e8. https://doi.org/10.5334/jors.ag. Friedman, Jerome. 2000. “Greedy Function Approximation: A Gradient Boosting Machine.” The Annals of Statistics 29 (November). https://doi.org/10.1214/aos/1013203451. Gao, Guangliang, Zhifeng Bao, Jie Cao, A. Qin, Timos Sellis, Fellow, IEEE, and Zhiang Wu. 2019. “Location-Centered House Price Prediction: A Multi-Task Learning Approach,” January. https://arxiv.org/abs/1901.01774. Garrod, Guy D, and Kenneth G Willis. 1992. “Valuing Goods’ Characteristics: An Application of the Hedonic Price Method to Environmental Attributes.” Journal of Environmental Management 34 (1): 59–76. https://doi.org/10.1016/S0301-4797(05)80110-0. Geitner, Robert, Robby Fritzsch, Jürgen Popp, and Thomas Bocklitz. 2019. “Corr2D: Implementation of Two-Dimensional Correlation Analysis in R.” Journal of Statistical Software, Articles 90 (3): 1–33. https://doi.org/10.18637/jss.v090.i03. Gentleman, Robert, and Duncan Temple Lang. 2007. “Statistical Analyses and Reproducible Research.” Journal of Computational and Graphical Statistics 16 (1): 1–23. Geramifard, Alborz, Christoph Dann, Robert H. Klein, William Dabney, and Jonathan P. How. 2015. “RLPy: A Value-Function-Based Reinforcement Learning Framework for Education and Research.” Journal of Machine Learning Research 16 (46): 1573–8. http://jmlr.org/papers/v16/geramifard15a.html. Goodman, Steven N., Daniele Fanelli, and John P. A. Ioannidis. 2016a. “What Does Research Reproducibility Mean?” Science Translational Medicine 8 (341). https://doi.org/10.1126/scitranslmed.aaf5027. ———. 2016b. “What Does Research Reproducibility Mean?” Science Translational Medicine 8 (341): 341ps12–341ps12. https://doi.org/10.1126/scitranslmed.aaf5027. Gosiewska, Alicja, and Przemyslaw Biecek. 2020. “Lifting Interpretability-Performance Trade-off via Automated Feature Engineering.” https://arxiv.org/abs/2002.04267. Gosiewska, Alicja, Aleksandra Gacek, Piotr Lubon, and Przemyslaw Biecek. 2019. “SAFE Ml: Surrogate Assisted Feature Extraction for Model Learning.” http://arxiv.org/abs/1902.11035. Guazzelli, Alex, Michael Zeller, Wen-Ching Lin, and Graham Williams. 2009. “PMML: An Open Standard for Sharing Models.” The R Journal 1 (1): 60–65. https://doi.org/10.32614/RJ-2009-010. Gundersen, Odd Erik, and Sigbjørn Kjensmo. 2018. “State of the Art: Reproducibility in Artificial Intelligence.” https://www.aaai.org/ocs/index.php/AAAI/AAAI18/paper/view/17248. Günther, Frauke, and Stefan Fritsch. 2010. “neuralnet: Training of Neural Networks.” The R Journal 2 (1): 30–38. https://doi.org/10.32614/RJ-2010-006. H2O.ai. 2017. H2O Automl. http://docs.h2o.ai/h2o/latest-stable/h2o-docs/automl.html. Halstead, M. H. 1977. Elements of Software Science. Elsevier. Hankin, Robin. 2007. “Introducing Untb, an R Package for Simulating Ecological Drift Under the Unified Neutral Theory of Biodiversity.” Journal of Statistical Software, Articles 22 (12): 1–15. https://doi.org/10.18637/jss.v022.i12. Hastie, Trevor, and Rahul Mazumder. 2015a. “SoftImpute: Matrix Completion via Iterative Soft-Thresholded Svd.” https://CRAN.R-project.org/package=softImpute. ———. 2015b. SoftImpute: Matrix Completion via Iterative Soft-Thresholded Svd. https://CRAN.R-project.org/package=softImpute. Herbold, Steffen. 2020. “Autorank: A Python Package for Automated Ranking of Classifiers.” Journal of Open Source Software 5 (48): 2173. https://doi.org/10.21105/joss.02173. Heusser, Andrew C., Kirsten Ziman, Lucy L. W. Owen, and Jeremy R. Manning. 2018. “HyperTools: A Python Toolbox for Gaining Geometric Insights into High-Dimensional Data.” Journal of Machine Learning Research 18 (152): 1–6. http://jmlr.org/papers/v18/17-434.html. Heyman, Axel, and Dag Sommervoll. 2019. “House Prices and Relative Location.” Cities 95 (September): 102373. https://doi.org/10.1016/j.cities.2019.06.004. Holzinger, Andreas, Georg Langs, Helmut Denk, Kurt Zatloukal, and Heimo Müller. 2019. “Causability and Explainability of Artificial Intelligence in Medicine.” WIREs Data Mining and Knowledge Discovery 9 (4): e1312. https://doi.org/10.1002/widm.1312. Hothorn, Torsten, Peter Bühlmann, Thomas Kneib, Matthias Schmid, and Benjamin Hofner. 2010. “Model-Based Boosting 2.0.” Journal of Machine Learning Research 11 (71): 2109–13. http://jmlr.org/papers/v11/hothorn10a.html. Hu, Feng, and Hang Li. 2013. “A Novel Boundary Oversampling Algorithm Based on Neighborhood Rough Set Model: NRSBoundary-Smote.” Mathematical Problems in Engineering 2013 (November). https://doi.org/10.1155/2013/694809. Hughes, Nathan, Richard Morris, and Melissa Tomkins. 2020. “PyEscape: A Narrow Escape Problem Simulator Package for Python.” Journal of Open Source Software 5 (47): 2072. https://doi.org/10.21105/joss.02072. Hung, Ling-Hong, Daniel Kristiyanto, Sung Lee, and Ka Yee Yeung. 2016. “GUIdock: Using Docker Containers with a Common Graphics User Interface to Address the Reproducibility of Research.” PloS One 11 (April): e0152686. https://doi.org/10.1371/journal.pone.0152686. Jadhav, Anil. 2020. “A Novel Weighted Tpr-Tnr Measure to Assess Performance of the Classifiers.” Expert Systems with Applications 152 (March): 113391. https://doi.org/10.1016/j.eswa.2020.113391. Jas, Mainak, Titipat Achakulvisut, Aid Idrizović, Daniel Acuna, Matthew Antalek, Vinicius Marques, Tommy Odland, et al. 2020. “Pyglmnet: Python Implementation of Elastic-Net Regularized Generalized Linear Models.” Journal of Open Source Software 5 (47): 1959. https://doi.org/10.21105/joss.01959. José M. Jerez, Pedro J. García-Laencina, Ignacio Molina. 2010. “Missing Data Imputation Using Statistical and Machine Learning Methods in a Real Breast Cancer Problem.” Elsevier 50: 105–15. https://doi.org/10.1016/j.artmed.2010.05.002. Josse, Julie, and François Husson. 2016. “missMDA: A Package for Handling Missing Values in Multivariate Data Analysis.” Journal of Statistical Software 70 (1): 1–31. https://doi.org/10.18637/jss.v070.i01. “Journal Metrics - Impact, Speed and Reach.” n.d. https://www.journals.elsevier.com/international-journal-of-approximate-reasoning/news/journal-metricsimpact-speed-and-reach. Kim, Donghoh, and Hee-Seok Oh. 2009. “EMD: A Package for Empirical Mode Decomposition and Hilbert Spectrum.” The R Journal 1 (1): 40–46. https://doi.org/10.32614/RJ-2009-002. Kitzes, Justin, Daniel Turek, and Fatma Deniz. 2017. The Practice of Reproducible Research: Case Studies and Lessons from the Data-Intensive Sciences. Univ of California Press. Kossaifi, Jean, Yannis Panagakis, Anima Anandkumar, and Maja Pantic. 2019. “TensorLy: Tensor Learning in Python.” Journal of Machine Learning Research 20 (26): 1–6. http://jmlr.org/papers/v20/18-277.html. Kowarik, Alexander, and Matthias Templ. 2016a. “Imputation with the R Package VIM.” Journal of Statistical Software 74 (7): 1–16. https://doi.org/10.18637/jss.v074.i07. ———. 2016b. “Imputation with the R Package Vim.” Journal of Statistical Software 74 (7): 1–16. https://www.jstatsoft.org/article/view/v045i03. ———. 2016c. “Imputation with the R Package VIM.” Journal of Statistical Software 74 (7): 1–16. https://doi.org/10.18637/jss.v074.i07. Kuhn, Max. 2008. “Building Predictive Models in R Using the Caret Package.” Journal of Statistical Software, Articles 28 (5): 1–26. https://doi.org/10.18637/jss.v028.i05. Landau, William Michael. 2018. “The Drake R Package: A Pipeline Toolkit for Reproducibility and High-Performance Computing.” Journal of Open Source Software 3 (21). https://doi.org/10.21105/joss.00550. Lau, Matthew, Thomas F. J.-M Pasquier, and Margo Seltzer. 2020. “Rclean: A Tool for Writing Cleaner, More Transparent Code.” Journal of Open Source Software 5 (46): 1312. https://doi.org/10.21105/joss.01312. Law, Stephen. 2017. “Defining Street-Based Local Area and Measuring Its Effect on House Price Using a Hedonic Price Approach: The Case Study of Metropolitan London.” Cities 60 (February): 166–79. https://doi.org/10.1016/j.cities.2016.08.008. LeVeque, Randall. 2009. “Python Tools for Reproducible Research on Hyperbolic Problems.” Computing in Science &amp; Engineering 11 (January): 19–27. https://doi.org/10.1109/MCSE.2009.13. Li, Xingguo, Tuo Zhao, Xiaoming Yuan, and Han Liu. 2015. “The Flare Package for High Dimensional Linear Regression and Precision Matrix Estimation in R.” Journal of Machine Learning Research 16 (18): 553–57. http://jmlr.org/papers/v16/li15a.html. Lipton, Zachary Chase. 2016. “The Mythos of Model Interpretability.” CoRR abs/1606.03490. http://arxiv.org/abs/1606.03490. Little, R. J. A., and D. B. Rubin. 2002. Statistical Analysis with Missing Data. Wiley Series in Probability and Mathematical Statistics. Probability and Mathematical Statistics. Wiley. http://books.google.com/books?id=aYPwAAAAMAAJ. Liu, Rui, and Lu Liu. 2019. “Predicting housing price in China based on long short-term memory incorporating modified genetic algorithm.” Soft Computing, 1–10. https://doi.org/10.1007/s00500-018-03739-w. Löfstedt, Tommy, Vincent Guillemot, Vincent Frouin, Edouard Duchesnay, and Fouad Hadj-Selem. 2018. “Simulated Data for Linear Regression with Structured and Sparse Penalties: Introducing Pylearn-Simulate.” Journal of Statistical Software, Articles 87 (3): 1–33. https://doi.org/10.18637/jss.v087.i03. Markos, Angelos, Alfonso D’Enza, and Michel van de Velden. 2019. “Beyond Tandem Analysis: Joint Dimension Reduction and Clustering in R.” Journal of Statistical Software, Articles 91 (10): 1–24. https://doi.org/10.18637/jss.v091.i10. Marwick, B. n.d. “Rrtools: Creates a Reproducible Research Compendium (2018).” Marwick, Ben. 2016. “Computational Reproducibility in Archaeological Research: Basic Principles and a Case Study of Their Implementation.” Journal of Archaeological Method and Theory 24 (2): 424–50. https://doi.org/10.1007/s10816-015-9272-9. Marwick, Ben, Carl Boettiger, and Lincoln Mullen. 2017. “Packaging Data Analytical Work Reproducibly Using R (and Friends).” The American Statistician 72 (1): 80–88. https://doi.org/10.1080/00031305.2017.1375986. Matthias Templ, Andreas Alfons, Alexander Kowarik. 2020. VIM: Visualization and Imputation of Missing Values. https://cran.r-project.org/web/packages/VIM/index.html. Matthias Templ, Peter Filzmoser, Alexander Kowarik. 2011. “Iterative Stepwise Regression Imputation Using Standard and Robust Methods.” http://file.statistik.tuwien.ac.at/filz/papers/CSDA11TKF.pdf. Matthijs Meire, Dirk Van den Poel, Michel Ballings. 2016. ImputeMissings: Impute Missing Values in a Predictive Context. https://cran.r-project.org/web/packages/imputeMissings/index.html. Mayer, Michael. 2019. “MissRanger: Fast Imputation of Missing Values.” https://CRAN.R-project.org/package=missRanger. McCabe, T. J. 1976. “A Complexity Measure.” IEEE Transactions on Software Engineering 2 (4): 308–20. McDermott, James, and Richard S. Forsyth. 2016. “Diagnosing a Disorder in a Classification Benchmark.” Pattern Recognition Letters 73: 41–43. https://doi.org/https://doi.org/10.1016/j.patrec.2016.01.004. McNutt, Marcia. 2014. “Journals Unite for Reproducibility.” Science 346 (6210): 679–79. https://doi.org/10.1126/science.aaa1724. Melo], Vinícius [Veloso de, and Wolfgang Banzhaf. 2018. “Automatic Feature Engineering for Regression Models with Machine Learning: An Evolutionary Computation and Statistics Hybrid.” Information Sciences 430-431: 287–313. https://doi.org/https://doi.org/10.1016/j.ins.2017.11.041. Mevik, Björn-Helge, and Ron Wehrens. 2007. “The Pls Package: Principal Component and Partial Least Squares Regression in R.” Journal of Statistical Software, Articles 18 (2): 1–23. https://doi.org/10.18637/jss.v018.i02. Meyer, David, Evgenia Dimitriadou, Kurt Hornik, Andreas Weingessel, and Friedrich Leisch. 2019. E1071: Misc Functions of the Department of Statistics, Probability Theory Group (Formerly: E1071), Tu Wien. https://CRAN.R-project.org/package=e1071. Mi, Xuefei, Tetsuhisa Miwa, and Torsten Hothorn. 2009. “New Numerical Algorithm for Multivariate Normal Probabilities in Package mvtnorm.” The R Journal 1 (1): 37–39. https://doi.org/10.32614/RJ-2009-001. Michel Lang, Jakob Richter, Bernd Bischl. 2020. Mlr3: Machine Learning in R - Next Generation. https://cran.r-project.org/web/packages/mlr3/index.html. Molnar, Christoph. 2019a. Interpretable Machine Learning: A Guide for Making Black Box Models Explainable. https://christophm.github.io/interpretable-ml-book/simple.html. ———. 2019b. Interpretable Machine Learning: A Guide for Making Black Box Models Explainable. https://christophm.github.io/interpretable-ml-book/simple.html. ———. 2018. “Iml: An R Package for Interpretable Machine Learning.” Journal of Open Source Software 3 (June): 786. https://doi.org/10.21105/joss.00786. Murdoch, Kumbier, Singh. 2018. “Interpretable Machine Learning: Definitions, Methods, and Applications,” 2. https://arxiv.org/pdf/1901.04592.pdf?fbclid=IwAR2frcHrhLc4iaH5-TmKKq263NVvAKHtG4uQoiVNDeLAG3QFzdje-yzZjiQ. Musil, Carol, Camille Warner, Piyanee Klainin-Yobas, and Susan Jones. 2002. “A Comparison of Imputation Techniques for Handling Missing Data.” Western Journal of Nursing Research 24 (December): 815–29. https://doi.org/10.1177/019394502762477004. Nordh, Jerker. 2017. “pyParticleEst: A Python Framework for Particle-Based Estimation Methods.” Journal of Statistical Software 78 (3). https://doi.org/10.18637/jss.v078.i03. Obadia, Yohan. 2017. “The Use of Knn for Missing Values.” https://towardsdatascience.com/the-use-of-knn-for-missing-values-cf33d935c637. Obermeyer, Ziad, and Ezekiel J. Emanuel. 2016. “Predicting the Future - Big Data, Machine Learning, and Clinical Medicine.” The New England Journal of Medicine 375 (13): 1216–9. https://doi.org/10.1056/NEJMp1606181. Özalp, Ayşe, and Halil Akinci. 2017. “The Use of Hedonic Pricing Method to Determine the Parameters Affecting Residential Real Estate Prices.” Arabian Journal of Geosciences 10 (December). https://doi.org/10.1007/s12517-017-3331-3. Pandey. 2019. “Interpretable Machine Learning: Extracting Human Understandable Insights from Any Machine Learning Model,” April. https://towardsdatascience.com/interpretable-machine-learning-1dec0f2f3e6b. Park, Byeonghwa, and Jae Bae. 2015. “Using machine learning algorithms for housing price prediction: The case of Fairfax County, Virginia housing data.” Expert Systems with Applications 42 (April). https://doi.org/10.1016/j.eswa.2014.11.040. Patil, Prasad, Roger D. Peng, and Jeffrey T. Leek. 2016. “A Statistical Definition for Reproducibility and Replicability.” Science. https://doi.org/10.1101/066803. Patricia Arroba, Marina Zapater, José L. Risco-Martín. 2015. “Enhancing Regression Models for Complex Systems Using Evolutionary Techniques for Feature Engineering.” Journal of Grid Computing 13: 409–23. https://doi.org/10.1007/s10723-014-9313-8. Peng, Roger D. 2011. “Reproducible Research in Computational Science.” Science 334 (6060): 1226–7. https://doi.org/10.1126/science.1213847. Piccolo, Stephen R., and Michael B. Frampton. 2016. “Tools and Techniques for Computational Reproducibility.” GigaScience 5 (1). https://doi.org/10.1186/s13742-016-0135-4. Pue, A. 2019. “Graph Transliterator: A Graph-Based Transliteration Tool.” Journal of Open Source Software 4 (44): 1717. https://doi.org/10.21105/joss.01717. Raff, Edward. 2020. “Quantifying Independently Reproducible Machine Learning.” https://thegradient.pub/independently-reproducible-machine-learning/. Rafiei, Mohammad H., and Hojjat Adeli. 2015. “A Novel Machine Learning Model for Estimation of Sale Prices of Real Estate Units.” Journal of Construction Engineering and Management 142 (August). https://doi.org/10.1061/(ASCE)CO.1943-7862.0001047. Randeniya, TD, Gayani Ranasinghe, and Susantha Amarawickrama. 2017. “A model to Estimate the Implicit Values of Housing Attributes by Applying the Hedonic Pricing Method.” International Journal of Built Environment and Sustainability 4 (May). https://doi.org/10.11113/ijbes.v4.n2.182. R Core Team. 2018. R: A Language and Environment for Statistical Computing. Vienna, Austria: R Foundation for Statistical Computing. https://www.R-project.org/. Rebecca R. Andridge, Roderick J. A. Little. 2010. “A Review of Hot Deck Imputation for Survey Non-Response.” https://doi.org/10.1111/j.1751-5823.2010.00103.x. “Reproducibility in Science: A Guide to enhancing reproducibility in scientific results and writing.” 2014. http://ropensci.github.io/reproducibility-guide/. Ripley, Brian. 2019. Class: Functions for Classification. https://CRAN.R-project.org/package=class. Role, François, Stanislas Morbieu, and Mohamed Nadif. 2019. “CoClust: A Python Package for Co-Clustering.” Journal of Statistical Software 88 (7). https://doi.org/10.18637/jss.v088.i07. Ronan, Tom, Shawn Anastasio, Zhijie Qi, Pedro Henrique S. Vieira Tavares, Roman Sloutsky, and Kristen M. Naegle. 2018. “OpenEnsembles: A Python Resource for Ensemble Clustering.” Journal of Machine Learning Research 19 (26): 1–6. http://jmlr.org/papers/v19/18-100.html. Rosenberg, David E., Yves Filion, Rebecca Teasley, Samuel Sandoval-Solis, Jory S. Hecht, Jakobus E. van Zyl, George F. McMahon, Jeffery S. Horsburgh, Joseph R. Kasprzyk, and David G. Tarboton. 2020. “The Next Frontier: Making Research More Reproducible.” Journal of Water Resources Planning and Management 146 (6): 01820002. https://doi.org/10.1061/(ASCE)WR.1943-5452.0001215. Rubin, DONALD B. 1976. “Inference and missing data.” Biometrika 63 (3): 581–92. https://doi.org/10.1093/biomet/63.3.581. Rubin, Donald B. 1976. “Inference and Missing Data.” https://doi.org/10.1093/biomet/63.3.581. Rudin, Cynthia. 2019. “Stop Explaining Black Box Machine Learning Models for High Stakes Decisions and Use Interpretable Models Instead.” Nature Machine Intelligence 1 (5): 206–15. https://doi.org/10.1038/s42256-019-0048-x. Sakia, R. M. 1992. “The Box-Cox Transformation Technique: A Review.” Journal of the Royal Statistical Society. Series D (the Statistician) 41 (2): 169–78. http://www.jstor.org/stable/2348250. Sayyad Shirabad, J., and T. J. Menzies. 2005. “The PROMISE Repository of Software Engineering Databases.” School of Information Technology and Engineering, University of Ottawa, Canada. http://promise.site.uottawa.ca/SERepository. Selim, Hasan. 2009. “Determinants of House Prices in Turkey: Hedonic Regression Versus Artificial Neural Network.” Expert Syst. Appl. 36 (March): 2843–52. https://doi.org/10.1016/j.eswa.2008.01.044. Sinz, Fabian, Joern-Philipp Lies, Sebastian Gerwinn, and Matthias Bethge. 2014. “Natter: A Python Natural Image Statistics Toolbox.” Journal of Statistical Software 61 (October): 1–34. https://doi.org/10.18637/jss.v061.i05. Slezak, Peter, and Iveta Waczulikova. 2011. “Reproducibility and Repeatability.” Physiological Research / Academia Scientiarum Bohemoslovaca 60 (April): 203–4; author reply 204. Soetaert, Karline, Thomas Petzoldt, and R. Woodrow Setzer. 2010. “Solving Differential Equations in R.” The R Journal 2 (2): 5–15. https://doi.org/10.32614/RJ-2010-013. Stanisic, Luka, Arnaud Legrand, and Vincent Danjean. 2015. “An Effective Git and Org-Mode Based Workflow for Reproducible Research.” SIGOPS Oper. Syst. Rev. 49 (1): 61–70. https://doi.org/10.1145/2723872.2723881. Stekhoven, Daniel J. 2013. MissForest: Nonparametric Missing Value Imputation Using Random Forest. https://CRAN.R-project.org/package=missForest. Stekhoven, Daniel J., and Peter Buehlmann. 2012a. “MissForest - Non-Parametric Missing Value Imputation for Mixed-Type Data.” Bioinformatics 28 (1): 112–18. ———. 2012b. “MissForest - Non-Parametric Missing Value Imputation for Mixed-Type Data.” Bioinformatics 28 (1): 112–18. Stodden, Victoria, David H. Bailey, Jonathan M. Borwein, Randall J. LeVeque, William J. Rider, and William Stein. 2013. “Setting the Default to Reproducible Reproducibility in Computational and Experimental Mathematics.” In. Stodden, Victoria, Marcia McNutt, David H. Bailey, Ewa Deelman, Yolanda Gil, Brooks Hanson, Michael A. Heroux, John P. A. Ioannidis, and Michela Taufer. 2016. “Enhancing Reproducibility for Computational Methods.” Science 354 (6317): 1240–1. https://doi.org/10.1126/science.aah6168. Stodden, Victoria, Jennifer Seiler, and Zhaokun Ma. 2018. “An Empirical Analysis of Journal Policy Effectiveness for Computational Reproducibility.” Proceedings of the National Academy of Sciences 115 (11): 2584–9. https://doi.org/10.1073/pnas.1708290115. Strobl, Carolin, Torsten Hothorn, and Achim Zeileis. 2009. “Party on!” The R Journal 1 (2): 14–17. https://doi.org/10.32614/RJ-2009-013. Su, Xiaoyuan, Taghi Khoshgoftaar, and Russ Greiner. 2008. “Using Imputation Techniques to Help Learn Accurate Classifiers.” Proceedings - International Conference on Tools with Artificial Intelligence, ICTAI 1 (December): 437–44. https://doi.org/10.1109/ICTAI.2008.60. Templ, Matthias, Alexander Kowarik, and Andreas Alfons. 2020. VIM: Visualization and Imputation of Missing Values. https://CRAN.R-project.org/package=VIM. Therneau, Terry, and Beth Atkinson. 2019a. Rpart: Recursive Partitioning and Regression Trees. https://CRAN.R-project.org/package=rpart. ———. 2019b. Rpart: Recursive Partitioning and Regression Trees. https://CRAN.R-project.org/package=rpart. Thomas, Kluyver, Ragan-Kelley Benjamin, P&amp;eacute;rez Fernando, Granger Brian, Bussonnier Matthias, Frederic Jonathan, Kelley Kyle, et al. 2016. “Jupyter Notebooks &amp;Ndash; a Publishing Format for Reproducible Computational Workflows.” Stand Alone 0 (Positioning and Power in Academic Publishing: Players, Agents and Agendas): 87–90. https://doi.org/10.3233/978-1-61499-649-1-87. Titz, Johannes. 2020. “Mimosa: A Modern Graphical User Interface for 2-Level Mixed Models.” Journal of Open Source Software 5 (49): 2116. https://doi.org/10.21105/joss.02116. Trevor Hastie, Rahul Mazumder. 2015. SoftImpute: Matrix Completion via Iterative Soft-Thresholded Svd. https://cran.r-project.org/web/packages/softImpute/index.html. van Buuren, Stef, and Karin Groothuis-Oudshoorn. 2011a. “mice: Multivariate Imputation by Chained Equations in R.” Journal of Statistical Software 45 (3): 1–67. https://www.jstatsoft.org/v45/i03/. ———. 2011b. “mice: Multivariate Imputation by Chained Equations in R.” Journal of Statistical Software 45 (3): 1–67. https://www.jstatsoft.org/v45/i03/. Vandewalle, Patrick, Jelena Kovacevic, and Martin Vetterli. 2009. “Reproducible Research in Signal Processing.” IEEE Signal Processing Magazine 26 (3): 37–47. Vanschoren, Joaquin, Jan N. van Rijn, Bernd Bischl, and Luis Torgo. 2013. “OpenML: Networked Science in Machine Learning.” SIGKDD Explorations 15 (2): 49–60. https://doi.org/10.1145/2641190.2641198. Velez, White, D. R., and J. H Moore. 2007. “A Balanced Accuracy Function for Epistasis Modeling in Imbalanceddatasets Using Multifactor Dimensionality Reduction.” Genetic Epidemiology, no. 31: 306–15. https://doi.org/10.1002/gepi.20211. Wilhelm, Stefan, and B. G. Manjunath. 2010. “tmvtnorm: A Package for the Truncated Multivariate Normal Distribution.” The R Journal 2 (1): 25–29. https://doi.org/10.32614/RJ-2010-005. Wright, Marvin N., Stefan Wager, and Philipp Probst. 2020. Ranger: A Fast Implementation of Random Forests. https://CRAN.R-project.org/package=ranger. Wright, Marvin N., and Andreas Ziegler. 2017. “ranger: A Fast Implementation of Random Forests for High Dimensional Data in C++ and R.” Journal of Statistical Software 77 (1): 1–17. https://doi.org/10.18637/jss.v077.i01. Xia, Xiao-Qin, Michael McClelland, and Yipeng Wang. 2010. “PypeR, a Python Package for Using R in Python.” Journal of Statistical Software, Code Snippets 35 (2): 1–8. http://www.jstatsoft.org/v35/c02. Yuan, Lester. 2007. “Maximum Likelihood Method for Predicting Environmental Conditions from Assemblage Composition: The R Package Bio.infer.” Journal of Statistical Software, Articles 22 (3): 1–20. https://doi.org/10.18637/jss.v022.i03. Zhao, Tuo, Han Liu, Kathryn Roeder, John Lafferty, and Larry Wasserman. 2012. “The Huge Package for High-Dimensional Undirected Graph Estimation in R.” Journal of Machine Learning Research 13 (37): 1059–62. http://jmlr.org/papers/v13/zhao12a.html. Zwicker, David. 2020. “Py-Pde: A Python Package for Solving Partial Differential Equations.” Journal of Open Source Software 5 (48): 2158. https://doi.org/10.21105/joss.02158. "]
]
