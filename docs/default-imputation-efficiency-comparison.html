<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>2.1 Default imputation efficiency comparison | ML Case Studies</title>
  <meta name="description" content="Case studies for reproducibility, imputation, and interpretability" />
  <meta name="generator" content="bookdown 0.20 and GitBook 2.6.7" />

  <meta property="og:title" content="2.1 Default imputation efficiency comparison | ML Case Studies" />
  <meta property="og:type" content="book" />
  
  <meta property="og:image" content="images/cover.png" />
  <meta property="og:description" content="Case studies for reproducibility, imputation, and interpretability" />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="2.1 Default imputation efficiency comparison | ML Case Studies" />
  
  <meta name="twitter:description" content="Case studies for reproducibility, imputation, and interpretability" />
  <meta name="twitter:image" content="images/cover.png" />



<meta name="date" content="2020-07-02" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="imputation.html"/>
<link rel="next" href="the-hajada-imputation-test.html"/>
<script src="libs/header-attrs/header-attrs.js"></script>
<script src="libs/jquery/jquery.min.js"></script>
<link href="libs/gitbook/css/style.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-clipboard.css" rel="stylesheet" />









<script src="libs/accessible-code-block/empty-anchor.js"></script>
<script src="libs/kePrint/kePrint.js"></script>


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<link rel="stylesheet" href="css/style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./"><h3>ML Case Studies</h3></a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a>
<ul>
<li class="chapter" data-level="" data-path="technical-setup.html"><a href="technical-setup.html"><i class="fa fa-check"></i>Technical Setup</a></li>
</ul></li>
<li class="chapter" data-level="1" data-path="reproducibility.html"><a href="reproducibility.html"><i class="fa fa-check"></i><b>1</b> Reproducibility of scientific papers</a>
<ul>
<li class="chapter" data-level="1.1" data-path="how-to-measure-reproducibility-classification-of-problems-with-reproducing-scientific-papers.html"><a href="how-to-measure-reproducibility-classification-of-problems-with-reproducing-scientific-papers.html"><i class="fa fa-check"></i><b>1.1</b> How to measure reproducibility? Classification of problems with reproducing scientific papers</a>
<ul>
<li class="chapter" data-level="1.1.1" data-path="how-to-measure-reproducibility-classification-of-problems-with-reproducing-scientific-papers.html"><a href="how-to-measure-reproducibility-classification-of-problems-with-reproducing-scientific-papers.html#abstract"><i class="fa fa-check"></i><b>1.1.1</b> Abstract</a></li>
<li class="chapter" data-level="1.1.2" data-path="how-to-measure-reproducibility-classification-of-problems-with-reproducing-scientific-papers.html"><a href="how-to-measure-reproducibility-classification-of-problems-with-reproducing-scientific-papers.html#introduction"><i class="fa fa-check"></i><b>1.1.2</b> Introduction</a></li>
<li class="chapter" data-level="1.1.3" data-path="how-to-measure-reproducibility-classification-of-problems-with-reproducing-scientific-papers.html"><a href="how-to-measure-reproducibility-classification-of-problems-with-reproducing-scientific-papers.html#related-work"><i class="fa fa-check"></i><b>1.1.3</b> Related Work</a></li>
<li class="chapter" data-level="1.1.4" data-path="how-to-measure-reproducibility-classification-of-problems-with-reproducing-scientific-papers.html"><a href="how-to-measure-reproducibility-classification-of-problems-with-reproducing-scientific-papers.html#methodology"><i class="fa fa-check"></i><b>1.1.4</b> Methodology</a></li>
<li class="chapter" data-level="1.1.5" data-path="how-to-measure-reproducibility-classification-of-problems-with-reproducing-scientific-papers.html"><a href="how-to-measure-reproducibility-classification-of-problems-with-reproducing-scientific-papers.html#results"><i class="fa fa-check"></i><b>1.1.5</b> Results</a></li>
<li class="chapter" data-level="1.1.6" data-path="how-to-measure-reproducibility-classification-of-problems-with-reproducing-scientific-papers.html"><a href="how-to-measure-reproducibility-classification-of-problems-with-reproducing-scientific-papers.html#summary-and-conclusions"><i class="fa fa-check"></i><b>1.1.6</b> Summary and conclusions</a></li>
</ul></li>
<li class="chapter" data-level="1.2" data-path="aging-articles-how-time-affects-reproducibility-of-scientific-papers.html"><a href="aging-articles-how-time-affects-reproducibility-of-scientific-papers.html"><i class="fa fa-check"></i><b>1.2</b> Aging articles. How time affects reproducibility of scientific papers?</a>
<ul>
<li class="chapter" data-level="1.2.1" data-path="how-to-measure-reproducibility-classification-of-problems-with-reproducing-scientific-papers.html"><a href="how-to-measure-reproducibility-classification-of-problems-with-reproducing-scientific-papers.html#abstract"><i class="fa fa-check"></i><b>1.2.1</b> Abstract</a></li>
<li class="chapter" data-level="1.2.2" data-path="how-to-measure-reproducibility-classification-of-problems-with-reproducing-scientific-papers.html"><a href="how-to-measure-reproducibility-classification-of-problems-with-reproducing-scientific-papers.html#introduction"><i class="fa fa-check"></i><b>1.2.2</b> Introduction</a></li>
<li class="chapter" data-level="1.2.3" data-path="aging-articles-how-time-affects-reproducibility-of-scientific-papers.html"><a href="aging-articles-how-time-affects-reproducibility-of-scientific-papers.html#codeextractor-package"><i class="fa fa-check"></i><b>1.2.3</b> CodeExtractoR package</a></li>
<li class="chapter" data-level="1.2.4" data-path="how-to-measure-reproducibility-classification-of-problems-with-reproducing-scientific-papers.html"><a href="how-to-measure-reproducibility-classification-of-problems-with-reproducing-scientific-papers.html#methodology"><i class="fa fa-check"></i><b>1.2.4</b> Methodology</a></li>
<li class="chapter" data-level="1.2.5" data-path="how-to-measure-reproducibility-classification-of-problems-with-reproducing-scientific-papers.html"><a href="how-to-measure-reproducibility-classification-of-problems-with-reproducing-scientific-papers.html#results"><i class="fa fa-check"></i><b>1.2.5</b> Results</a></li>
<li class="chapter" data-level="1.2.6" data-path="how-to-measure-reproducibility-classification-of-problems-with-reproducing-scientific-papers.html"><a href="how-to-measure-reproducibility-classification-of-problems-with-reproducing-scientific-papers.html#summary-and-conclusions"><i class="fa fa-check"></i><b>1.2.6</b> Summary and conclusions</a></li>
</ul></li>
<li class="chapter" data-level="1.3" data-path="ways-to-reproduce-articles-in-terms-of-release-date-and-magazine.html"><a href="ways-to-reproduce-articles-in-terms-of-release-date-and-magazine.html"><i class="fa fa-check"></i><b>1.3</b> Ways to reproduce articles in terms of release date and magazine</a>
<ul>
<li class="chapter" data-level="1.3.1" data-path="how-to-measure-reproducibility-classification-of-problems-with-reproducing-scientific-papers.html"><a href="how-to-measure-reproducibility-classification-of-problems-with-reproducing-scientific-papers.html#abstract"><i class="fa fa-check"></i><b>1.3.1</b> Abstract</a></li>
<li class="chapter" data-level="1.3.2" data-path="how-to-measure-reproducibility-classification-of-problems-with-reproducing-scientific-papers.html"><a href="how-to-measure-reproducibility-classification-of-problems-with-reproducing-scientific-papers.html#methodology"><i class="fa fa-check"></i><b>1.3.2</b> Methodology</a></li>
<li class="chapter" data-level="1.3.3" data-path="how-to-measure-reproducibility-classification-of-problems-with-reproducing-scientific-papers.html"><a href="how-to-measure-reproducibility-classification-of-problems-with-reproducing-scientific-papers.html#results"><i class="fa fa-check"></i><b>1.3.3</b> Results</a></li>
</ul></li>
<li class="chapter" data-level="1.4" data-path="reproducibility-of-outdated-articles-about-up-to-date-r-packages.html"><a href="reproducibility-of-outdated-articles-about-up-to-date-r-packages.html"><i class="fa fa-check"></i><b>1.4</b> Reproducibility of outdated articles about up-to-date R packages</a>
<ul>
<li class="chapter" data-level="1.4.1" data-path="how-to-measure-reproducibility-classification-of-problems-with-reproducing-scientific-papers.html"><a href="how-to-measure-reproducibility-classification-of-problems-with-reproducing-scientific-papers.html#abstract"><i class="fa fa-check"></i><b>1.4.1</b> Abstract</a></li>
<li class="chapter" data-level="1.4.2" data-path="reproducibility-of-outdated-articles-about-up-to-date-r-packages.html"><a href="reproducibility-of-outdated-articles-about-up-to-date-r-packages.html#introduction-and-motivation"><i class="fa fa-check"></i><b>1.4.2</b> Introduction and Motivation</a></li>
<li class="chapter" data-level="1.4.3" data-path="how-to-measure-reproducibility-classification-of-problems-with-reproducing-scientific-papers.html"><a href="how-to-measure-reproducibility-classification-of-problems-with-reproducing-scientific-papers.html#related-work"><i class="fa fa-check"></i><b>1.4.3</b> Related Work</a></li>
<li class="chapter" data-level="1.4.4" data-path="how-to-measure-reproducibility-classification-of-problems-with-reproducing-scientific-papers.html"><a href="how-to-measure-reproducibility-classification-of-problems-with-reproducing-scientific-papers.html#methodology"><i class="fa fa-check"></i><b>1.4.4</b> Methodology</a></li>
<li class="chapter" data-level="1.4.5" data-path="how-to-measure-reproducibility-classification-of-problems-with-reproducing-scientific-papers.html"><a href="how-to-measure-reproducibility-classification-of-problems-with-reproducing-scientific-papers.html#results"><i class="fa fa-check"></i><b>1.4.5</b> Results</a></li>
<li class="chapter" data-level="1.4.6" data-path="how-to-measure-reproducibility-classification-of-problems-with-reproducing-scientific-papers.html"><a href="how-to-measure-reproducibility-classification-of-problems-with-reproducing-scientific-papers.html#summary-and-conclusions"><i class="fa fa-check"></i><b>1.4.6</b> Summary and conclusions</a></li>
</ul></li>
<li class="chapter" data-level="1.5" data-path="correlation-between-reproducibility-of-research-papers-and-their-objective.html"><a href="correlation-between-reproducibility-of-research-papers-and-their-objective.html"><i class="fa fa-check"></i><b>1.5</b> Correlation between reproducibility of research papers and their objective</a>
<ul>
<li class="chapter" data-level="1.5.1" data-path="how-to-measure-reproducibility-classification-of-problems-with-reproducing-scientific-papers.html"><a href="how-to-measure-reproducibility-classification-of-problems-with-reproducing-scientific-papers.html#abstract"><i class="fa fa-check"></i><b>1.5.1</b> Abstract</a></li>
<li class="chapter" data-level="1.5.2" data-path="reproducibility-of-outdated-articles-about-up-to-date-r-packages.html"><a href="reproducibility-of-outdated-articles-about-up-to-date-r-packages.html#introduction-and-motivation"><i class="fa fa-check"></i><b>1.5.2</b> Introduction and Motivation</a></li>
<li class="chapter" data-level="1.5.3" data-path="how-to-measure-reproducibility-classification-of-problems-with-reproducing-scientific-papers.html"><a href="how-to-measure-reproducibility-classification-of-problems-with-reproducing-scientific-papers.html#methodology"><i class="fa fa-check"></i><b>1.5.3</b> Methodology</a></li>
<li class="chapter" data-level="1.5.4" data-path="how-to-measure-reproducibility-classification-of-problems-with-reproducing-scientific-papers.html"><a href="how-to-measure-reproducibility-classification-of-problems-with-reproducing-scientific-papers.html#results"><i class="fa fa-check"></i><b>1.5.4</b> Results</a></li>
<li class="chapter" data-level="1.5.5" data-path="correlation-between-reproducibility-of-research-papers-and-their-objective.html"><a href="correlation-between-reproducibility-of-research-papers-and-their-objective.html#summary-conclusions-and-encouragement"><i class="fa fa-check"></i><b>1.5.5</b> Summary, conclusions and encouragement</a></li>
</ul></li>
<li class="chapter" data-level="1.6" data-path="how-active-development-affects-reproducibility.html"><a href="how-active-development-affects-reproducibility.html"><i class="fa fa-check"></i><b>1.6</b> How active development affects reproducibility</a>
<ul>
<li class="chapter" data-level="1.6.1" data-path="how-to-measure-reproducibility-classification-of-problems-with-reproducing-scientific-papers.html"><a href="how-to-measure-reproducibility-classification-of-problems-with-reproducing-scientific-papers.html#abstract"><i class="fa fa-check"></i><b>1.6.1</b> Abstract</a></li>
<li class="chapter" data-level="1.6.2" data-path="reproducibility-of-outdated-articles-about-up-to-date-r-packages.html"><a href="reproducibility-of-outdated-articles-about-up-to-date-r-packages.html#introduction-and-motivation"><i class="fa fa-check"></i><b>1.6.2</b> Introduction and Motivation</a></li>
<li class="chapter" data-level="1.6.3" data-path="how-to-measure-reproducibility-classification-of-problems-with-reproducing-scientific-papers.html"><a href="how-to-measure-reproducibility-classification-of-problems-with-reproducing-scientific-papers.html#methodology"><i class="fa fa-check"></i><b>1.6.3</b> Methodology</a></li>
<li class="chapter" data-level="1.6.4" data-path="how-to-measure-reproducibility-classification-of-problems-with-reproducing-scientific-papers.html"><a href="how-to-measure-reproducibility-classification-of-problems-with-reproducing-scientific-papers.html#results"><i class="fa fa-check"></i><b>1.6.4</b> Results</a></li>
<li class="chapter" data-level="1.6.5" data-path="how-to-measure-reproducibility-classification-of-problems-with-reproducing-scientific-papers.html"><a href="how-to-measure-reproducibility-classification-of-problems-with-reproducing-scientific-papers.html#summary-and-conclusions"><i class="fa fa-check"></i><b>1.6.5</b> Summary and conclusions</a></li>
</ul></li>
<li class="chapter" data-level="1.7" data-path="reproducibility-differences-of-articles-published-in-various-journals-and-using-r-or-python-language.html"><a href="reproducibility-differences-of-articles-published-in-various-journals-and-using-r-or-python-language.html"><i class="fa fa-check"></i><b>1.7</b> Reproducibility differences of articles published in various journals and using R or Python language</a>
<ul>
<li class="chapter" data-level="1.7.1" data-path="how-to-measure-reproducibility-classification-of-problems-with-reproducing-scientific-papers.html"><a href="how-to-measure-reproducibility-classification-of-problems-with-reproducing-scientific-papers.html#abstract"><i class="fa fa-check"></i><b>1.7.1</b> Abstract</a></li>
<li class="chapter" data-level="1.7.2" data-path="reproducibility-of-outdated-articles-about-up-to-date-r-packages.html"><a href="reproducibility-of-outdated-articles-about-up-to-date-r-packages.html#introduction-and-motivation"><i class="fa fa-check"></i><b>1.7.2</b> Introduction and Motivation</a></li>
<li class="chapter" data-level="1.7.3" data-path="how-to-measure-reproducibility-classification-of-problems-with-reproducing-scientific-papers.html"><a href="how-to-measure-reproducibility-classification-of-problems-with-reproducing-scientific-papers.html#methodology"><i class="fa fa-check"></i><b>1.7.3</b> Methodology</a></li>
<li class="chapter" data-level="1.7.4" data-path="how-to-measure-reproducibility-classification-of-problems-with-reproducing-scientific-papers.html"><a href="how-to-measure-reproducibility-classification-of-problems-with-reproducing-scientific-papers.html#results"><i class="fa fa-check"></i><b>1.7.4</b> Results</a></li>
<li class="chapter" data-level="1.7.5" data-path="how-to-measure-reproducibility-classification-of-problems-with-reproducing-scientific-papers.html"><a href="how-to-measure-reproducibility-classification-of-problems-with-reproducing-scientific-papers.html#summary-and-conclusions"><i class="fa fa-check"></i><b>1.7.5</b> Summary and conclusions</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="2" data-path="imputation.html"><a href="imputation.html"><i class="fa fa-check"></i><b>2</b> Imputation</a>
<ul>
<li class="chapter" data-level="2.1" data-path="default-imputation-efficiency-comparison.html"><a href="default-imputation-efficiency-comparison.html"><i class="fa fa-check"></i><b>2.1</b> Default imputation efficiency comparison</a>
<ul>
<li class="chapter" data-level="2.1.1" data-path="how-to-measure-reproducibility-classification-of-problems-with-reproducing-scientific-papers.html"><a href="how-to-measure-reproducibility-classification-of-problems-with-reproducing-scientific-papers.html#abstract"><i class="fa fa-check"></i><b>2.1.1</b> Abstract</a></li>
<li class="chapter" data-level="2.1.2" data-path="reproducibility-of-outdated-articles-about-up-to-date-r-packages.html"><a href="reproducibility-of-outdated-articles-about-up-to-date-r-packages.html#introduction-and-motivation"><i class="fa fa-check"></i><b>2.1.2</b> Introduction and Motivation</a></li>
<li class="chapter" data-level="2.1.3" data-path="how-to-measure-reproducibility-classification-of-problems-with-reproducing-scientific-papers.html"><a href="how-to-measure-reproducibility-classification-of-problems-with-reproducing-scientific-papers.html#related-work"><i class="fa fa-check"></i><b>2.1.3</b> Related Work</a></li>
<li class="chapter" data-level="2.1.4" data-path="how-to-measure-reproducibility-classification-of-problems-with-reproducing-scientific-papers.html"><a href="how-to-measure-reproducibility-classification-of-problems-with-reproducing-scientific-papers.html#methodology"><i class="fa fa-check"></i><b>2.1.4</b> Methodology</a></li>
<li class="chapter" data-level="2.1.5" data-path="how-to-measure-reproducibility-classification-of-problems-with-reproducing-scientific-papers.html"><a href="how-to-measure-reproducibility-classification-of-problems-with-reproducing-scientific-papers.html#results"><i class="fa fa-check"></i><b>2.1.5</b> Results</a></li>
<li class="chapter" data-level="2.1.6" data-path="how-to-measure-reproducibility-classification-of-problems-with-reproducing-scientific-papers.html"><a href="how-to-measure-reproducibility-classification-of-problems-with-reproducing-scientific-papers.html#summary-and-conclusions"><i class="fa fa-check"></i><b>2.1.6</b> Summary and conclusions</a></li>
</ul></li>
<li class="chapter" data-level="2.2" data-path="the-hajada-imputation-test.html"><a href="the-hajada-imputation-test.html"><i class="fa fa-check"></i><b>2.2</b> The Hajada Imputation Test</a>
<ul>
<li class="chapter" data-level="2.2.1" data-path="how-to-measure-reproducibility-classification-of-problems-with-reproducing-scientific-papers.html"><a href="how-to-measure-reproducibility-classification-of-problems-with-reproducing-scientific-papers.html#abstract"><i class="fa fa-check"></i><b>2.2.1</b> Abstract</a></li>
<li class="chapter" data-level="2.2.2" data-path="reproducibility-of-outdated-articles-about-up-to-date-r-packages.html"><a href="reproducibility-of-outdated-articles-about-up-to-date-r-packages.html#introduction-and-motivation"><i class="fa fa-check"></i><b>2.2.2</b> Introduction and Motivation</a></li>
<li class="chapter" data-level="2.2.3" data-path="how-to-measure-reproducibility-classification-of-problems-with-reproducing-scientific-papers.html"><a href="how-to-measure-reproducibility-classification-of-problems-with-reproducing-scientific-papers.html#methodology"><i class="fa fa-check"></i><b>2.2.3</b> Methodology</a></li>
<li class="chapter" data-level="2.2.4" data-path="how-to-measure-reproducibility-classification-of-problems-with-reproducing-scientific-papers.html"><a href="how-to-measure-reproducibility-classification-of-problems-with-reproducing-scientific-papers.html#results"><i class="fa fa-check"></i><b>2.2.4</b> Results</a></li>
<li class="chapter" data-level="2.2.5" data-path="the-hajada-imputation-test.html"><a href="the-hajada-imputation-test.html#summary"><i class="fa fa-check"></i><b>2.2.5</b> Summary</a></li>
<li class="chapter" data-level="2.2.6" data-path="the-hajada-imputation-test.html"><a href="the-hajada-imputation-test.html#conclusions"><i class="fa fa-check"></i><b>2.2.6</b> Conclusions</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="comparison-of-performance-of-data-imputation-methods-in-the-context-of-their-impact-on-the-prediction-efficiency-of-classification-algorithms.html"><a href="comparison-of-performance-of-data-imputation-methods-in-the-context-of-their-impact-on-the-prediction-efficiency-of-classification-algorithms.html"><i class="fa fa-check"></i><b>2.3</b> Comparison of performance of data imputation methods in the context of their impact on the prediction efficiency of classification algorithms</a>
<ul>
<li class="chapter" data-level="2.3.1" data-path="how-to-measure-reproducibility-classification-of-problems-with-reproducing-scientific-papers.html"><a href="how-to-measure-reproducibility-classification-of-problems-with-reproducing-scientific-papers.html#abstract"><i class="fa fa-check"></i><b>2.3.1</b> Abstract</a></li>
<li class="chapter" data-level="2.3.2" data-path="reproducibility-of-outdated-articles-about-up-to-date-r-packages.html"><a href="reproducibility-of-outdated-articles-about-up-to-date-r-packages.html#introduction-and-motivation"><i class="fa fa-check"></i><b>2.3.2</b> Introduction and motivation</a></li>
<li class="chapter" data-level="2.3.3" data-path="how-to-measure-reproducibility-classification-of-problems-with-reproducing-scientific-papers.html"><a href="how-to-measure-reproducibility-classification-of-problems-with-reproducing-scientific-papers.html#methodology"><i class="fa fa-check"></i><b>2.3.3</b> Methodology</a></li>
<li class="chapter" data-level="2.3.4" data-path="how-to-measure-reproducibility-classification-of-problems-with-reproducing-scientific-papers.html"><a href="how-to-measure-reproducibility-classification-of-problems-with-reproducing-scientific-papers.html#results"><i class="fa fa-check"></i><b>2.3.4</b> Results</a></li>
<li class="chapter" data-level="2.3.5" data-path="how-to-measure-reproducibility-classification-of-problems-with-reproducing-scientific-papers.html"><a href="how-to-measure-reproducibility-classification-of-problems-with-reproducing-scientific-papers.html#summary-and-conclusions"><i class="fa fa-check"></i><b>2.3.5</b> Summary and conclusions</a></li>
</ul></li>
<li class="chapter" data-level="2.4" data-path="various-data-imputation-techniques-in-r.html"><a href="various-data-imputation-techniques-in-r.html"><i class="fa fa-check"></i><b>2.4</b> Various data imputation techniques in R</a>
<ul>
<li class="chapter" data-level="2.4.1" data-path="how-to-measure-reproducibility-classification-of-problems-with-reproducing-scientific-papers.html"><a href="how-to-measure-reproducibility-classification-of-problems-with-reproducing-scientific-papers.html#abstract"><i class="fa fa-check"></i><b>2.4.1</b> Abstract</a></li>
<li class="chapter" data-level="2.4.2" data-path="reproducibility-of-outdated-articles-about-up-to-date-r-packages.html"><a href="reproducibility-of-outdated-articles-about-up-to-date-r-packages.html#introduction-and-motivation"><i class="fa fa-check"></i><b>2.4.2</b> Introduction and motivation</a></li>
<li class="chapter" data-level="2.4.3" data-path="how-to-measure-reproducibility-classification-of-problems-with-reproducing-scientific-papers.html"><a href="how-to-measure-reproducibility-classification-of-problems-with-reproducing-scientific-papers.html#methodology"><i class="fa fa-check"></i><b>2.4.3</b> Methodology</a></li>
<li class="chapter" data-level="2.4.4" data-path="how-to-measure-reproducibility-classification-of-problems-with-reproducing-scientific-papers.html"><a href="how-to-measure-reproducibility-classification-of-problems-with-reproducing-scientific-papers.html#results"><i class="fa fa-check"></i><b>2.4.4</b> Results</a></li>
<li class="chapter" data-level="2.4.5" data-path="how-to-measure-reproducibility-classification-of-problems-with-reproducing-scientific-papers.html"><a href="how-to-measure-reproducibility-classification-of-problems-with-reproducing-scientific-papers.html#summary-and-conclusions"><i class="fa fa-check"></i><b>2.4.5</b> Summary and conclusions</a></li>
</ul></li>
<li class="chapter" data-level="2.5" data-path="imputation-techniques-comparison-in-r-programming-language.html"><a href="imputation-techniques-comparison-in-r-programming-language.html"><i class="fa fa-check"></i><b>2.5</b> Imputation techniques’ comparison in R programming language</a>
<ul>
<li class="chapter" data-level="2.5.1" data-path="how-to-measure-reproducibility-classification-of-problems-with-reproducing-scientific-papers.html"><a href="how-to-measure-reproducibility-classification-of-problems-with-reproducing-scientific-papers.html#abstract"><i class="fa fa-check"></i><b>2.5.1</b> Abstract</a></li>
<li class="chapter" data-level="2.5.2" data-path="imputation-techniques-comparison-in-r-programming-language.html"><a href="imputation-techniques-comparison-in-r-programming-language.html#introduction-motivation"><i class="fa fa-check"></i><b>2.5.2</b> Introduction &amp; Motivation</a></li>
<li class="chapter" data-level="2.5.3" data-path="how-to-measure-reproducibility-classification-of-problems-with-reproducing-scientific-papers.html"><a href="how-to-measure-reproducibility-classification-of-problems-with-reproducing-scientific-papers.html#methodology"><i class="fa fa-check"></i><b>2.5.3</b> Methodology</a></li>
<li class="chapter" data-level="2.5.4" data-path="how-to-measure-reproducibility-classification-of-problems-with-reproducing-scientific-papers.html"><a href="how-to-measure-reproducibility-classification-of-problems-with-reproducing-scientific-papers.html#results"><i class="fa fa-check"></i><b>2.5.4</b> Results</a></li>
<li class="chapter" data-level="2.5.5" data-path="the-hajada-imputation-test.html"><a href="the-hajada-imputation-test.html#conclusions"><i class="fa fa-check"></i><b>2.5.5</b> Conclusions</a></li>
</ul></li>
<li class="chapter" data-level="2.6" data-path="how-imputation-techniques-interact-with-machine-learning-algorithms.html"><a href="how-imputation-techniques-interact-with-machine-learning-algorithms.html"><i class="fa fa-check"></i><b>2.6</b> How imputation techniques interact with machine learning algorithms</a>
<ul>
<li class="chapter" data-level="2.6.1" data-path="how-to-measure-reproducibility-classification-of-problems-with-reproducing-scientific-papers.html"><a href="how-to-measure-reproducibility-classification-of-problems-with-reproducing-scientific-papers.html#abstract"><i class="fa fa-check"></i><b>2.6.1</b> Abstract</a></li>
<li class="chapter" data-level="2.6.2" data-path="reproducibility-of-outdated-articles-about-up-to-date-r-packages.html"><a href="reproducibility-of-outdated-articles-about-up-to-date-r-packages.html#introduction-and-motivation"><i class="fa fa-check"></i><b>2.6.2</b> Introduction and Motivation</a></li>
<li class="chapter" data-level="2.6.3" data-path="how-to-measure-reproducibility-classification-of-problems-with-reproducing-scientific-papers.html"><a href="how-to-measure-reproducibility-classification-of-problems-with-reproducing-scientific-papers.html#methodology"><i class="fa fa-check"></i><b>2.6.3</b> Methodology</a></li>
<li class="chapter" data-level="2.6.4" data-path="how-to-measure-reproducibility-classification-of-problems-with-reproducing-scientific-papers.html"><a href="how-to-measure-reproducibility-classification-of-problems-with-reproducing-scientific-papers.html#results"><i class="fa fa-check"></i><b>2.6.4</b> Results</a></li>
<li class="chapter" data-level="2.6.5" data-path="how-to-measure-reproducibility-classification-of-problems-with-reproducing-scientific-papers.html"><a href="how-to-measure-reproducibility-classification-of-problems-with-reproducing-scientific-papers.html#summary-and-conclusions"><i class="fa fa-check"></i><b>2.6.5</b> Summary and conclusions</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="3" data-path="interpretability.html"><a href="interpretability.html"><i class="fa fa-check"></i><b>3</b> Interpretability</a>
<ul>
<li class="chapter" data-level="3.1" data-path="building-an-explainable-model-for-ordinal-classification-on-eucalyptus-dataset-meeting-black-box-model-performance-levels-.html"><a href="building-an-explainable-model-for-ordinal-classification-on-eucalyptus-dataset-meeting-black-box-model-performance-levels-.html"><i class="fa fa-check"></i><b>3.1</b> Building an explainable model for ordinal classification on Eucalyptus dataset. Meeting black box model performance levels.</a>
<ul>
<li class="chapter" data-level="3.1.1" data-path="how-to-measure-reproducibility-classification-of-problems-with-reproducing-scientific-papers.html"><a href="how-to-measure-reproducibility-classification-of-problems-with-reproducing-scientific-papers.html#abstract"><i class="fa fa-check"></i><b>3.1.1</b> Abstract</a></li>
<li class="chapter" data-level="3.1.2" data-path="reproducibility-of-outdated-articles-about-up-to-date-r-packages.html"><a href="reproducibility-of-outdated-articles-about-up-to-date-r-packages.html#introduction-and-motivation"><i class="fa fa-check"></i><b>3.1.2</b> Introduction and Motivation</a></li>
<li class="chapter" data-level="3.1.3" data-path="how-to-measure-reproducibility-classification-of-problems-with-reproducing-scientific-papers.html"><a href="how-to-measure-reproducibility-classification-of-problems-with-reproducing-scientific-papers.html#related-work"><i class="fa fa-check"></i><b>3.1.3</b> Related Work</a></li>
<li class="chapter" data-level="3.1.4" data-path="how-to-measure-reproducibility-classification-of-problems-with-reproducing-scientific-papers.html"><a href="how-to-measure-reproducibility-classification-of-problems-with-reproducing-scientific-papers.html#methodology"><i class="fa fa-check"></i><b>3.1.4</b> Methodology</a></li>
<li class="chapter" data-level="3.1.5" data-path="how-to-measure-reproducibility-classification-of-problems-with-reproducing-scientific-papers.html"><a href="how-to-measure-reproducibility-classification-of-problems-with-reproducing-scientific-papers.html#results"><i class="fa fa-check"></i><b>3.1.5</b> Results</a></li>
<li class="chapter" data-level="3.1.6" data-path="building-an-explainable-model-for-ordinal-classification-on-eucalyptus-dataset-meeting-black-box-model-performance-levels-.html"><a href="building-an-explainable-model-for-ordinal-classification-on-eucalyptus-dataset-meeting-black-box-model-performance-levels-.html#model-explanantion"><i class="fa fa-check"></i><b>3.1.6</b> Model explanantion</a></li>
<li class="chapter" data-level="3.1.7" data-path="how-to-measure-reproducibility-classification-of-problems-with-reproducing-scientific-papers.html"><a href="how-to-measure-reproducibility-classification-of-problems-with-reproducing-scientific-papers.html#summary-and-conclusions"><i class="fa fa-check"></i><b>3.1.7</b> Summary and conclusions</a></li>
<li class="chapter" data-level="3.1.8" data-path="building-an-explainable-model-for-ordinal-classification-on-eucalyptus-dataset-meeting-black-box-model-performance-levels-.html"><a href="building-an-explainable-model-for-ordinal-classification-on-eucalyptus-dataset-meeting-black-box-model-performance-levels-.html#references"><i class="fa fa-check"></i><b>3.1.8</b> References</a></li>
</ul></li>
<li class="chapter" data-level="3.2" data-path="predicting-code-defects-using-interpretable-static-measures-.html"><a href="predicting-code-defects-using-interpretable-static-measures-.html"><i class="fa fa-check"></i><b>3.2</b> Predicting code defects using interpretable static measures.</a>
<ul>
<li class="chapter" data-level="3.2.1" data-path="how-to-measure-reproducibility-classification-of-problems-with-reproducing-scientific-papers.html"><a href="how-to-measure-reproducibility-classification-of-problems-with-reproducing-scientific-papers.html#abstract"><i class="fa fa-check"></i><b>3.2.1</b> Abstract</a></li>
<li class="chapter" data-level="3.2.2" data-path="reproducibility-of-outdated-articles-about-up-to-date-r-packages.html"><a href="reproducibility-of-outdated-articles-about-up-to-date-r-packages.html#introduction-and-motivation"><i class="fa fa-check"></i><b>3.2.2</b> Introduction and Motivation</a></li>
<li class="chapter" data-level="3.2.3" data-path="predicting-code-defects-using-interpretable-static-measures-.html"><a href="predicting-code-defects-using-interpretable-static-measures-.html#dataset"><i class="fa fa-check"></i><b>3.2.3</b> Dataset</a></li>
<li class="chapter" data-level="3.2.4" data-path="how-to-measure-reproducibility-classification-of-problems-with-reproducing-scientific-papers.html"><a href="how-to-measure-reproducibility-classification-of-problems-with-reproducing-scientific-papers.html#methodology"><i class="fa fa-check"></i><b>3.2.4</b> Methodology</a></li>
<li class="chapter" data-level="3.2.5" data-path="how-to-measure-reproducibility-classification-of-problems-with-reproducing-scientific-papers.html"><a href="how-to-measure-reproducibility-classification-of-problems-with-reproducing-scientific-papers.html#results"><i class="fa fa-check"></i><b>3.2.5</b> Results</a></li>
<li class="chapter" data-level="3.2.6" data-path="how-to-measure-reproducibility-classification-of-problems-with-reproducing-scientific-papers.html"><a href="how-to-measure-reproducibility-classification-of-problems-with-reproducing-scientific-papers.html#summary-and-conclusions"><i class="fa fa-check"></i><b>3.2.6</b> Summary and conclusions</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="using-interpretable-machine-learning-models-in-the-higgs-boson-detection-.html"><a href="using-interpretable-machine-learning-models-in-the-higgs-boson-detection-.html"><i class="fa fa-check"></i><b>3.3</b> Using interpretable Machine Learning models in the Higgs boson detection.</a>
<ul>
<li class="chapter" data-level="3.3.1" data-path="how-to-measure-reproducibility-classification-of-problems-with-reproducing-scientific-papers.html"><a href="how-to-measure-reproducibility-classification-of-problems-with-reproducing-scientific-papers.html#abstract"><i class="fa fa-check"></i><b>3.3.1</b> Abstract</a></li>
<li class="chapter" data-level="3.3.2" data-path="reproducibility-of-outdated-articles-about-up-to-date-r-packages.html"><a href="reproducibility-of-outdated-articles-about-up-to-date-r-packages.html#introduction-and-motivation"><i class="fa fa-check"></i><b>3.3.2</b> Introduction and Motivation</a></li>
<li class="chapter" data-level="3.3.3" data-path="how-to-measure-reproducibility-classification-of-problems-with-reproducing-scientific-papers.html"><a href="how-to-measure-reproducibility-classification-of-problems-with-reproducing-scientific-papers.html#related-work"><i class="fa fa-check"></i><b>3.3.3</b> Related Work</a></li>
<li class="chapter" data-level="3.3.4" data-path="how-to-measure-reproducibility-classification-of-problems-with-reproducing-scientific-papers.html"><a href="how-to-measure-reproducibility-classification-of-problems-with-reproducing-scientific-papers.html#methodology"><i class="fa fa-check"></i><b>3.3.4</b> Methodology</a></li>
<li class="chapter" data-level="3.3.5" data-path="how-to-measure-reproducibility-classification-of-problems-with-reproducing-scientific-papers.html"><a href="how-to-measure-reproducibility-classification-of-problems-with-reproducing-scientific-papers.html#results"><i class="fa fa-check"></i><b>3.3.5</b> Results</a></li>
<li class="chapter" data-level="3.3.6" data-path="how-to-measure-reproducibility-classification-of-problems-with-reproducing-scientific-papers.html"><a href="how-to-measure-reproducibility-classification-of-problems-with-reproducing-scientific-papers.html#summary-and-conclusions"><i class="fa fa-check"></i><b>3.3.6</b> Summary and conclusions</a></li>
</ul></li>
<li class="chapter" data-level="3.4" data-path="can-automated-regression-beat-linear-model.html"><a href="can-automated-regression-beat-linear-model.html"><i class="fa fa-check"></i><b>3.4</b> Can Automated Regression beat linear model?</a>
<ul>
<li class="chapter" data-level="3.4.1" data-path="how-to-measure-reproducibility-classification-of-problems-with-reproducing-scientific-papers.html"><a href="how-to-measure-reproducibility-classification-of-problems-with-reproducing-scientific-papers.html#abstract"><i class="fa fa-check"></i><b>3.4.1</b> Abstract</a></li>
<li class="chapter" data-level="3.4.2" data-path="reproducibility-of-outdated-articles-about-up-to-date-r-packages.html"><a href="reproducibility-of-outdated-articles-about-up-to-date-r-packages.html#introduction-and-motivation"><i class="fa fa-check"></i><b>3.4.2</b> Introduction and Motivation</a></li>
<li class="chapter" data-level="3.4.3" data-path="using-interpretable-machine-learning-models-in-the-higgs-boson-detection-.html"><a href="using-interpretable-machine-learning-models-in-the-higgs-boson-detection-.html#data"><i class="fa fa-check"></i><b>3.4.3</b> Data</a></li>
<li class="chapter" data-level="3.4.4" data-path="how-to-measure-reproducibility-classification-of-problems-with-reproducing-scientific-papers.html"><a href="how-to-measure-reproducibility-classification-of-problems-with-reproducing-scientific-papers.html#methodology"><i class="fa fa-check"></i><b>3.4.4</b> Methodology</a></li>
<li class="chapter" data-level="3.4.5" data-path="how-to-measure-reproducibility-classification-of-problems-with-reproducing-scientific-papers.html"><a href="how-to-measure-reproducibility-classification-of-problems-with-reproducing-scientific-papers.html#results"><i class="fa fa-check"></i><b>3.4.5</b> Results</a></li>
<li class="chapter" data-level="3.4.6" data-path="how-to-measure-reproducibility-classification-of-problems-with-reproducing-scientific-papers.html"><a href="how-to-measure-reproducibility-classification-of-problems-with-reproducing-scientific-papers.html#summary-and-conclusions"><i class="fa fa-check"></i><b>3.4.6</b> Summary and conclusions</a></li>
</ul></li>
<li class="chapter" data-level="3.5" data-path="interpretable-non-linear-feature-engineering-techniques-for-linear-regression-models-exploration-on-concrete-compressive-strength-dataset-with-a-new-feature-importance-metric-.html"><a href="interpretable-non-linear-feature-engineering-techniques-for-linear-regression-models-exploration-on-concrete-compressive-strength-dataset-with-a-new-feature-importance-metric-.html"><i class="fa fa-check"></i><b>3.5</b> Interpretable, non-linear feature engineering techniques for linear regression models - exploration on concrete compressive strength dataset with a new feature importance metric.</a>
<ul>
<li class="chapter" data-level="3.5.1" data-path="how-to-measure-reproducibility-classification-of-problems-with-reproducing-scientific-papers.html"><a href="how-to-measure-reproducibility-classification-of-problems-with-reproducing-scientific-papers.html#abstract"><i class="fa fa-check"></i><b>3.5.1</b> Abstract</a></li>
<li class="chapter" data-level="3.5.2" data-path="interpretable-non-linear-feature-engineering-techniques-for-linear-regression-models-exploration-on-concrete-compressive-strength-dataset-with-a-new-feature-importance-metric-.html"><a href="interpretable-non-linear-feature-engineering-techniques-for-linear-regression-models-exploration-on-concrete-compressive-strength-dataset-with-a-new-feature-importance-metric-.html#introduction-and-related-works"><i class="fa fa-check"></i><b>3.5.2</b> Introduction and Related Works</a></li>
<li class="chapter" data-level="3.5.3" data-path="how-to-measure-reproducibility-classification-of-problems-with-reproducing-scientific-papers.html"><a href="how-to-measure-reproducibility-classification-of-problems-with-reproducing-scientific-papers.html#methodology"><i class="fa fa-check"></i><b>3.5.3</b> Methodology</a></li>
<li class="chapter" data-level="3.5.4" data-path="how-to-measure-reproducibility-classification-of-problems-with-reproducing-scientific-papers.html"><a href="how-to-measure-reproducibility-classification-of-problems-with-reproducing-scientific-papers.html#results"><i class="fa fa-check"></i><b>3.5.4</b> Results</a></li>
<li class="chapter" data-level="3.5.5" data-path="how-to-measure-reproducibility-classification-of-problems-with-reproducing-scientific-papers.html"><a href="how-to-measure-reproducibility-classification-of-problems-with-reproducing-scientific-papers.html#summary-and-conclusions"><i class="fa fa-check"></i><b>3.5.5</b> Summary and conclusions</a></li>
</ul></li>
<li class="chapter" data-level="3.6" data-path="surpassing-black-box-models-performance-on-unbalanced-data-with-an-interpretable-one-using-advanced-feature-engineering.html"><a href="surpassing-black-box-models-performance-on-unbalanced-data-with-an-interpretable-one-using-advanced-feature-engineering.html"><i class="fa fa-check"></i><b>3.6</b> Surpassing black box model’s performance on unbalanced data with an interpretable one using advanced feature engineering</a>
<ul>
<li class="chapter" data-level="3.6.1" data-path="how-to-measure-reproducibility-classification-of-problems-with-reproducing-scientific-papers.html"><a href="how-to-measure-reproducibility-classification-of-problems-with-reproducing-scientific-papers.html#abstract"><i class="fa fa-check"></i><b>3.6.1</b> Abstract</a></li>
<li class="chapter" data-level="3.6.2" data-path="reproducibility-of-outdated-articles-about-up-to-date-r-packages.html"><a href="reproducibility-of-outdated-articles-about-up-to-date-r-packages.html#introduction-and-motivation"><i class="fa fa-check"></i><b>3.6.2</b> Introduction and Motivation</a></li>
<li class="chapter" data-level="3.6.3" data-path="using-interpretable-machine-learning-models-in-the-higgs-boson-detection-.html"><a href="using-interpretable-machine-learning-models-in-the-higgs-boson-detection-.html#data"><i class="fa fa-check"></i><b>3.6.3</b> Data</a></li>
<li class="chapter" data-level="3.6.4" data-path="how-to-measure-reproducibility-classification-of-problems-with-reproducing-scientific-papers.html"><a href="how-to-measure-reproducibility-classification-of-problems-with-reproducing-scientific-papers.html#methodology"><i class="fa fa-check"></i><b>3.6.4</b> Methodology</a></li>
<li class="chapter" data-level="3.6.5" data-path="how-to-measure-reproducibility-classification-of-problems-with-reproducing-scientific-papers.html"><a href="how-to-measure-reproducibility-classification-of-problems-with-reproducing-scientific-papers.html#results"><i class="fa fa-check"></i><b>3.6.5</b> Results</a></li>
<li class="chapter" data-level="3.6.6" data-path="surpassing-black-box-models-performance-on-unbalanced-data-with-an-interpretable-one-using-advanced-feature-engineering.html"><a href="surpassing-black-box-models-performance-on-unbalanced-data-with-an-interpretable-one-using-advanced-feature-engineering.html#final-results"><i class="fa fa-check"></i><b>3.6.6</b> Final Results</a></li>
<li class="chapter" data-level="3.6.7" data-path="the-hajada-imputation-test.html"><a href="the-hajada-imputation-test.html#conclusions"><i class="fa fa-check"></i><b>3.6.7</b> Conclusions</a></li>
</ul></li>
<li class="chapter" data-level="3.7" data-path="which-neighbours-affected-house-prices-in-the-90s.html"><a href="which-neighbours-affected-house-prices-in-the-90s.html"><i class="fa fa-check"></i><b>3.7</b> Which Neighbours Affected House Prices in the ’90s?</a>
<ul>
<li class="chapter" data-level="3.7.1" data-path="how-to-measure-reproducibility-classification-of-problems-with-reproducing-scientific-papers.html"><a href="how-to-measure-reproducibility-classification-of-problems-with-reproducing-scientific-papers.html#abstract"><i class="fa fa-check"></i><b>3.7.1</b> Abstract</a></li>
<li class="chapter" data-level="3.7.2" data-path="how-to-measure-reproducibility-classification-of-problems-with-reproducing-scientific-papers.html"><a href="how-to-measure-reproducibility-classification-of-problems-with-reproducing-scientific-papers.html#introduction"><i class="fa fa-check"></i><b>3.7.2</b> Introduction</a></li>
<li class="chapter" data-level="3.7.3" data-path="how-to-measure-reproducibility-classification-of-problems-with-reproducing-scientific-papers.html"><a href="how-to-measure-reproducibility-classification-of-problems-with-reproducing-scientific-papers.html#related-work"><i class="fa fa-check"></i><b>3.7.3</b> Related Work</a></li>
<li class="chapter" data-level="3.7.4" data-path="using-interpretable-machine-learning-models-in-the-higgs-boson-detection-.html"><a href="using-interpretable-machine-learning-models-in-the-higgs-boson-detection-.html#data"><i class="fa fa-check"></i><b>3.7.4</b> Data</a></li>
<li class="chapter" data-level="3.7.5" data-path="which-neighbours-affected-house-prices-in-the-90s.html"><a href="which-neighbours-affected-house-prices-in-the-90s.html#sec3-7-methodology"><i class="fa fa-check"></i><b>3.7.5</b> Methodology</a></li>
<li class="chapter" data-level="3.7.6" data-path="how-to-measure-reproducibility-classification-of-problems-with-reproducing-scientific-papers.html"><a href="how-to-measure-reproducibility-classification-of-problems-with-reproducing-scientific-papers.html#results"><i class="fa fa-check"></i><b>3.7.6</b> Results</a></li>
<li class="chapter" data-level="3.7.7" data-path="the-hajada-imputation-test.html"><a href="the-hajada-imputation-test.html#conclusions"><i class="fa fa-check"></i><b>3.7.7</b> Conclusions</a></li>
</ul></li>
<li class="chapter" data-level="3.8" data-path="explainable-computer-vision-with-embedding-and-k-nn-classifier.html"><a href="explainable-computer-vision-with-embedding-and-k-nn-classifier.html"><i class="fa fa-check"></i><b>3.8</b> Explainable Computer Vision with embedding and k-NN classifier</a>
<ul>
<li class="chapter" data-level="3.8.1" data-path="how-to-measure-reproducibility-classification-of-problems-with-reproducing-scientific-papers.html"><a href="how-to-measure-reproducibility-classification-of-problems-with-reproducing-scientific-papers.html#abstract"><i class="fa fa-check"></i><b>3.8.1</b> Abstract</a></li>
<li class="chapter" data-level="3.8.2" data-path="how-to-measure-reproducibility-classification-of-problems-with-reproducing-scientific-papers.html"><a href="how-to-measure-reproducibility-classification-of-problems-with-reproducing-scientific-papers.html#introduction"><i class="fa fa-check"></i><b>3.8.2</b> Introduction</a></li>
<li class="chapter" data-level="3.8.3" data-path="how-to-measure-reproducibility-classification-of-problems-with-reproducing-scientific-papers.html"><a href="how-to-measure-reproducibility-classification-of-problems-with-reproducing-scientific-papers.html#methodology"><i class="fa fa-check"></i><b>3.8.3</b> Methodology</a></li>
<li class="chapter" data-level="3.8.4" data-path="how-to-measure-reproducibility-classification-of-problems-with-reproducing-scientific-papers.html"><a href="how-to-measure-reproducibility-classification-of-problems-with-reproducing-scientific-papers.html#results"><i class="fa fa-check"></i><b>3.8.4</b> Results</a></li>
<li class="chapter" data-level="3.8.5" data-path="explainable-computer-vision-with-embedding-and-k-nn-classifier.html"><a href="explainable-computer-vision-with-embedding-and-k-nn-classifier.html#discussion-and-conclusion"><i class="fa fa-check"></i><b>3.8.5</b> Discussion and Conclusion</a></li>
</ul></li>
<li class="chapter" data-level="3.8.6" data-path="explainable-computer-vision-with-embedding-and-k-nn-classifier.html"><a href="explainable-computer-vision-with-embedding-and-k-nn-classifier.html#bibliography"><i class="fa fa-check"></i><b>3.8.6</b> Bibliography</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="acknowledgements.html"><a href="acknowledgements.html"><i class="fa fa-check"></i><b>4</b> Acknowledgements</a></li>
<li class="chapter" data-level="" data-path="building-an-explainable-model-for-ordinal-classification-on-eucalyptus-dataset-meeting-black-box-model-performance-levels-.html"><a href="building-an-explainable-model-for-ordinal-classification-on-eucalyptus-dataset-meeting-black-box-model-performance-levels-.html#references"><i class="fa fa-check"></i>References</a></li>
<li class="divider"></li>
<li><a href="https://bookdown.org" target="_blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">ML Case Studies</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="default-imputation-efficiency-comparison" class="section level2" number="2.1">
<h2><span class="header-section-number">2.1</span> Default imputation efficiency comparison</h2>
<p><em>Authors: Jakub Pingielski, Paulina Przybyłek, Renata Rólkiewicz, Jakub Wiśniewski (Warsaw University of Technology)</em></p>
<div id="abstract" class="section level3" number="2.1.1">
<h3><span class="header-section-number">2.1.1</span> Abstract</h3>
<p>Imputation of missing values is one of the most common preprocessing steps when working with many real-life datasets. There are many ways to approach this task. In this article, we will be using 3 different simple imputation methods and 6 more sophisticated methods from popular R packages but without changing or tuning their parameters. We will test how their imputation affects the score of models trained on classification datasets. Effectiveness of the imputations will be checked by measuring models’ performance on different measures but focusing on one which is sensitive to class imbalance - Weighted TPR-TNR. We will try to answer the question of whether simple imputation methods are still insignificant for more complex methods with default parameters, or maybe in this case simple imputation methods are not only faster but more efficient?</p>
</div>
<div id="introduction-and-motivation" class="section level3" number="2.1.2">
<h3><span class="header-section-number">2.1.2</span> Introduction and Motivation</h3>
<p>Data is becoming more and more useful and valuable in various industries. Companies started to gather data on a scale that has not been imagined before. But with more data comes more problems. One of them is missing values. Omitting observations affected by them is one way, but why doing it while the information that they possess might be still valuable? This is why the imputation of missing values is essential when dealing with real-life data. But there are various methods of imputation and for an inexperienced user, it might be confusing and hard to decide which one to use. It may be even harder to design metrics that allow comparing methods between them. We decided that we should judge the effectiveness on the performance score of the model which has been trained on dataset imputed using some method. So when model trained on data imputed with <span class="math inline">\(method_1\)</span> has a higher performance score than the same model trained on the same data but imputed with <span class="math inline">\(method_2\)</span>, that means that <span class="math inline">\(method_1\)</span> is more effective.<br />
Sometimes omitting rows or even removing columns and NA can give satisfying results but the majority of the time, it is worse than simple imputation methods. For simple tasks mean/median and mode are fast and computationally efficient and can give good results. But they have no variance and because of that can give worse results on complex datasets than more robust methods. On the other hand, more complex models like for example random forests are in theory more advanced and despite adding algorithmic complexity and computational costs to training procedure, are thought to be better in quality of imputation. So models that had been trained on data that has been previously imputed with more complex methods are more likely to achieve higher performance scores.<br />
Furthermore, parameter changing and tuning can be difficult and time-consuming. Deciding which method is the best for an inexperienced user who will not be tuning hyperparameters is the purpose of this article. We are benchmarking methods that have different complexity from various packages along with simple imputation methods but without parameter tuning, so that casual users can benefit from it. The default methods from packages are easy to use and despite not living up to their potential they might acceptable results.</p>
</div>
<div id="related-work" class="section level3" number="2.1.3">
<h3><span class="header-section-number">2.1.3</span> Related Work</h3>
<p>Missing data imputation is a challenging issue in machine learning (ML) and data mining. <span class="citation">Little and Rubin (<a href="#ref-2-1-little-rubin" role="doc-biblioref">2002</a>)</span> defined three main categories of missing values, but most more or less generic methods of imputation, which have been proposed in the last few decades, are suited for data from one of this category. This category is called missing at random, in short MAR. This assumes that the fact that a value is missing is not dependent on other features of the data. So we want to avoid a situation when for example occurrence of missing values in feature <span class="math inline">\(X_i\)</span> is highly correlated with some values in feature <span class="math inline">\(X_j\)</span>.<br />
Many programmers are actively creating and looking for new strategies of data imputation that enable creating the best ML model possible. They use various packages to visualize and impute missing data with different methods like knn (k-Nearest Neighbor) or even different model for each feature with missing values <span class="citation">(Kowarik and Templ <a href="#ref-2-1-VIM" role="doc-biblioref">2016</a><a href="#ref-2-1-VIM" role="doc-biblioref">a</a>)</span>. Some methods from popular packages follow a fully conditional specification approach where one variable with missing values becomes the target variable <span class="citation">(van Buuren and Groothuis-Oudshoorn <a href="#ref-2-1-mice" role="doc-biblioref">2011</a><a href="#ref-2-1-mice" role="doc-biblioref">a</a>)</span>. To our knowledge, there are not many available papers comparing different methods of imputations from different packages and measuring their efficiency on multiple models and datasets. The use of an appropriate metric is crucial for this task because not all metrics are suitable for all datasets. <span class="citation">Jadhav (<a href="#ref-2-1-weighted-tpr-tnr" role="doc-biblioref">2020</a>)</span> presented a new metric Weighted TPR-TNR for measuring the performance of ML models on imbalanced datasets, which according to the authors is the best overall metric available.</p>
</div>
<div id="methodology" class="section level3" number="2.1.4">
<h3><span class="header-section-number">2.1.4</span> Methodology</h3>
<p>In our article, we compare different methods of imputations - simple and advanced methods (from several R packages) - and measuring their efficiency on multiple models and classification datasets. This section of the article contains information about selected datasets, methods of imputations, ML algorithms, and metrics used to assess the quality of the classification after applying imputation to given datasets. Also, at the end of the section, the workflow of our experiment is briefly presented.</p>
<hr />
<div id="datasets" class="section level4" number="2.1.4.1">
<h4><span class="header-section-number">2.1.4.1</span> <em>Datasets</em></h4>
<p>In the experiment, we used various OpenML datasets, some of them belong to OpenML100 repository <span class="citation">(Bischl, Casalicchio, et al. <a href="#ref-2-1-OpenML100" role="doc-biblioref">2017</a>)</span>. Datasets have been selected so that they have missing values and datasets differ in the number of observations, features, and percentage of data missing. All these datasets were designed for the classification task. The information about these datasets is given in Table <a href="default-imputation-efficiency-comparison.html#tab:2-1-statistics-tab">2.1</a>.</p>
<table class="table" style="margin-left: auto; margin-right: auto;">
<caption>
<span id="tab:2-1-statistics-tab">TABLE 2.1: </span>Datasets Information
</caption>
<thead>
<tr>
<th style="text-align:left;">
Dataset
</th>
<th style="text-align:right;">
Number of observations
</th>
<th style="text-align:right;">
Number of features
</th>
<th style="text-align:right;">
Imbalance ratio
</th>
<th style="text-align:right;">
Percent of missing values
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;">
cylinder-bands
</td>
<td style="text-align:right;">
540
</td>
<td style="text-align:right;">
31
</td>
<td style="text-align:right;">
1.368
</td>
<td style="text-align:right;">
5.299
</td>
</tr>
<tr>
<td style="text-align:left;">
credit-approval
</td>
<td style="text-align:right;">
690
</td>
<td style="text-align:right;">
16
</td>
<td style="text-align:right;">
1.248
</td>
<td style="text-align:right;">
0.607
</td>
</tr>
<tr>
<td style="text-align:left;">
adult
</td>
<td style="text-align:right;">
5000
</td>
<td style="text-align:right;">
13
</td>
<td style="text-align:right;">
3.146
</td>
<td style="text-align:right;">
0.958
</td>
</tr>
<tr>
<td style="text-align:left;">
eucalyptus
</td>
<td style="text-align:right;">
736
</td>
<td style="text-align:right;">
16
</td>
<td style="text-align:right;">
1.307
</td>
<td style="text-align:right;">
3.864
</td>
</tr>
<tr>
<td style="text-align:left;">
dresses-sales
</td>
<td style="text-align:right;">
500
</td>
<td style="text-align:right;">
13
</td>
<td style="text-align:right;">
1.381
</td>
<td style="text-align:right;">
14.692
</td>
</tr>
<tr>
<td style="text-align:left;">
colic
</td>
<td style="text-align:right;">
368
</td>
<td style="text-align:right;">
20
</td>
<td style="text-align:right;">
1.706
</td>
<td style="text-align:right;">
16.291
</td>
</tr>
<tr>
<td style="text-align:left;">
sick
</td>
<td style="text-align:right;">
3772
</td>
<td style="text-align:right;">
28
</td>
<td style="text-align:right;">
15.329
</td>
<td style="text-align:right;">
2.171
</td>
</tr>
<tr>
<td style="text-align:left;">
labor
</td>
<td style="text-align:right;">
57
</td>
<td style="text-align:right;">
17
</td>
<td style="text-align:right;">
1.850
</td>
<td style="text-align:right;">
33.643
</td>
</tr>
<tr>
<td style="text-align:left;">
hepatitis
</td>
<td style="text-align:right;">
155
</td>
<td style="text-align:right;">
20
</td>
<td style="text-align:right;">
3.844
</td>
<td style="text-align:right;">
5.387
</td>
</tr>
<tr>
<td style="text-align:left;">
vote
</td>
<td style="text-align:right;">
435
</td>
<td style="text-align:right;">
17
</td>
<td style="text-align:right;">
1.589
</td>
<td style="text-align:right;">
5.301
</td>
</tr>
<tr>
<td style="text-align:left;">
echoMonths
</td>
<td style="text-align:right;">
130
</td>
<td style="text-align:right;">
10
</td>
<td style="text-align:right;">
1.031
</td>
<td style="text-align:right;">
7.462
</td>
</tr>
</tbody>
</table>
<p>Imbalance ratio is computed as follows <span class="math inline">\(imbalance\_{ratio} = n/p\)</span> where <span class="math inline">\(n\)</span> - negative class and <span class="math inline">\(p\)</span> - positive class.</p>
<hr />
</div>
<div id="imputing-strategies" class="section level4" number="2.1.4.2">
<h4><span class="header-section-number">2.1.4.2</span> <em>Imputing strategies</em></h4>
<p>We selected and analyzed nine different imputation strategies that are known and willingly used by programmers. Some of them belong to simple methods of imputation and others are more complex. A description of these methods is below.</p>
<ol style="list-style-type: decimal">
<li><em>remove columns</em> - removing columns with any missing values from the dataset.</li>
<li><em>random fill</em> - imputing with random values from the given feature.</li>
<li><em>median and mode</em> - imputing with median (for numerical features) and mode (for categorical features) from the given feature.</li>
<li><em>missForest</em> - imputing with Random Forests (using missForest package <span class="citation">(Stekhoven and Buehlmann <a href="#ref-2-1-missForest" role="doc-biblioref">2012</a><a href="#ref-2-1-missForest" role="doc-biblioref">a</a>)</span>). Function from this package builds a random forest model for each feature with missing values. Then it uses the model to predict the value of missing instances in the feature based on other features.</li>
<li><em>vim knn</em> - imputing with k-Nearest Neighbor Imputation based on a variation of the Gower Distance (using VIM package <span class="citation">(Kowarik and Templ <a href="#ref-2-1-VIM" role="doc-biblioref">2016</a><a href="#ref-2-1-VIM" role="doc-biblioref">a</a>)</span>). An aggregation of the k values of the nearest neighbors is used as an imputed value.</li>
<li><em>vim hotdeck</em> - imputing with sequential, random (within a domain) hot-deck algorithm (using VIM package). In this method, the dataset is sorted and missing values are imputed sequentially running through the dataset line (observation) by line (observation).</li>
<li><em>vim irmi</em> - imputing with Iterative Robust Model-Based Imputation (IRMI) (using VIM package). In each step of the iteration (inner loop), one variable is used as a response variable and the remaining variables serve as the regressors. The procedure is repeated until the algorithm converges (outer loop).</li>
<li><em>pmm (mice)</em> - imputing with Fully Conditional Specification and predictive mean matching (using mice package <span class="citation">(van Buuren and Groothuis-Oudshoorn <a href="#ref-2-1-mice" role="doc-biblioref">2011</a><a href="#ref-2-1-mice" role="doc-biblioref">a</a>)</span>). For each observation in a feature with missing values, this method finds observation (from available values) with the closest predictive mean to that feature. The observed value from this “match” is then used as an imputed value.</li>
<li><em>cart (mice)</em> - imputing with Fully Conditional Specification and classification and regression trees (using mice package). CART models select a feature and find a split that achieves the most information gain. Splitting process is repeated until the maximum height is reached or branches have only samples from one class.</li>
</ol>
<p>The first three methods will be called simple imputation methods. They are self-explanatory and do not have any hyperparameters to tune. In contrast, algorithms provided in missForest, VIM, and mice packages are complex and may require costly hyperparameters tuning. We used default implementations in our experiments, based on the assumption that additional complexity and more hyperparameters might create a too steep learning curve for an average data scientist.</p>
<p>Since neither mlr nor caret package enables using custom imputing methods in modeling pipeline, the only way to avoid data leakage was to impute values separately for train and test datasets. This imposes a severe limitation on our imputation effectiveness, especially for small datasets, since information learned during imputing values in training data set cannot be used during test set imputation.</p>
<hr />
</div>
<div id="classification-algorithms" class="section level4" number="2.1.4.3">
<h4><span class="header-section-number">2.1.4.3</span> <em>Classification algorithms</em></h4>
<p>All models were based on classifiers from the caret package <span class="citation">(Kuhn <a href="#ref-2-1-caret" role="doc-biblioref">2008</a>)</span> with AUC being the summary metric that was used to select the optimal model. Our goal was to use classification models that are as different from each other as possible, to check if model complexity and flexibility have an impact on the effectiveness of imputation strategy. The most effective imputation strategy should enable models to achieve the highest performance scores.
Selected classification algorithms are listed below.</p>
<ol style="list-style-type: decimal">
<li>Bagged CART - simple classification trees with bootstrap aggregation. <em>treebag</em> builds multiple models from different subsets of data. At the end constructs a final aggregated and more accurate (according to the chosen metric) model.</li>
<li>Random Forest - very flexible, robust, and popular algorithm, with no assumptions about data. Multiple classification trees trained on bagged data. For this article implementation called <em>ranger</em> will be used because of its lower computing time leverage.<br />
</li>
<li>Multivariate Adaptive Regression Spline - assumes that relation between features and response can be expressed by combining two linear models at a certain cutoff point - can be regarded as a model with flexibility between linear and nonlinear models. Implementation called <em>earth</em> will be used.</li>
<li>k-Nearest Neighbors - assumes that similar observations are close to each other in feature space, assumes a very local impact of features. <em>knn</em> is a simple algorithm and will be a good benchmark for others.</li>
<li>Naive Bayes - nonlinear model with the assumption that all features are independent of each other. Pros of <em>nb</em> are: fast in training, easily interpretable, has little hyperparameters to tune.</li>
</ol>
<p>While some of these algorithms have multiple hyperparameters to tune it will not be covered in this article. Tuning for different data can be time-consuming especially when datasets are large and tuning them could mitigate the effect of data imputation. Based on this factor default implementations and parameters of those algorithms will be used. It might not get the best score in metrics but it will be fair for all the imputation methods. It can be argued that the impact of imputation will be even more visible without parameter tuning. All algorithms will be trained and tested on the same data but some will be encoded.</p>
<hr />
</div>
<div id="metrics" class="section level4" number="2.1.4.4">
<h4><span class="header-section-number">2.1.4.4</span> <em>Metrics</em></h4>
<p>We performed the aforementioned imputing methods for 11 datasets from the OpenML repository. Since all datasets were designed for binary classification problems and many of them were heavily imbalanced, we used 4 metrics to evaluate our models’ performance (see Table below).</p>
<table>
<caption>Classifier Evaluation Measures</caption>
<thead>
<tr class="header">
<th>Measure</th>
<th>Formula</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>AUC</td>
<td>Measures area under plot of Sensitivity against Specificity</td>
</tr>
<tr class="even">
<td>Balanced Accuracy</td>
<td><span class="math inline">\(\frac{1}{2}(Sensitivity+Specificity)\)</span></td>
</tr>
<tr class="odd">
<td>Geometric Mean</td>
<td><span class="math inline">\(\sqrt{Sensitivity*Specificity}\)</span></td>
</tr>
<tr class="even">
<td>Weighted TPR-TNR</td>
<td><span class="math inline">\((Sensitivity*\frac{N}{P+N})+(Specificity*\frac{P}{P+N})\)</span></td>
</tr>
</tbody>
</table>
<p>Area under ROC curve (AUC) is used as a general performance metrics, while the rest is used to specifically evaluate model performance on imbalanced data sets.<br />
Accuracy measures how a good model is incorrectly predicting both positive and negative cases. If your dataset is imbalanced, then “regular” accuracy may not be enough (cost of misclassification of minority class instance is higher than for majority class instance). That is why we use Balanced Accuracy, which is the arithmetic mean of the TPR (Sensitivity) and FPR (Specificity).<br />
Geometric Mean (of Sensitivity and Specificity) is a metric that measures the balance between classification performances on both the majority and minority classes.<br />
Weighted TPR-TNR is a very promising new single-valued measure for classification. It takes into account the imbalance ratio of the dataset and assigns different weights to the TPR and TNR (where P is the total number of positive cases and N is the total number of negative cases).</p>
<hr />
</div>
<div id="experiment" class="section level4" number="2.1.4.5">
<h4><span class="header-section-number">2.1.4.5</span> <em>Experiment</em></h4>
<p>To sum up, in the experiment we used 5 different classification algorithms and 11 different datasets to assess the performance of the imputation of missing values using the previously described measures. The experiment workflow was presented in Figure <a href="default-imputation-efficiency-comparison.html#fig:2-1-workflow">2.1</a>.</p>
<div class="figure" style="text-align: center"><span id="fig:2-1-workflow"></span>
<img src="images/2-1-workflow.png" alt="Experiment workflow" width="800" />
<p class="caption">
FIGURE 2.1: Experiment workflow
</p>
</div>
<p>The caret package in R programming was used to conduct the experiment. Classification algorithms used in this experiment were: <em>ranger</em>, <em>earth</em>, <em>treebag</em>, <em>knn</em>, <em>nb</em> existing in function train() from this package.</p>
<p>The procedure followed to assess the performance of the imputation of missing values was as follows:</p>
<ol style="list-style-type: decimal">
<li>The dataset was divided into train and test subsets with a split ratio of 80/20.</li>
<li>We imputed values separately in train and test datasets. This process was repeated for all nine different imputing strategies. The time of imputation was measured independently outside this workflow.</li>
<li>All imputed train and test datasets were used in five classification algorithms.</li>
<li>Numerical variables were centered and scaled to mean 0 and standard deviation 1. For <em>knn</em> datasets were also encoded with one-hot encoding.</li>
<li>The classifiers were built using transformed train datasets.</li>
<li>The classification results were obtained using a transformed test datasets.</li>
</ol>
<p>Before dividing into training and test subsets and building classifiers, seed equal to one was set.</p>
</div>
</div>
<div id="results" class="section level3" number="2.1.5">
<h3><span class="header-section-number">2.1.5</span> Results</h3>
<p>Before analyzing the results, three caveats:</p>
<ol style="list-style-type: decimal">
<li>some of our datasets were quite small, with 100 or so observations</li>
<li>some of our datasets were heavily imbalanced</li>
<li>some imputations methods did not cover or generally impute data due to the specificity of the dataset. These methods are:
<ul>
<li><p><em>remove columns</em> 3/11 failed (labor, vote, and colic datasets)</p></li>
<li><p><em>pmm (mice)</em> and <em>cart (mice)</em> 3/11 failed (cylinder-bands, credit-approval, and labor datasets)</p></li>
<li><p><em>vim irmi</em> 5/11 failed (all of the above)</p></li>
</ul></li>
</ol>
<p>For <em>removing columns</em> explanation is simple. In some datasets, missing values are in every column so after removing all columns model has no
data to be trained on. For other methods, it is more complicated to explain. Apart from removing columns methods they did not throw an error and tried
to imput the data but without success.</p>
<p>While analyzing the results we are looking at two things:</p>
<ol style="list-style-type: decimal">
<li>for which imputation method the metrics have the best results</li>
<li>time of imputation</li>
</ol>
<p>Especially for big datasets, sophisticated imputation methods required many hours or even days of computing. For that reason datasets with tens of thousands of observations were truncated to 5000 (refers to the adult dataset).</p>
<hr />
<div id="imputation-results" class="section level4" number="2.1.5.1">
<h4><span class="header-section-number">2.1.5.1</span> <em>Imputation results</em></h4>
<div id="mean-metrics" class="section level5" number="2.1.5.1.1">
<h5><span class="header-section-number">2.1.5.1.1</span> <strong><em>Mean metrics</em></strong></h5>
<p>With this first look at the results of imputations performance metric scores will be taken into consideration. In Table <a href="default-imputation-efficiency-comparison.html#tab:2-1-mean-metrics-tab">2.2</a> was presented this performance metrics ordered by Weighted TPR-TNR. Note that in this experiment methods that failed to impute data will be omitted so the whole picture of efficiency among imputation metrics might be distorted. This issue will be addressed in the next section.</p>
<table class="table" style="margin-left: auto; margin-right: auto;">
<caption>
<span id="tab:2-1-mean-metrics-tab">TABLE 2.2: </span>Mean metrics for each model
</caption>
<thead>
<tr>
<th style="text-align:left;">
Imputation name
</th>
<th style="text-align:right;">
AUC
</th>
<th style="text-align:right;">
BACC
</th>
<th style="text-align:right;">
GM
</th>
<th style="text-align:right;">
Weighted TPR-TNR
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;">
random fill
</td>
<td style="text-align:right;">
0.8717
</td>
<td style="text-align:right;">
0.8037
</td>
<td style="text-align:right;">
0.7900
</td>
<td style="text-align:right;">
0.7687
</td>
</tr>
<tr>
<td style="text-align:left;">
median and mode
</td>
<td style="text-align:right;">
0.8670
</td>
<td style="text-align:right;">
0.7931
</td>
<td style="text-align:right;">
0.7783
</td>
<td style="text-align:right;">
0.7516
</td>
</tr>
<tr>
<td style="text-align:left;">
vim hotdeck
</td>
<td style="text-align:right;">
0.8534
</td>
<td style="text-align:right;">
0.7839
</td>
<td style="text-align:right;">
0.7701
</td>
<td style="text-align:right;">
0.7486
</td>
</tr>
<tr>
<td style="text-align:left;">
missForest
</td>
<td style="text-align:right;">
0.8515
</td>
<td style="text-align:right;">
0.7879
</td>
<td style="text-align:right;">
0.7611
</td>
<td style="text-align:right;">
0.7453
</td>
</tr>
<tr>
<td style="text-align:left;">
vim knn
</td>
<td style="text-align:right;">
0.8667
</td>
<td style="text-align:right;">
0.7771
</td>
<td style="text-align:right;">
0.7563
</td>
<td style="text-align:right;">
0.7445
</td>
</tr>
<tr>
<td style="text-align:left;">
cart (mice)
</td>
<td style="text-align:right;">
0.8581
</td>
<td style="text-align:right;">
0.7884
</td>
<td style="text-align:right;">
0.7702
</td>
<td style="text-align:right;">
0.7379
</td>
</tr>
<tr>
<td style="text-align:left;">
pmm (mice)
</td>
<td style="text-align:right;">
0.8568
</td>
<td style="text-align:right;">
0.7847
</td>
<td style="text-align:right;">
0.7678
</td>
<td style="text-align:right;">
0.7360
</td>
</tr>
<tr>
<td style="text-align:left;">
vim irmi
</td>
<td style="text-align:right;">
0.8220
</td>
<td style="text-align:right;">
0.7474
</td>
<td style="text-align:right;">
0.7215
</td>
<td style="text-align:right;">
0.6812
</td>
</tr>
<tr>
<td style="text-align:left;">
remove columns
</td>
<td style="text-align:right;">
0.7648
</td>
<td style="text-align:right;">
0.6623
</td>
<td style="text-align:right;">
0.5541
</td>
<td style="text-align:right;">
0.5523
</td>
</tr>
</tbody>
</table>
<p>As seen above <em>random fill</em> is the best imputation method in all metrics. Second in place in all metrics was <em>median and mode</em>. <em>cart (mice)</em> was third 2 times (GM and BACC) and <em>vim knn</em> with <em>vim hotdeck</em> were third each one time. Simple imputation methods were superior for these datasets. The worst idea seems to be removing columns and using irmi.</p>
</div>
<div id="ranking-of-results" class="section level5" number="2.1.5.1.2">
<h5><span class="header-section-number">2.1.5.1.2</span> <strong><em>Ranking of results</em></strong></h5>
<p>Each model has its specific features and might be thriving after applying different imputation methods. Seeing which method was the best (and the worst) for each model should give a more wide perspective. For creating ranking only Weighted TPR-TNR will be taken into consideration. This is because this metric takes into account the imbalance ratio of the datasets and thanks to its equal evaluation for all datasets will be ensured. Rank feature here is average of ranks from all datasets so if some imputation was second in the metric it would be given rank 2. If two imputations had the same values in the metric they would both have the same rank. If the imputation method failed to impute it was assigned the last rank. Top 3 best imputations was presented in Table <a href="default-imputation-efficiency-comparison.html#tab:2-1-ranking-best-tab">2.3</a>.</p>
<table class="table" style="margin-left: auto; margin-right: auto;">
<caption>
<span id="tab:2-1-ranking-best-tab">TABLE 2.3: </span>Top 3 best imputations for each model
</caption>
<thead>
<tr>
<th style="text-align:left;">
Model name
</th>
<th style="text-align:left;">
1th method imputation
</th>
<th style="text-align:left;">
2th method imputation
</th>
<th style="text-align:left;">
3th method imputation
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;">
ranger
</td>
<td style="text-align:left;">
vim knn
</td>
<td style="text-align:left;">
missForest
</td>
<td style="text-align:left;">
pmm (mice)
</td>
</tr>
<tr>
<td style="text-align:left;">
earth
</td>
<td style="text-align:left;">
random fill
</td>
<td style="text-align:left;">
median and mode
</td>
<td style="text-align:left;">
cart (mice)
</td>
</tr>
<tr>
<td style="text-align:left;">
treebag
</td>
<td style="text-align:left;">
random fill
</td>
<td style="text-align:left;">
cart (mice)
</td>
<td style="text-align:left;">
median and mode
</td>
</tr>
<tr>
<td style="text-align:left;">
knn
</td>
<td style="text-align:left;">
missForest
</td>
<td style="text-align:left;">
random fill
</td>
<td style="text-align:left;">
vim knn
</td>
</tr>
<tr>
<td style="text-align:left;">
nb
</td>
<td style="text-align:left;">
random fill
</td>
<td style="text-align:left;">
median and mode
</td>
<td style="text-align:left;">
missForest
</td>
</tr>
</tbody>
</table>
<p>Random fill is the best in 3 out of 5 models. For ranger and knn the best are VIM knn and missForest respectively. In our top 3 ranking also appears median and mode and two mice methods - pmm and cart.</p>
<p>Now let’s see the top 3 worst imputations in Table <a href="default-imputation-efficiency-comparison.html#tab:2-1-ranking-worst-tab">2.4</a>.</p>
<table class="table" style="margin-left: auto; margin-right: auto;">
<caption>
<span id="tab:2-1-ranking-worst-tab">TABLE 2.4: </span>Top 3 worst imputations for each model
</caption>
<thead>
<tr>
<th style="text-align:left;">
Model name
</th>
<th style="text-align:left;">
7th method imputation
</th>
<th style="text-align:left;">
8th method imputation
</th>
<th style="text-align:left;">
9th method imputation
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;">
ranger
</td>
<td style="text-align:left;">
vim hotdeck
</td>
<td style="text-align:left;">
vim irmi
</td>
<td style="text-align:left;">
remove columns
</td>
</tr>
<tr>
<td style="text-align:left;">
earth
</td>
<td style="text-align:left;">
pmm (mice)
</td>
<td style="text-align:left;">
vim irmi
</td>
<td style="text-align:left;">
remove columns
</td>
</tr>
<tr>
<td style="text-align:left;">
treebag
</td>
<td style="text-align:left;">
vim knn
</td>
<td style="text-align:left;">
vim irmi
</td>
<td style="text-align:left;">
remove columns
</td>
</tr>
<tr>
<td style="text-align:left;">
knn
</td>
<td style="text-align:left;">
vim irmi
</td>
<td style="text-align:left;">
pmm (mice)
</td>
<td style="text-align:left;">
remove columns
</td>
</tr>
<tr>
<td style="text-align:left;">
nb
</td>
<td style="text-align:left;">
cart (mice)
</td>
<td style="text-align:left;">
vim irmi
</td>
<td style="text-align:left;">
remove columns
</td>
</tr>
</tbody>
</table>
<p>Removing columns is the worst choice when imputing data, which is not surprising. It removes vital information from data resulting in significantly worse performance. Second, from the end is almost for all models vim irmi.</p>
<p>Visualization in Figure <a href="default-imputation-efficiency-comparison.html#fig:2-1-ranking">2.2</a> shows the mean rank of imputation methods for each model. Ranks are a great way to visualize how effective the imputation really is. They are not sensitive to the dataset, the best rank will always be one, unlike some performance measure that might be significantly different between datasets. The average was measured on 11 datasets. The higher the rank, the better. Some trends and stability of methods are visible.</p>
<div class="figure" style="text-align: center"><span id="fig:2-1-ranking"></span>
<img src="images/2-1-ranks.png" alt="Average rank plot" width="800" />
<p class="caption">
FIGURE 2.2: Average rank plot
</p>
</div>
<p><em>Random fill</em> is the best method. It was the best for three models and the second-best for one model. <em>Median and mode</em> and <em>missForest</em> were also really good for all models. The worst without a doubt is <em>removing columns</em>. Some methods are really good for certain models but very bad for others (for example <em>pmm (mice)</em>) in contrary to <em>random fill</em> and <em>median and mode</em> which were equally good for all models but not the best for knn and ranger.</p>
</div>
<div id="similarity-of-imputations" class="section level5" number="2.1.5.1.3">
<h5><span class="header-section-number">2.1.5.1.3</span> <strong><em>Similarity of imputations</em></strong></h5>
<p>To see how similar are imputations to each other we used biplot. Biplot is using PCA to reduce the dimensionality of the data in our case of results. If imputation names are close to each other, it means that they produce similar results. The closer the arrows are to each other the more metrics values are correlated. Data came from our imputation results for all models without NA values. Then for each imputation data was averaged and processed by PCA (see Figure <a href="default-imputation-efficiency-comparison.html#fig:2-1-pca">2.3</a>).</p>
<div class="figure" style="text-align: center"><span id="fig:2-1-pca"></span>
<img src="images/2-1-PCA.png" alt="Rank biplot" width="800" />
<p class="caption">
FIGURE 2.3: Rank biplot
</p>
</div>
<p>Some imputation techniques seem to be distant from others. Mainly VIM’s irmi, remove columns, pmm, and knn. Judging by the red arrows (loadings plot), there are similarities in ranks among treebag and earth models. Knn and ranger are not very correlated with ranks achieved by them.</p>
<hr />
</div>
</div>
<div id="imputation-time" class="section level4" number="2.1.5.2">
<h4><span class="header-section-number">2.1.5.2</span> <em>Imputation time</em></h4>
<p>In imputation not only results matter. While having small dataset imputation time is not something worth considering. But when having tens or hundreds of thousands of observations some more complex imputations might take a really long time. In this case, maximum samples in data are 5000 but differences are already visible. This experiment was repeated 10 times to ensure higher reliability of data.</p>
<div id="mean-time" class="section level5" number="2.1.5.2.1">
<h5><span class="header-section-number">2.1.5.2.1</span> <strong><em>Mean time</em></strong></h5>
<table class="table" style="margin-left: auto; margin-right: auto;">
<caption>
<span id="tab:2-1-time-tab">TABLE 2.5: </span>Average results - time
</caption>
<thead>
<tr>
<th style="text-align:left;">
Imputation name
</th>
<th style="text-align:right;">
Time
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;">
remove columns
</td>
<td style="text-align:right;">
0.0140
</td>
</tr>
<tr>
<td style="text-align:left;">
random fill
</td>
<td style="text-align:right;">
0.0154
</td>
</tr>
<tr>
<td style="text-align:left;">
median and mode
</td>
<td style="text-align:right;">
0.0272
</td>
</tr>
<tr>
<td style="text-align:left;">
vim hotdeck
</td>
<td style="text-align:right;">
0.2484
</td>
</tr>
<tr>
<td style="text-align:left;">
vim knn
</td>
<td style="text-align:right;">
1.6833
</td>
</tr>
<tr>
<td style="text-align:left;">
pmm (mice)
</td>
<td style="text-align:right;">
8.1867
</td>
</tr>
<tr>
<td style="text-align:left;">
missForest
</td>
<td style="text-align:right;">
8.9694
</td>
</tr>
<tr>
<td style="text-align:left;">
vim irmi
</td>
<td style="text-align:right;">
10.1300
</td>
</tr>
<tr>
<td style="text-align:left;">
cart (mice)
</td>
<td style="text-align:right;">
20.0550
</td>
</tr>
</tbody>
</table>
<p>As one might predict, imputation with mean and mode removing columns or filling NA’s with random value are the fastest methods (see Table <a href="default-imputation-efficiency-comparison.html#tab:2-1-time-tab">2.5</a>). Imputations from the VIM package also have small computation time. Imputing with missForest package or using predictive mean matching in mice package is more than 1000 times slower, whilst using cart instead of pmm in mice package additionally increases computation time more than threefold.</p>
</div>
<div id="distribution-of-time" class="section level5" number="2.1.5.2.2">
<h5><span class="header-section-number">2.1.5.2.2</span> <strong><em>Distribution of time</em></strong></h5>
<div class="figure" style="text-align: center"><span id="fig:2-1-time"></span>
<img src="images/2-1-time.png" alt="Time distribution" width="800" />
<p class="caption">
FIGURE 2.4: Time distribution
</p>
</div>
<p>Imputation time is shown in the shape of boxplots (see Figure <a href="default-imputation-efficiency-comparison.html#fig:2-1-time">2.4</a>). A clear distinction between mice, missForest, and VIM’ irmi packages functions which are significantly slower than other methods. On contrary here imputation time was taken into consideration even when NA’s were produced.</p>
</div>
<div id="influence-of-amount-of-na-on-time" class="section level5" number="2.1.5.2.3">
<h5><span class="header-section-number">2.1.5.2.3</span> <strong><em>Influence of amount of NA on time</em></strong></h5>
<div class="figure" style="text-align: center"><span id="fig:2-1-influence-NA"></span>
<img src="images/2-1-influence-NA.png" alt="Number of NA and imputation time" width="800" />
<p class="caption">
FIGURE 2.5: Number of NA and imputation time
</p>
</div>
<p>The time needed to process NA’s is very volatile (see Figure <a href="default-imputation-efficiency-comparison.html#fig:2-1-influence-NA">2.5</a>). For simple imputation methods and VIM’s hotdeck time needed compared to the rest of the imputations seems to be constant. This plot was achieved by getting information from datasets and imputing ten times. Then the median was taken from times of dataset imputation. Fluctuations might be the effect of different data types, a number of factors, continuous variables distribution.</p>
</div>
</div>
</div>
<div id="summary-and-conclusions" class="section level3" number="2.1.6">
<h3><span class="header-section-number">2.1.6</span> Summary and conclusions</h3>
<p>Our experiments have unexpected results - it turns out that using a naive strategy of imputing with random values resulted in models with the best performance. The reason for the poor performance of more complex imputation methods might be the fact that we used default values of hyperparameters and some tuning may be required to show the full potential of these methods. Some datasets were simple and small therefore potential imputation leverage could be insignificant. What we did prove was that simple imputation methods might still be valuable despite their apparent flaws. What was significant was the big overhead that simple imputations had in terms of results achieved in time. While having a big dataset with a limited number of missing values they might be the best solution because some methods use whole data to train it’s missing value imputation.<br />
We might not ignore huge computational overhead that is the result of using complex computing methods. The point of this paper was to test those methods for inexperienced users and find the most valuable one for them. As for now considering both performance and time the best for most models are <em>random fill</em>, <em>median and mode</em>. They achieve more than satisfying results, are reliable and work for every dataset. Judging by the results it is better to use simple imputation methods than more complex methods with default parameters from various packages.</p>

</div>
</div>
<h3>References</h3>
<div id="refs" class="references">
<div id="ref-2-1-OpenML100">
<p>Bischl, Bernd, Giuseppe Casalicchio, Matthias Feurer, Frank Hutter, Michel Lang, Rafael Mantovani, Jan van Rijn, and Joaquin Vanschoren. 2017. “OpenML Benchmarking Suites and the Openml100,” August.</p>
</div>
<div id="ref-2-1-weighted-tpr-tnr">
<p>Jadhav, Anil. 2020. “A Novel Weighted Tpr-Tnr Measure to Assess Performance of the Classifiers.” <em>Expert Systems with Applications</em> 152 (March): 113391. <a href="https://doi.org/10.1016/j.eswa.2020.113391">https://doi.org/10.1016/j.eswa.2020.113391</a>.</p>
</div>
<div id="ref-2-1-VIM">
<p>Kowarik, Alexander, and Matthias Templ. 2016a. “Imputation with the R Package VIM.” <em>Journal of Statistical Software</em> 74 (7): 1–16. <a href="https://doi.org/10.18637/jss.v074.i07">https://doi.org/10.18637/jss.v074.i07</a>.</p>
</div>
<div id="ref-2-1-caret">
<p>Kuhn, Max. 2008. “Building Predictive Models in R Using the Caret Package.” <em>Journal of Statistical Software, Articles</em> 28 (5): 1–26. <a href="https://doi.org/10.18637/jss.v028.i05">https://doi.org/10.18637/jss.v028.i05</a>.</p>
</div>
<div id="ref-2-1-little-rubin">
<p>Little, R. J. A., and D. B. Rubin. 2002. <em>Statistical Analysis with Missing Data</em>. Wiley Series in Probability and Mathematical Statistics. Probability and Mathematical Statistics. Wiley. <a href="http://books.google.com/books?id=aYPwAAAAMAAJ">http://books.google.com/books?id=aYPwAAAAMAAJ</a>.</p>
</div>
<div id="ref-2-1-missForest">
<p>Stekhoven, Daniel J., and Peter Buehlmann. 2012a. “MissForest - Non-Parametric Missing Value Imputation for Mixed-Type Data.” <em>Bioinformatics</em> 28 (1): 112–18.</p>
</div>
<div id="ref-2-1-mice">
<p>van Buuren, Stef, and Karin Groothuis-Oudshoorn. 2011a. “mice: Multivariate Imputation by Chained Equations in R.” <em>Journal of Statistical Software</em> 45 (3): 1–67. <a href="https://www.jstatsoft.org/v45/i03/">https://www.jstatsoft.org/v45/i03/</a>.</p>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="imputation.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="the-hajada-imputation-test.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook/js/app.min.js"></script>
<script src="libs/gitbook/js/lunr.js"></script>
<script src="libs/gitbook/js/clipboard.min.js"></script>
<script src="libs/gitbook/js/plugin-search.js"></script>
<script src="libs/gitbook/js/plugin-sharing.js"></script>
<script src="libs/gitbook/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook/js/plugin-bookdown.js"></script>
<script src="libs/gitbook/js/jquery.highlight.js"></script>
<script src="libs/gitbook/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": true,
"facebook": false,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/mini-pw/2020L-WB-Book/edit/master/2-1-default-imputation-efficiency-comparison.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["book.pdf", "book.epub"],
"toc": {
"collapse": "section"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
